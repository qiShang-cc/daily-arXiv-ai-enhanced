{"id": "2509.08830", "pdf": "https://arxiv.org/pdf/2509.08830", "abs": "https://arxiv.org/abs/2509.08830", "authors": ["Seong-A Park", "Jong-Eui Chae", "Sungdong Kim", "Hyung-Chul Lee", "Hyun-Lim Yang"], "title": "A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals", "categories": ["eess.SP", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "In clinical settings, monitoring hemodynamics is crucial for managing patient\nprognosis, necessitating the integrated analysis of multiple physiological\nsignals. While recent research has analyzed single signals such as\nelectrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a\nproposal for an approach that encompasses the complex signal analysis required\nin actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul\nNational University hospital PHYsiological signal Masked representation\nlearning) model extracts physiological features reflecting the electrical,\npressure, and fluid characteristics of the cardiac cycle in the process of\nrestoring three masked physiological signals based on self-supervised learning\n(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing\nmultiple physical characteristics, the model can extract more enriched features\nonly using non-invasive signals. We evaluated the model's performance in\nclinical downstream tasks such as hypotension, stroke volume, systolic blood\npressure, diastolic blood pressure, and age prediction. Our results showed that\nthe SNUPHY-M significantly outperformed supervised or SSL models, especially in\nprediction tasks using non-invasive signals. To the best of our knowledge,\nSNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis\ninvolving ECG, PPG, and ABP signals. This approach effectively supports\nclinical decision-making and enables precise diagnostics, contributing\nsignificantly to the early diagnosis and management of hemodynamics without\ninvasiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSNUPHY-M\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u5fc3\u7535\u56fe\uff08ECG\uff09\u3001\u5149\u7535\u5bb9\u79ef\u56fe\uff08PPG\uff09\u548c\u52a8\u8109\u8840\u538b\uff08ABP\uff09\u4fe1\u53f7\uff0c\u4ee5\u63d0\u5347\u8840\u6db2\u52a8\u529b\u5b66\u7684\u65e9\u671f\u8bca\u65ad\u548c\u975e\u4fb5\u5165\u6027\u7ba1\u7406\u3002", "motivation": "\u4e34\u5e8a\u4e2d\u8840\u6db2\u52a8\u529b\u5b66\u7684\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u4fe1\u53f7\u5206\u6790\uff0c\u7f3a\u4e4f\u5b9e\u9645\u4e34\u5e8a\u573a\u666f\u6240\u9700\u7684\u590d\u6742\u4fe1\u53f7\u7efc\u5408\u5206\u6790\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u591a\u751f\u7406\u4fe1\u53f7\u7684\u65b9\u6cd5\u6765\u652f\u6301\u7cbe\u51c6\u8bca\u65ad\u548c\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86SNUPHY-M\u6a21\u578b\uff0c\u901a\u8fc7\u6062\u590d\u4e09\u79cd\u63a9\u853d\u751f\u7406\u4fe1\u53f7\uff08ECG\u3001PPG\u548cABP\uff09\u6765\u63d0\u53d6\u53cd\u6620\u5fc3\u810f\u5468\u671f\u7684\u7535\u5b66\u3001\u538b\u529b\u548c\u6d41\u4f53\u7279\u6027\u7684\u751f\u7406\u7279\u5f81\u3002", "result": "SNUPHY-M\u5728\u4f4e\u8840\u538b\u3001\u5fc3\u8f93\u51fa\u91cf\u3001\u8840\u538b\u9884\u6d4b\u7b49\u4e34\u5e8a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u6216\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4f7f\u7528\u975e\u4fb5\u5165\u6027\u4fe1\u53f7\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SNUPHY-M\u662f\u9996\u4e2a\u5c06\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u5e94\u7528\u4e8e\u5fc3\u8840\u7ba1\u5206\u6790\u7684\u6a21\u578b\uff0c\u6709\u6548\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u548c\u65e0\u521b\u8bca\u65ad\uff0c\u4e3a\u8840\u6db2\u52a8\u529b\u5b66\u7684\u65e9\u671f\u7ba1\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2509.08950", "pdf": "https://arxiv.org/pdf/2509.08950", "abs": "https://arxiv.org/abs/2509.08950", "authors": ["Jarvis Haupt", "Qin Lu", "Yanning Shen", "Jia Chen", "Yue Dong", "Dan McCreary", "Mehmet Ak\u00e7akaya", "Georgios B. Giannakis"], "title": "Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities", "categories": ["eess.SP", "cs.LG"], "comment": "Accepted to the IEEE Signal Processing Magazine Special Issue on\n  Artificial Intelligence for Education: A Signal Processing Perspective", "summary": "Powerful artificial intelligence (AI) tools that have emerged in recent years\n-- including large language models, automated coding assistants, and advanced\nimage and speech generation technologies -- are the result of monumental human\nachievements. These breakthroughs reflect mastery across multiple technical\ndisciplines and the resolution of significant technological challenges.\nHowever, some of the most profound challenges may still lie ahead. These\nchallenges are not purely technical but pertain to the fair and responsible use\nof AI in ways that genuinely improve the global human condition. This article\nexplores one promising application aligned with that vision: the use of AI\ntools to facilitate and enhance education, with a specific focus on signal\nprocessing (SP). It presents two interrelated perspectives: identifying and\naddressing technical limitations, and applying AI tools in practice to improve\neducational experiences. Primers are provided on several core technical issues\nthat arise when using AI in educational settings, including how to ensure\nfairness and inclusivity, handle hallucinated outputs, and achieve efficient\nuse of resources. These and other considerations -- such as transparency,\nexplainability, and trustworthiness -- are illustrated through the development\nof an immersive, structured, and reliable \"smart textbook.\" The article serves\nas a resource for researchers and educators seeking to advance AI's role in\nengineering education.", "AI": {"tldr": "AI\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u4fe1\u53f7\u5904\u7406\u9886\u57df\uff0c\u65e8\u5728\u89e3\u51b3\u516c\u5e73\u6027\u3001\u5305\u5bb9\u6027\u7b49\u6280\u672f\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u6559\u6750\u63a8\u52a8\u6559\u80b2\u6539\u9769\u3002", "motivation": "\u63a2\u7d22AI\u5728\u63a8\u52a8\u5168\u7403\u6559\u80b2\u516c\u5e73\u548c\u63d0\u5347\u4eba\u7c7b\u798f\u7949\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u4fe1\u53f7\u5904\u7406\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u4f9b\u6280\u672f\u95ee\u9898\u7684\u5165\u95e8\u77e5\u8bc6\uff0c\u5f00\u53d1\u6c89\u6d78\u5f0f\u3001\u7ed3\u6784\u5316\u7684\u667a\u80fd\u6559\u6750\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u786e\u4fddAI\u5de5\u5177\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u8d56\u6027\u3002", "result": "\u4e3a\u7814\u7a76\u8005\u548c\u6559\u80b2\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u66f4\u597d\u5730\u5229\u7528AI\u6280\u672f\u3002", "conclusion": "AI\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u516c\u5e73\u3001\u900f\u660e\u7b49\u6280\u672f\u548c\u793e\u4f1a\u95ee\u9898\u3002"}}
{"id": "2509.08973", "pdf": "https://arxiv.org/pdf/2509.08973", "abs": "https://arxiv.org/abs/2509.08973", "authors": ["Harshit Agrawal", "Ari Hietanen", "Simo S\u00e4rkk\u00e4"], "title": "Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Purpose: Scatter artifacts drastically degrade the image quality of cone-beam\ncomputed tomography (CBCT) scans. Although deep learning-based methods show\npromise in estimating scatter from CBCT measurements, their deployment in\nmobile CBCT systems or edge devices is still limited due to the large memory\nfootprint of the networks. This study addresses the issue by applying networks\nat varying resolutions and suggesting an optimal one, based on speed and\naccuracy.\n  Methods: First, the reconstruction error in down-up sampling of CBCT scatter\nsignal was examined at six resolutions by comparing four interpolation methods.\nNext, a recent state-of-the-art method was trained across five image\nresolutions and evaluated for the reductions in floating-point operations\n(FLOPs), inference times, and GPU memory requirements.\n  Results: Reducing the input size and network parameters achieved a 78-fold\nreduction in FLOPs compared to the baseline method, while maintaining comarable\nperformance in terms of mean-absolute-percentage-error (MAPE) and\nmean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to\n4.42%, and the MSE decreased to 1.34 \\times 10^{-2} compared to 2.01 \\times\n10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and\n12, respectively. Further experiments comparing scatter-corrected\nreconstructions on a large, simulated dataset and real CBCT scans from water\nand Sedentex CT phantoms clearly demonstrated the robustness of our method.\n  Conclusion: This study highlights the underappreciated role of downsampling\nin deep learning-based scatter estimation. The substantial reduction in FLOPs\nand GPU memory requirements achieved by our method enables scatter correction\nin resource-constrained environments, such as mobile CBCT and edge devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u5e94\u7528\u7f51\u7edc\u5e76\u63d0\u51fa\u57fa\u4e8e\u901f\u5ea6\u548c\u51c6\u786e\u6027\u7684\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86CBCT\u6563\u5c04\u4f30\u8ba1\u7684\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "CBCT\u626b\u63cf\u4e2d\u7684\u6563\u5c04\u4f2a\u5f71\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5e9e\u5927\u5185\u5b58\u5360\u7528\u9650\u5236\u4e86\u5176\u5728\u79fb\u52a8CBCT\u7cfb\u7edf\u6216\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u7814\u7a76\u9996\u5148\u5728\u516d\u79cd\u5206\u8fa8\u7387\u4e0b\u6bd4\u8f83\u4e86\u56db\u79cd\u63d2\u503c\u65b9\u6cd5\u7684CBCT\u6563\u5c04\u4fe1\u53f7\u91cd\u5efa\u8bef\u5dee\uff0c\u968f\u540e\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u56fe\u50cf\u5206\u8fa8\u7387\u4e0b\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11FLOPs\u3001\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u9700\u6c42\u3002", "result": "\u51cf\u5c11\u8f93\u5165\u5927\u5c0f\u548c\u7f51\u7edc\u53c2\u6570\u4f7fFLOPs\u964d\u4f4e\u4e8678\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u7684MAPE\u548cMSE\u6027\u80fd\uff1b\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\u5206\u522b\u51cf\u5c11\u4e8616\u500d\u548c12\u500d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u964d\u91c7\u6837\u5728\u6df1\u5ea6\u5b66\u4e60\u6563\u5c04\u4f30\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u5982\u79fb\u52a8CBCT\u548c\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2509.09005", "pdf": "https://arxiv.org/pdf/2509.09005", "abs": "https://arxiv.org/abs/2509.09005", "authors": ["Hirley Alves", "Nurul H. Mahmood", "Onel L. A. L\u00f3pez", "Sumudu Samarakoon", "Seppo Yrj\u00f6l\u00e4", "Matti Latva-Aho", "Markku Juntti", "Ari Pouttu", "Armin Dekorsy", "Arthur Sousa de Sena", "Aydin Sezgin", "Bho Matthiesen", "Chafika Benzaid", "Chathuranga Weeraddana", "David Hutchison", "Dileepa Marasinghe", "Doganalp Ergenc", "Eduard Jorswieck", "Erkki Harjula", "Falko Dressler", "Harri Saarnisaari", "Italo Atzeni", "Jaap Van De Beek", "Jacek Rak", "Konstantin Mikhaylov", "Lauri Loven", "Madhusanka Liyanage", "Marcos Katz", "Marja Matinmikko-Blue", "Mehdi Rasti", "Mika Ylianttila Nhan Nguyen", "Pawani Porambage", "Petar Popovski", "Petri Ahokangas", "Premanandana Rajatheva", "Robert-Jeron Reifert", "Tharaka Hewa", "Tommy Svensson"], "title": "6G Resilience -- White Paper", "categories": ["eess.SP", "cs.ET", "cs.SI"], "comment": null, "summary": "6G must be designed to withstand, adapt to, and evolve amid prolonged,\ncomplex disruptions. Mobile networks' shift from efficiency-first to\nsustainability-aware has motivated this white paper to assert that resilience\nis a primary design goal, alongside sustainability and efficiency, encompassing\ntechnology, architecture, and economics. We promote resilience by analysing\ndependencies between mobile networks and other critical systems, such as\nenergy, transport, and emergency services, and illustrate how cascading\nfailures spread through infrastructures. We formalise resilience using the 3R\nframework: reliability, robustness, resilience. Subsequently, we translate this\ninto measurable capabilities: graceful degradation, situational awareness,\nrapid reconfiguration, and learning-driven improvement and recovery.\n  Architecturally, we promote edge-native and locality-aware designs, open\ninterfaces, and programmability to enable islanded operations, fallback modes,\nand multi-layer diversity (radio, compute, energy, timing). Key enablers\ninclude AI-native control loops with verifiable behaviour, zero-trust security\nrooted in hardware and supply-chain integrity, and networking techniques that\nprioritise critical traffic, time-sensitive flows, and inter-domain\ncoordination.\n  Resilience also has a techno-economic aspect: open platforms and high-quality\ncomplementors generate ecosystem externalities that enhance resilience while\nopening new markets. We identify nine business-model groups and several\npatterns aligned with the 3R objectives, and we outline governance and\nstandardisation. This white paper serves as an initial step and catalyst for 6G\nresilience. It aims to inspire researchers, professionals, government\nofficials, and the public, providing them with the essential components to\nunderstand and shape the development of 6G resilience.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5c06\u97e7\u6027\u4f5c\u4e3a6G\u7f51\u7edc\u7684\u4e3b\u8981\u8bbe\u8ba1\u76ee\u6807\u4e4b\u4e00\uff0c\u5f3a\u8c03\u5176\u5728\u6280\u672f\u3001\u67b6\u6784\u548c\u7ecf\u6d4e\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa3R\u6846\u67b6\uff08\u53ef\u9760\u6027\u3001\u5065\u58ee\u6027\u3001\u97e7\u6027\uff09\u6765\u91cf\u5316\u97e7\u6027\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u4ece\u6548\u7387\u4f18\u5148\u8f6c\u5411\u53ef\u6301\u7eed\u6027\u610f\u8bc6\uff0c\u4fc3\u4f7f\u5c06\u97e7\u6027\uff08resilience\uff09\u5217\u4e3a6G\u7684\u6838\u5fc3\u8bbe\u8ba1\u76ee\u6807\u4e4b\u4e00\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u548c\u957f\u671f\u7684\u5e72\u6270\u3002", "method": "\u901a\u8fc7\u5206\u6790\u79fb\u52a8\u7f51\u7edc\u4e0e\u5176\u4ed6\u5173\u952e\u7cfb\u7edf\uff08\u5982\u80fd\u6e90\u3001\u4ea4\u901a\u548c\u5e94\u6025\u670d\u52a1\uff09\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u51fa3R\u6846\u67b6\uff0c\u5e76\u8f6c\u5316\u4e3a\u53ef\u91cf\u5316\u7684\u80fd\u529b\uff08\u5982\u4f18\u96c5\u964d\u7ea7\u3001\u5feb\u901f\u91cd\u6784\u7b49\uff09\u3002\u67b6\u6784\u4e0a\u63d0\u51fa\u8fb9\u7f18\u539f\u751f\u3001\u5f00\u653e\u63a5\u53e3\u548c\u53ef\u7f16\u7a0b\u6027\u8bbe\u8ba1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff08\u5982AI\u539f\u751f\u63a7\u5236\u73af\u8def\u3001\u96f6\u4fe1\u4efb\u5b89\u5168\uff09\u548c\u7ecf\u6d4e\uff08\u5f00\u653e\u5e73\u53f0\u3001\u5546\u4e1a\u6a21\u5f0f\uff09\u7684\u8d4b\u80fd\u624b\u6bb5\uff0c\u4ee5\u589e\u5f3a6G\u7684\u97e7\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a6G\u97e7\u6027\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u65e8\u5728\u542f\u53d1\u5404\u754c\u5171\u540c\u63a8\u52a86G\u97e7\u6027\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.08859", "pdf": "https://arxiv.org/pdf/2509.08859", "abs": "https://arxiv.org/abs/2509.08859", "authors": ["Vincenzo Suriani", "Daniele Affinita", "Domenico D. Bloisi", "Daniele Nardi"], "title": "Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication", "categories": ["cs.RO", "cs.AI"], "comment": "The 19th International Conference on Intelligent Autonomous Systems\n  (IAS 19), 2025, Genoa", "summary": "Coordinating a fully distributed multi-agent system (MAS) can be challenging\nwhen the communication channel has very limited capabilities in terms of\nsending rate and packet payload. When the MAS has to deal with active obstacles\nin a highly partially observable environment, the communication channel\nacquires considerable relevance. In this paper, we present an approach to deal\nwith task assignments in extremely active scenarios, where tasks need to be\nfrequently reallocated among the agents participating in the coordination\nprocess. Inspired by market-based task assignments, we introduce a novel\ndistributed coordination method to orchestrate autonomous agents' actions\nefficiently in low communication scenarios. In particular, our algorithm takes\ninto account asymmetric obstacles. While in the real world, the majority of\nobstacles are asymmetric, they are usually treated as symmetric ones, thus\nlimiting the applicability of existing methods. To summarize, the presented\narchitecture is designed to tackle scenarios where the obstacles are active and\nasymmetric, the communication channel is poor and the environment is partially\nobservable. Our approach has been validated in simulation and in the real\nworld, using a team of NAO robots during official RoboCup competitions.\nExperimental results show a notable reduction in task overlaps in limited\ncommunication settings, with a decrease of 52% in the most frequent reallocated\ntask.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u534f\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u901a\u4fe1\u80fd\u529b\u6709\u9650\u7684\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u5206\u914d\u4efb\u52a1\uff0c\u7279\u522b\u9488\u5bf9\u975e\u5bf9\u79f0\u969c\u788d\u7269\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\uff0c\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u91cd\u53e0\u663e\u8457\u51cf\u5c11\u3002", "motivation": "\u89e3\u51b3\u5728\u901a\u4fe1\u80fd\u529b\u6709\u9650\u4e14\u73af\u5883\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u4efb\u52a1\u5206\u914d\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u52a8\u6001\u548c\u975e\u5bf9\u79f0\u969c\u788d\u7269\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u9650\u5236\u3002", "method": "\u53d7\u57fa\u4e8e\u5e02\u573a\u7684\u4efb\u52a1\u5206\u914d\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u534f\u8c03\u7b97\u6cd5\uff0c\u8003\u8651\u4e86\u975e\u5bf9\u79f0\u969c\u788d\u7269\uff0c\u9002\u7528\u4e8e\u4f4e\u901a\u4fe1\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728RoboCup\u6bd4\u8d5b\u4e2d\uff0c\u4efb\u52a1\u91cd\u53e0\u51cf\u5c11\u4e8652%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u3001\u975e\u5bf9\u79f0\u969c\u788d\u7269\u548c\u901a\u4fe1\u53d7\u9650\u7684\u73af\u5883\uff0c\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u6548\u7387\u3002"}}
{"id": "2509.09018", "pdf": "https://arxiv.org/pdf/2509.09018", "abs": "https://arxiv.org/abs/2509.09018", "authors": ["Xueyi Wang", "C. J. C.", "Lamoth", "Elisabeth Wilhelm"], "title": "Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "The paper has been acceptted and presented in the 47th Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society", "summary": "A sleep forecast allows individuals and healthcare providers to anticipate\nand proactively address factors influencing restful rest, ultimately improving\nmental and physical well-being. This work presents an adaptive spatial and\ntemporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model\ncombines convolutional layers to capture spatial feature interactions between\nmultiple features and recurrent neural network layers to handle longer-term\ntemporal health-related data. A domain classifier is further integrated to\ngeneralize across different subjects. We conducted several experiments using\nfive input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes\n(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline\nmodels, achieving its lowest RMSE (0.282) with a seven-day input window and a\none-day predicting window. Moreover, the method maintained strong performance\neven when forecasting multiple days into the future, demonstrating its\nversatility for real-world applications. Visual comparisons reveal that the\nmodel accurately tracks both the overall sleep score level and daily\nfluctuations. These findings prove that the proposed framework provides a\nrobust and adaptable solution for personalized sleep forecasting using sparse\ndata from commercial wearable devices and domain adaptation techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65f6\u7a7a\u6a21\u578b\uff08AdaST-Sleep\uff09\u7528\u4e8e\u9884\u6d4b\u7761\u7720\u8bc4\u5206\uff0c\u7ed3\u5408\u5377\u79ef\u5c42\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u5904\u7406\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u5206\u7c7b\u5668\u5b9e\u73b0\u8de8\u4e3b\u4f53\u6cdb\u5316\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u7a97\u53e3\u5c3a\u5bf8\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7761\u7720\u9884\u6d4b\u6709\u52a9\u4e8e\u4e2a\u4f53\u548c\u533b\u7597\u63d0\u4f9b\u8005\u63d0\u524d\u5e72\u9884\u5f71\u54cd\u7761\u7720\u7684\u56e0\u7d20\uff0c\u6539\u5584\u8eab\u5fc3\u5065\u5eb7\u3002\u5f53\u524d\u65b9\u6cd5\u5728\u7a00\u758f\u6570\u636e\u548c\u8de8\u4e3b\u4f53\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "AdaST-Sleep\u6a21\u578b\u7ed3\u5408\u5377\u79ef\u5c42\uff08\u6355\u6349\u7a7a\u95f4\u7279\u5f81\uff09\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\uff08\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff09\uff0c\u5e76\u96c6\u6210\u9886\u57df\u5206\u7c7b\u5668\u4ee5\u589e\u5f3a\u8de8\u4e3b\u4f53\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u6d4b\u8bd5\u4e86\u591a\u79cd\u8f93\u5165\u548c\u9884\u6d4b\u7a97\u53e3\u5c3a\u5bf8\u3002", "result": "\u6a21\u578b\u57287\u5929\u8f93\u5165\u7a97\u53e3\u548c1\u5929\u9884\u6d4b\u7a97\u53e3\u4e0b\u53d6\u5f97\u6700\u4f4eRMSE\uff080.282\uff09\uff0c\u5e76\u80fd\u51c6\u786e\u8ffd\u8e2a\u7761\u7720\u8bc4\u5206\u6c34\u5e73\u548c\u65e5\u5e38\u6ce2\u52a8\uff0c\u8868\u73b0\u51fa\u5bf9\u672a\u6765\u591a\u65e5\u9884\u6d4b\u7684\u9002\u5e94\u6027\u3002", "conclusion": "AdaST-Sleep\u4e3a\u4f7f\u7528\u5546\u4e1a\u53ef\u7a7f\u6234\u8bbe\u5907\u7a00\u758f\u6570\u636e\u8fdb\u884c\u4e2a\u6027\u5316\u7761\u7720\u9884\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.09024", "pdf": "https://arxiv.org/pdf/2509.09024", "abs": "https://arxiv.org/abs/2509.09024", "authors": ["Md Habib Ullah Khan", "Kaiyue Deng", "Ismail Mujtaba Khan", "Kelvin Fu"], "title": "Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites", "categories": ["cs.RO", "physics.app-ph"], "comment": "23 pages, 5 figures", "summary": "The demand for lightweight and high-strength composite structures is rapidly\ngrowing in aerospace and robotics, particularly for optimized drone frames.\nHowever, conventional composite manufacturing methods struggle to achieve\ncomplex 3D architectures for weight savings and rely on assembling separate\ncomponents, which introduce weak points at the joints. Additionally,\nmaintaining continuous fiber reinforcement remains challenging, limiting\nstructural efficiency. In this study, we demonstrate the lightweight Face\nCentered Cubic (FFC) lattice structured conceptualization of drone frames for\nweight reduction and complex topology fabrication through 3D Fiber Tethering\n(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,\neliminating weak points associated with traditional composite assembly.\nMechanical testing demonstrates that the fabricated drone frame exhibits a high\nspecific strength of around four to eight times the metal and thermoplastic,\noutperforming other conventional 3D printing methods. The drone frame weighs\nonly 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing\nstructural integrity and contributing to an extended flight time of three\nminutes, while flight testing confirms its stability and durability under\noperational conditions. The findings demonstrate the potential of single tow\nlattice truss-based drone frames, with 3DFiT serving as a scalable and\nefficient manufacturing method.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u7ea4\u7ef4\u7cfb\u6cca\uff083DFiT\uff09\u6280\u672f\u7684\u8f7b\u91cf\u5316\u65e0\u4eba\u673a\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u80a1\u8fde\u7eed\u7ea4\u7ef4\u6784\u5efa\u9762\u5fc3\u7acb\u65b9\uff08FCC\uff09\u6676\u683c\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ed3\u6784\u7684\u6bd4\u5f3a\u5ea6\u548c\u51cf\u8f7b\u4e86\u91cd\u91cf\u3002", "motivation": "\u822a\u7a7a\u822a\u5929\u548c\u673a\u5668\u4eba\u9886\u57df\u5bf9\u8f7b\u91cf\u9ad8\u5f3a\u590d\u5408\u6750\u6599\u7ed3\u6784\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f20\u7edf\u5236\u9020\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u590d\u67423D\u7ed3\u6784\u4e14\u5b58\u5728\u8fde\u63a5\u8584\u5f31\u7684\u95ee\u9898\u3002", "method": "\u91c7\u75283DFiT\u6280\u672f\uff0c\u901a\u8fc7\u8fde\u7eed\u5355\u80a1\u7ea4\u7ef4\u7cbe\u786e\u6392\u5217\u6784\u5efaFCC\u6676\u683c\u7ed3\u6784\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u590d\u5408\u6750\u6599\u7ec4\u88c5\u7684\u8584\u5f31\u70b9\u3002", "result": "\u5236\u9020\u7684\u65e0\u4eba\u673a\u6846\u67b6\u6bd4\u5f3a\u5ea6\u662f\u91d1\u5c5e\u548c\u70ed\u5851\u6027\u6750\u6599\u76844-8\u500d\uff0c\u91cd\u91cf\u4ec5260\u514b\uff0c\u6bd4\u5546\u7528DJI F450\u8f7b10%\uff0c\u98de\u884c\u65f6\u95f4\u5ef6\u957f\u4e863\u5206\u949f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5355\u80a1\u6676\u683c\u6841\u67b6\u7ed3\u54083DFiT\u6280\u672f\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4e3a\u8f7b\u91cf\u5316\u7ed3\u6784\u5236\u9020\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.09056", "pdf": "https://arxiv.org/pdf/2509.09056", "abs": "https://arxiv.org/abs/2509.09056", "authors": ["Michael Caulfield", "Randy Palamar", "Darren Dahunsi", "Mohammad Rahim Sobhani", "Negar Majidi", "Roger Zemp"], "title": "Improving the Elevational Focusing of Fast Orthogonal Row-Column Electronic Scanning (FORCES) Ultrasound Imaging using Retrospective Transmit Beamforming (RTB)", "categories": ["eess.SP"], "comment": "6 pages, 8 figures", "summary": "Recent developments in Row Column Arrays (RCAs) have presented promising\noptions for volumetric imaging without the need for the excessive channel\ncounts of fully wired 2D-arrays. Bias programmable RCAs, also known as Top\nOrthogonal to Bottom Electrode (TOBE) Arrays, show further promise in that\nimaging schemes, such as Fast Orthogonal Row-Column Electronic Scanning\n(FORCES) allow for full transmit and receive focusing everywhere in the image\nplane. However, due to its fixed elevational focus and large transmit aperture,\nFORCES experiences poor elevational focusing away from the focal point. In this\nstudy we present a modification to the FORCES imaging scheme by applying\nRetrospective Transmit Beamforming (RTB) in the elevational direction to allow\nfor elevational transmit focusing everywhere in the imaging plane. We evaluate\nFORCES and uFORCES methods, with and without RTB applied, when imaging both a\ncyst and wire phantom. With experiment we show improved elevational focusing\ncapabilities away from the focal point when RTB is applied to both FORCES and\nuFORCES. At the focal point, performance with RTB remains comparable or\nimproved relative to standard FORCES. This is quantified by the measurement of\nFull Width Half Max when imaging the wire phantom, and by the generalized\nContrast to Noise Ratio when imaging the tubular cyst phantom. We also\ndemonstrate the volumetric imaging capabilities of FORCES RTB with the wire\nphantom.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684FORCES\u6210\u50cf\u65b9\u6848\uff0c\u901a\u8fc7\u5e94\u7528\u9006\u5411\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\uff08RTB\uff09\u5728\u5782\u76f4\u65b9\u5411\u5b9e\u73b0\u5168\u56fe\u50cf\u5e73\u9762\u7684\u805a\u7126\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cRTB\u663e\u8457\u63d0\u9ad8\u4e86\u8fdc\u79bb\u7126\u70b9\u7684\u5782\u76f4\u805a\u7126\u80fd\u529b\uff0c\u5e76\u5728\u7126\u70b9\u5904\u4fdd\u6301\u6216\u4f18\u4e8e\u6807\u51c6FORCES\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684FORCES\u6210\u50cf\u65b9\u6848\u5728\u5782\u76f4\u65b9\u5411\u7684\u805a\u7126\u80fd\u529b\u8f83\u5dee\uff0c\u5c24\u5176\u662f\u5728\u8fdc\u79bb\u7126\u70b9\u65f6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7RTB\u6280\u672f\u6539\u5584\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u5168\u56fe\u50cf\u5e73\u9762\u7684\u5782\u76f4\u53d1\u5c04\u805a\u7126\u3002", "method": "\u5728FORCES\u6210\u50cf\u65b9\u6848\u4e2d\u5e94\u7528RTB\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u6539\u8fdb\u540e\u7684uFORCES\u548cFORCES\u65b9\u6cd5\u5728\u56ca\u80bf\u548c\u7ebf\u6a21\u4f53\u6210\u50cf\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRTB\u663e\u8457\u63d0\u5347\u4e86\u8fdc\u79bb\u7126\u70b9\u65f6\u7684\u5782\u76f4\u805a\u7126\u80fd\u529b\uff0c\u4e14\u5728\u7126\u70b9\u5904\u7684\u6027\u80fd\u4e0e\u6807\u51c6FORCES\u76f8\u5f53\u6216\u66f4\u4f18\u3002\u5177\u4f53\u901a\u8fc7\u7ebf\u6a21\u4f53\u7684\u534a\u9ad8\u5168\u5bbd\u548c\u56ca\u80bf\u6a21\u4f53\u7684\u5e7f\u4e49\u4fe1\u566a\u6bd4\u8fdb\u884c\u91cf\u5316\u3002", "conclusion": "RTB\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86FORCES\u6210\u50cf\u65b9\u6848\u4e2d\u5782\u76f4\u65b9\u5411\u805a\u7126\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3aRCAs\u5728\u4f53\u79ef\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u65b9\u6848\u3002"}}
{"id": "2509.09074", "pdf": "https://arxiv.org/pdf/2509.09074", "abs": "https://arxiv.org/abs/2509.09074", "authors": ["Alice Kate Li", "Thales C Silva", "Victoria Edwards", "Vijay Kumar", "M. Ani Hsieh"], "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to CoRL 2025 (Conference on Robot Learning). 15 pages 11\n  figures", "summary": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such\nthat it converges to the trajectory's end point. Despite demonstrated efficacy\nin using Koopman operator theory for modeling dynamical systems, Koopman does\nnot inherently enforce convergence to desired trajectories nor to specified\ngoals -- a requirement when learning from demonstrations (LfD). We present\nKoopMotion which represents motion flow fields as dynamical systems,\nparameterized by Koopman Operators to mimic desired trajectories, and leverages\nthe divergence properties of the learnt flow fields to obtain smooth motion\nfields that converge to a desired reference trajectory when a robot is placed\naway from the desired trajectory, and tracks the trajectory until the end\npoint. To demonstrate the effectiveness of our approach, we show evaluations of\nKoopMotion on the LASA human handwriting dataset and a 3D manipulator\nend-effector trajectory dataset, including spectral analysis. We also perform\nexperiments on a physical robot, verifying KoopMotion on a miniature autonomous\nsurface vehicle operating in a non-static fluid flow environment. Our approach\nis highly sample efficient in both space and time, requiring only 3\\% of the\nLASA dataset to generate dense motion plans. Additionally, KoopMotion provides\na significant improvement over baselines when comparing metrics that measure\nspatial and temporal dynamics modeling efficacy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d41\u573a\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5KoopMotion\uff0c\u5229\u7528Koopman\u7b97\u5b50\u6784\u5efa\u52a8\u6001\u7cfb\u7edf\u4ee5\u6a21\u62df\u671f\u671b\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u6d41\u573a\u7684\u53d1\u6563\u7279\u6027\u5b9e\u73b0\u5e73\u6ed1\u8fd0\u52a8\uff0c\u4f7f\u673a\u5668\u4eba\u6536\u655b\u81f3\u76ee\u6807\u8f68\u8ff9\u3002", "motivation": "Koopman\u7b97\u5b50\u7406\u8bba\u867d\u80fd\u6709\u6548\u5efa\u6a21\u52a8\u6001\u7cfb\u7edf\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u81f3\u76ee\u6807\u8f68\u8ff9\u6216\u7ec8\u70b9\u3002\u6587\u4e2d\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u6f14\u793a\u6570\u636e\uff08LfD\uff09\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KoopMotion\u5c06\u8fd0\u52a8\u6d41\u573a\u5efa\u6a21\u4e3a\u52a8\u6001\u7cfb\u7edf\uff0c\u53c2\u6570\u5316\u4e3aKoopman\u7b97\u5b50\uff0c\u5229\u7528\u6d41\u573a\u7684\u53d1\u6563\u7279\u6027\u751f\u6210\u5e73\u6ed1\u8fd0\u52a8\u573a\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u8fdc\u79bb\u76ee\u6807\u8f68\u8ff9\u65f6\u4ecd\u80fd\u6536\u655b\u5e76\u8ddf\u8e2a\u81f3\u7ec8\u70b9\u3002", "result": "\u5728LASA\u624b\u5199\u6570\u636e\u96c6\u548c3D\u673a\u68b0\u81c2\u672b\u7aef\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5305\u62ec\u8c31\u5206\u6790\u3002\u5b9e\u7269\u5b9e\u9a8c\u5728\u5fae\u578b\u81ea\u4e3b\u6c34\u9762\u673a\u5668\u4eba\u4e0a\u6210\u529f\u9a8c\u8bc1\u3002\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4ec5\u97003%\u6570\u636e\u751f\u6210\u5bc6\u96c6\u8fd0\u52a8\u8ba1\u5212\u3002", "conclusion": "KoopMotion\u5728\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u6548\u679c\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u9759\u6001\u6d41\u4f53\u73af\u5883\u7b49\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2509.09120", "pdf": "https://arxiv.org/pdf/2509.09120", "abs": "https://arxiv.org/abs/2509.09120", "authors": ["Rong Ye", "Xue-Qin Jiang", "Hui Feng", "Jian Wang", "Runhe Qiu"], "title": "Signed Graph Learning with Hidden Nodes", "categories": ["eess.SP"], "comment": "25 pages, 7 figures, published to Signal Processing", "summary": "Signed graphs, which are characterized by both positive and negative edge\nweights, have recently attracted significant attention in the field of graph\nsignal processing (GSP). Existing works on signed graph learning typically\nassume that all graph nodes are available. However, in some specific\napplications, only a subset of nodes can be observed while the remaining nodes\nstay hidden. To address this challenge, we propose a novel method for\nidentifying signed graph that accounts for hidden nodes, termed \\textit{signed\ngraph learning with hidden nodes under column-sparsity regularization}\n(SGL-HNCS). Our method is based on the assumption that graph signals are smooth\nover signed graphs, i.e., signal values of two nodes connected by positive\n(negative) edges are similar (dissimilar). Rooted in this prior assumption, the\ntopology inference of a signed graph is formulated as a constrained\noptimization problem with column-sparsity regularization, where the goal is to\nreconstruct the signed graph Laplacian matrix without disregarding the\ninfluence of hidden nodes. We solve the constrained optimization problem using\na tailored block coordinate descent (BCD) approach. Experimental results using\nsynthetic data and real-world data demonstrate the efficiency of the proposed\nSGL-HNCS method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08SGL-HNCS\uff09\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u9690\u85cf\u8282\u70b9\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u5e26\u7b26\u53f7\u56fe\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u548c\u5217\u7a00\u758f\u6b63\u5219\u5316\u91cd\u5efa\u5e26\u7b26\u53f7\u56fe\u7684\u62c9\u666e\u62c9\u65af\u77e9\u9635\u3002", "motivation": "\u73b0\u6709\u5e26\u7b26\u53f7\u56fe\u5b66\u4e60\u901a\u5e38\u5047\u8bbe\u6240\u6709\u8282\u70b9\u53ef\u89c1\uff0c\u4f46\u5728\u67d0\u4e9b\u5e94\u7528\u4e2d\u4ec5\u90e8\u5206\u8282\u70b9\u53ef\u89c2\u6d4b\uff0c\u5269\u4f59\u8282\u70b9\u9690\u85cf\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u56fe\u4fe1\u53f7\u5728\u5e26\u7b26\u53f7\u56fe\u4e0a\u5e73\u6ed1\u7684\u5047\u8bbe\uff0c\u5c06\u62d3\u6251\u63a8\u7406\u5efa\u6a21\u4e3a\u5e26\u5217\u7a00\u758f\u6b63\u5219\u5316\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSGL-HNCS\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u8868\u73b0\u9ad8\u6548\u3002", "conclusion": "SGL-HNCS\u6210\u529f\u89e3\u51b3\u4e86\u5e26\u9690\u85cf\u8282\u70b9\u7684\u5e26\u7b26\u53f7\u56fe\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09093", "pdf": "https://arxiv.org/pdf/2509.09093", "abs": "https://arxiv.org/abs/2509.09093", "authors": ["Nan Mao", "Guanglu Jia", "Junpeng Chen", "Emmanouil Spyrakos-Papastavridis", "Jian S. Dai"], "title": "Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators", "categories": ["cs.RO"], "comment": "50 pages, 19 figures", "summary": "Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive\nactuators, complex control, and limited adaptability to dynamic tasks. This\nstudy proposes an innovative mechanism of underactuated metamorphic loading\nmanipulators (UMLM), integrating a metamorphic arm with a passively adaptive\ngripper. The metamorphic arm exploits geometric constraints, enabling the\ntopology reconfiguration and flexible motion trajectories without additional\nactuators. The adaptive gripper, driven entirely by the arm, conforms to\ndiverse objects through passive compliance. A structural model is developed,\nand a kinetostatics analysis is conducted to investigate isomorphic grasping\nconfigurations. To optimize performance, Particle-Swarm Optimization (PSO) is\nutilized to refine the gripper's dimensional parameters, ensuring robust\nadaptability across various applications. Simulation results validate the\nUMLM's easily implemented control strategy, operational versatility, and\neffectiveness in grasping diverse objects in dynamic environments. This work\nunderscores the practical potential of underactuated metamorphic mechanisms in\napplications requiring efficient and adaptable loading solutions. Beyond the\nspecific design, this generalized modeling and optimization framework extends\nto a broader class of manipulators, offering a scalable approach to the\ndevelopment of robotic systems that require efficiency, flexibility, and robust\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6b20\u9a71\u52a8\u53d8\u5f62\u52a0\u8f7d\u673a\u68b0\u81c2\uff08UMLM\uff09\uff0c\u7ed3\u5408\u53d8\u5f62\u81c2\u548c\u88ab\u52a8\u81ea\u9002\u5e94\u5939\u722a\uff0c\u89e3\u51b3\u4f20\u7edf\u56fa\u5b9a\u81ea\u7531\u5ea6\u52a0\u8f7d\u673a\u6784\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u548c\u4f18\u5316\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u7269\u4f53\u6293\u53d6\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u81ea\u7531\u5ea6\u52a0\u8f7d\u673a\u6784\u56e0\u6267\u884c\u5668\u8fc7\u591a\u3001\u63a7\u5236\u590d\u6742\u53ca\u52a8\u6001\u4efb\u52a1\u9002\u5e94\u6027\u5dee\uff0c\u9650\u5236\u5176\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7ed3\u6784\u7b80\u5355\u3001\u9002\u5e94\u6027\u5f3a\u7684\u673a\u68b0\u81c2\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faUMLM\u673a\u5236\uff0c\u5c06\u53d8\u5f62\u81c2\u4e0e\u88ab\u52a8\u81ea\u9002\u5e94\u5939\u722a\u7ed3\u5408\uff0c\u5229\u7528\u51e0\u4f55\u7ea6\u675f\u5b9e\u73b0\u62d3\u6251\u91cd\u6784\uff1b\u5efa\u7acb\u7ed3\u6784\u6a21\u578b\u5e76\u8fdb\u884c\u8fd0\u52a8\u9759\u529b\u5b66\u5206\u6790\uff1b\u4f7f\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u4f18\u5316\u5939\u722a\u53c2\u6570\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0cUMLM\u5177\u6709\u6613\u5b9e\u73b0\u7684\u64cd\u63a7\u7b56\u7565\u3001\u64cd\u4f5c\u7075\u6d3b\u6027\u548c\u52a8\u6001\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u6293\u53d6\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6b20\u9a71\u52a8\u53d8\u5f62\u673a\u6784\u5728\u9ad8\u6548\u3001\u7075\u6d3b\u52a0\u8f7d\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u76f8\u5173\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u901a\u7528\u5efa\u6a21\u548c\u4f18\u5316\u6846\u67b6\u3002"}}
{"id": "2509.09144", "pdf": "https://arxiv.org/pdf/2509.09144", "abs": "https://arxiv.org/abs/2509.09144", "authors": ["G Dhinesh Chandran", "Kota Srinivas Reddy", "Srikrishna Bhashyam"], "title": "Sequential Spectral Clustering of Data Sequences", "categories": ["eess.SP"], "comment": null, "summary": "We study the problem of nonparametric clustering of data sequences, where\neach data sequence comprises i.i.d. samples generated from an unknown\ndistribution. The true clusters are the clusters obtained using the Spectral\nclustering algorithm (SPEC) on the pairwise distance between the true\ndistributions corresponding to the data sequences. Since the true distributions\nare unknown, the objective is to estimate the clusters by observing the minimum\nnumber of samples from the data sequences for a given error probability. To\nsolve this problem, we propose the Sequential Spectral clustering algorithm\n(SEQ-SPEC), and show that it stops in finite time almost surely and is\nexponentially consistent. We also propose a computationally more efficient\nalgorithm called the Incremental Approximate Sequential Spectral clustering\nalgorithm (IA-SEQ-SPEC). Through simulations, we show that both our proposed\nalgorithms perform better than the fixed sample size SPEC, the Sequential\n$K$-Medoids clustering algorithm (SEQ-KMED) and the Sequential Single Linkage\nclustering algorithm (SEQ-SLINK). The IA-SEQ-SPEC, while being computationally\nefficient, performs close to SEQ-SPEC on both synthetic and real-world\ndatasets. To the best of our knowledge, this is the first work on spectral\nclustering of data sequences under a sequential framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6570\u636e\u5e8f\u5217\u7684\u975e\u53c2\u6570\u805a\u7c7b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff08SEQ-SPEC\u548cIA-SEQ-SPEC\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u672a\u77e5\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u6700\u5c0f\u6837\u672c\u91cf\u4f30\u8ba1\u6570\u636e\u5e8f\u5217\u7684\u771f\u5b9e\u805a\u7c7b\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1aSEQ-SPEC\uff08\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u505c\u6b62\u4e14\u6307\u6570\u4e00\u81f4\uff09\u548c\u8ba1\u7b97\u66f4\u9ad8\u6548\u7684IA-SEQ-SPEC\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u56fa\u5b9a\u6837\u672c\u91cf\u7684SPEC\u3001SEQ-KMED\u548cSEQ-SLINK\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5728\u5e8f\u5217\u6846\u67b6\u4e0b\u7814\u7a76\u8c31\u805a\u7c7b\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2509.09106", "pdf": "https://arxiv.org/pdf/2509.09106", "abs": "https://arxiv.org/abs/2509.09106", "authors": ["Haokai Su", "Haoxiang Luo", "Shunpeng Yang", "Kaiwen Jiang", "Wei Zhang", "Hua Chen"], "title": "LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots", "categories": ["cs.RO"], "comment": null, "summary": "Achieving stable and robust perceptive locomotion for bipedal robots in\nunstructured outdoor environments remains a critical challenge due to complex\nterrain geometry and susceptibility to external disturbances. In this work, we\npropose a novel reward design inspired by the Linear Inverted Pendulum Model\n(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM\nprovides theoretical guidance for dynamic balance by regulating the center of\nmass (CoM) height and the torso orientation. These are key factors for\nterrain-aware locomotion, as they help ensure a stable viewpoint for the\nrobot's camera. Building on this insight, we design a reward function that\npromotes balance and dynamic stability while encouraging accurate CoM\ntrajectory tracking. To adaptively trade off between velocity tracking and\nstability, we leverage the Reward Fusion Module (RFM) approach that prioritizes\nstability when needed. A double-critic architecture is adopted to separately\nevaluate stability and locomotion objectives, improving training efficiency and\nrobustness. We validate our approach through extensive experiments on a bipedal\nrobot in both simulation and real-world outdoor environments. The results\ndemonstrate superior terrain adaptability, disturbance rejection, and\nconsistent performance across a wide range of speeds and perceptual conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLIPM\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u7528\u4e8e\u63d0\u5347\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u611f\u77e5\u548c\u7a33\u5b9a\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u56e0\u5730\u5f62\u590d\u6742\u6027\u548c\u5916\u90e8\u5e72\u6270\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528LIPM\u7406\u8bba\u8bbe\u8ba1\u4e86\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408RFM\u6a21\u5757\u548c\u53cc\u8bc4\u4ef7\u5668\u67b6\u6784\uff0c\u4f18\u5316\u5e73\u8861\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u5907\u8f83\u5f3a\u7684\u5730\u5f62\u9002\u5e94\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u7a33\u5b9a\u6027\u548c\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.09147", "pdf": "https://arxiv.org/pdf/2509.09147", "abs": "https://arxiv.org/abs/2509.09147", "authors": ["Ziqi Yan", "Zhichao Zhang"], "title": "JFRFFNet: A Data-Model Co-Driven Graph Signal Denoising Model with Partial Prior Information", "categories": ["eess.SP"], "comment": null, "summary": "Wiener filtering in the joint time-vertex fractional Fourier transform\n(JFRFT) domain has shown high effectiveness in denoising time-varying graph\nsignals. Traditional filtering models use grid search to determine the\ntransform-order pair and compute filter coefficients, while learnable ones\nemploy gradient-descent strategies to optimize them; both require complete\nprior information of graph signals. To overcome this shortcoming, this letter\nproposes a data-model co-driven denoising approach, termed neural-network-aided\njoint time-vertex fractional Fourier filtering (JFRFFNet), which embeds the\nJFRFT-domain Wiener filter model into a neural network and updates the\ntransform-order pair and filter coefficients through a data-driven approach.\nThis design enables effective denoising using only partial prior information.\nExperiments demonstrate that JFRFFNet achieves significant improvements in\noutput signal-to-noise ratio compared with some state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e-\u6a21\u578b\u534f\u540c\u9a71\u52a8\u7684\u53bb\u566a\u65b9\u6cd5JFRFFNet\uff0c\u5728JFRFT\u57df\u4e2d\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u6ee4\u6ce2\u53c2\u6570\uff0c\u4ec5\u9700\u90e8\u5206\u5148\u9a8c\u4fe1\u606f\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u53bb\u566a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u7684\u5148\u9a8c\u4fe1\u606f\u4e14\u4f9d\u8d56\u7f51\u683c\u641c\u7d22\u6216\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u53c2\u6570\uff0cJFRFFNet\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5c06JFRFT\u57df\u7684\u7ef4\u7eb3\u6ee4\u6ce2\u6a21\u578b\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u66f4\u65b0\u53d8\u6362-\u9636\u5bf9\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJFRFFNet\u5728\u8f93\u51fa\u4fe1\u566a\u6bd4\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "JFRFFNet\u4e3a\u65f6\u53d8\u56fe\u4fe1\u53f7\u53bb\u566a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.09141", "pdf": "https://arxiv.org/pdf/2509.09141", "abs": "https://arxiv.org/abs/2509.09141", "authors": ["Jianping Li", "Xinhang Xu", "Zhongyuan Liu", "Shenghai Yuan", "Muqing Cao", "Lihua Xie"], "title": "AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes", "categories": ["cs.RO"], "comment": null, "summary": "LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)\nare fundamentally limited by the narrow field of view (FoV) of compact LiDAR\nsensors and the payload constraints that preclude multi-sensor configurations.\nTraditional motorized scanning systems with fixed-speed rotations lack scene\nawareness and task-level adaptability, leading to degraded odometry and mapping\nperformance in complex, occluded environments. Inspired by the active sensing\nbehavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),\na biologically inspired and computationally efficient framework for adaptive\nLiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model\npredictive control (MPC) and reinforcement learning (RL) in a hybrid\narchitecture: an analytical uncertainty model predicts future pose\nobservability for exploitation, while a lightweight neural network learns an\nimplicit cost map from panoramic depth representations to guide exploration. To\nsupport scalable training and generalization, we develop a point cloud-based\nsimulation environment with real-world LiDAR maps across diverse scenes,\nenabling sim-to-real transfer. Extensive experiments in both simulation and\nreal-world environments demonstrate that AEOS significantly improves odometry\naccuracy compared to fixed-rate, optimization-only, and fully learned\nbaselines, while maintaining real-time performance under onboard computational\nconstraints. The project page can be found at\nhttps://kafeiyin00.github.io/AEOS/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAEOS\u7684\u81ea\u9002\u5e94LiDAR\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408MPC\u548cRL\uff0c\u63d0\u5347\u4e86\u65e0\u4eba\u673aLiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709LiDAR\u7cfb\u7edf\u5728\u65e0\u4eba\u673a\u4e0a\u56e0\u7a84\u89c6\u573a\u548c\u8d1f\u8f7d\u9650\u5236\u8868\u73b0\u53d7\u9650\uff0c\u7f3a\u4e4f\u573a\u666f\u610f\u8bc6\u548c\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u590d\u6742\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\u9884\u6d4b\u672a\u6765\u4f4d\u59ff\u53ef\u89c2\u6d4b\u6027\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6df1\u5ea6\u8868\u793a\u4ee5\u5f15\u5bfc\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAEOS\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "AEOS\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u81ea\u9002\u5e94LiDAR\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673aLiDAR\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.09225", "pdf": "https://arxiv.org/pdf/2509.09225", "abs": "https://arxiv.org/abs/2509.09225", "authors": ["Lin Jin", "Hang Sheng", "Hui Feng", "Bo Hu"], "title": "On Sampling of Multiple Correlated Stochastic Signals", "categories": ["eess.SP"], "comment": null, "summary": "Multiple stochastic signals possess inherent statistical correlations, yet\nconventional sampling methods that process each channel independently result in\ndata redundancy. To leverage this correlation for efficient sampling, we model\ncorrelated channels as a linear combination of a smaller set of uncorrelated,\nwide-sense stationary latent sources. We establish a theoretical lower bound on\nthe total sampling density for zero mean-square error reconstruction, proving\nit equals the ratio of the joint spectral bandwidth of latent sources to the\nnumber of correlated signal channels. We then develop a constructive multi-band\nsampling scheme that attains this bound. The proposed method operates via\nspectral partitioning of the latent sources, followed by spatio-temporal\nsampling and interpolation. Experiments on synthetic and real datasets confirm\nthat our scheme achieves near-lossless reconstruction precisely at the\ntheoretical sampling density, validating its efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4fe1\u53f7\u76f8\u5173\u6027\u7684\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u76f8\u5173\u4fe1\u9053\u4e3a\u5c11\u6570\u4e0d\u76f8\u5173\u6f5c\u5728\u6e90\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u4e0b\u9650\u7684\u91c7\u6837\u5bc6\u5ea6\u3002", "motivation": "\u4f20\u7edf\u72ec\u7acb\u91c7\u6837\u65b9\u6cd5\u5bfc\u81f4\u6570\u636e\u5197\u4f59\uff0c\u5229\u7528\u4fe1\u53f7\u76f8\u5173\u6027\u53ef\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u3002", "method": "\u5c06\u76f8\u5173\u4fe1\u9053\u5efa\u6a21\u4e3a\u4e0d\u76f8\u5173\u6f5c\u5728\u6e90\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6ce2\u6bb5\u91c7\u6837\u65b9\u6848\uff0c\u5305\u62ec\u5149\u8c31\u5206\u533a\u3001\u65f6\u7a7a\u91c7\u6837\u548c\u63d2\u503c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u91c7\u6837\u5bc6\u5ea6\u4e0b\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u91cd\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9a8c\u8bc1\u4e86\u7406\u8bba\u4e0b\u9650\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.09206", "pdf": "https://arxiv.org/pdf/2509.09206", "abs": "https://arxiv.org/abs/2509.09206", "authors": ["Farhad Nawaz", "Faizan M. Tariq", "Sangjae Bae", "David Isele", "Avinash Singh", "Nadia Figueroa", "Nikolai Matni", "Jovin D'sa"], "title": "Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Accurately reasoning about future parking spot availability and integrated\nplanning is critical for enabling safe and efficient autonomous valet parking\nin dynamic, uncertain environments. Unlike existing methods that rely solely on\ninstantaneous observations or static assumptions, we present an approach that\npredicts future parking spot occupancy by explicitly distinguishing between\ninitially vacant and occupied spots, and by leveraging the predicted motion of\ndynamic agents. We introduce a probabilistic spot occupancy estimator that\nincorporates partial and noisy observations within a limited Field-of-View\n(FoV) model and accounts for the evolving uncertainty of unobserved regions.\nCoupled with this, we design a strategy planner that adaptively balances\ngoal-directed parking maneuvers with exploratory navigation based on\ninformation gain, and intelligently incorporates wait-and-go behaviors at\npromising spots. Through randomized simulations emulating large parking lots,\nwe demonstrate that our framework significantly improves parking efficiency,\nsafety margins, and trajectory smoothness compared to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u533a\u5206\u521d\u59cb\u7a7a\u7f6e\u548c\u5360\u7528\u505c\u8f66\u4f4d\u5e76\u7ed3\u5408\u52a8\u6001\u4ee3\u7406\u9884\u6d4b\u8fd0\u52a8\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u5ba2\u6cca\u8f66\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\uff0c\u51c6\u786e\u9884\u6d4b\u505c\u8f66\u4f4d\u53ef\u7528\u6027\u548c\u89c4\u5212\u662f\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u81ea\u4e3b\u4ee3\u5ba2\u6cca\u8f66\u7684\u5173\u952e\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u77ac\u65f6\u89c2\u5bdf\u6216\u9759\u6001\u5047\u8bbe\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6982\u7387\u505c\u8f66\u4f4d\u5360\u7528\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408\u6709\u9650\u89c6\u91ce\u5185\u7684\u90e8\u5206\u548c\u566a\u58f0\u89c2\u5bdf\uff0c\u5e76\u8003\u8651\u672a\u89c2\u5bdf\u533a\u57df\u7684\u6f14\u5316\u4e0d\u786e\u5b9a\u6027\uff1b\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b56\u7565\u89c4\u5212\u5668\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u76ee\u6807\u5bfc\u5411\u6cca\u8f66\u548c\u63a2\u7d22\u5bfc\u822a\uff0c\u5e76\u667a\u80fd\u6574\u5408\u7b49\u5f85\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5927\u578b\u505c\u8f66\u573a\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u6cca\u8f66\u6548\u7387\u3001\u5b89\u5168\u8fb9\u9645\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u9884\u6d4b\u548c\u9002\u5e94\u6027\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u4ee3\u5ba2\u6cca\u8f66\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.09264", "pdf": "https://arxiv.org/pdf/2509.09264", "abs": "https://arxiv.org/abs/2509.09264", "authors": ["Davoud Hajhassani", "Quentin Barth\u00e9lemy", "J\u00e9r\u00e9mie Mattout", "Marco Congedo"], "title": "Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG", "categories": ["eess.SP"], "comment": null, "summary": "Electroencephalography (EEG) signal cleaning has long been a critical\nchallenge in the research community. The presence of artifacts can\nsignificantly degrade EEG data quality, complicating analysis and potentially\nleading to erroneous interpretations. While various artifact rejection methods\nhave been proposed, the gold standard remains manual visual inspection by human\nexperts-a process that is time-consuming, subjective, and impractical for\nlarge-scale EEG studies. Existing techniques are often hindered by a strong\nreliance on manual hyperparameter tuning, sensitivity to outliers, and high\ncomputational costs. In this paper, we introduce the improved Riemannian Potato\nField (iRPF), a fast and fully automated method for EEG artifact rejection that\naddresses key limitations of current approaches. We evaluate iRPF against\nseveral state-of-the-art artifact rejection methods, using two publicly\navailable EEG databases, labeled for various artifact types, comprising 226 EEG\nrecordings. Our results demonstrate that iRPF outperforms all competitors\nacross multiple metrics, with gains of up to 22% in recall, 102% in\nspecificity, 54% in precision, and 24% in F1-score, compared to Isolation\nForest, Autoreject, Riemannian Potato, and Riemannian Potato Field,\nrespectively. Statistical analysis confirmed the significance of these\nimprovements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most\ncomparisons. Additionally, on a typical EEG recording iRPF performs artifact\ncleaning in under 8 milliseconds per epoch using a standard laptop,\nhighlighting its efficiency for large-scale EEG data processing and real-time\napplications. iRPF offers a robust and data-driven artifact rejection solution\nfor high-quality EEG pre-processing in brain-computer interfaces and clinical\nneuroimaging applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Riemannian Potato Field (iRPF)\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u81ea\u52a8\u53bb\u9664EEG\u4fe1\u53f7\u4e2d\u7684\u4f2a\u8ff9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "EEG\u4fe1\u53f7\u4e2d\u7684\u4f2a\u8ff9\u4f1a\u964d\u4f4e\u6570\u636e\u8d28\u91cf\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8c03\u53c2\u3001\u8017\u65f6\u4e14\u4e0d\u7a33\u5b9a\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6539\u8fdb\u7684Riemannian Potato Field (iRPF)\uff0c\u5b8c\u5168\u81ea\u52a8\u5316\u4e14\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21EEG\u6570\u636e\u5904\u7406\u3002", "result": "iRPF\u5728\u53ec\u56de\u7387\u3001\u7279\u5f02\u6027\u3001\u7cbe\u786e\u5ea6\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u63d0\u534722%\u3001102%\u300154%\u548c24%\uff0c\u4e14\u5904\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "iRPF\u4e3a\u8111\u673a\u63a5\u53e3\u548c\u4e34\u5e8a\u795e\u7ecf\u5f71\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u4f2a\u8ff9\u53bb\u9664\u65b9\u6848\u3002"}}
{"id": "2509.09283", "pdf": "https://arxiv.org/pdf/2509.09283", "abs": "https://arxiv.org/abs/2509.09283", "authors": ["Yueqi Zhang", "Quancheng Qian", "Taixian Hou", "Peng Zhai", "Xiaoyi Wei", "Kangmai Hu", "Jiafu Yi", "Lihua Zhang"], "title": "RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse", "categories": ["cs.RO"], "comment": "Accepted for IEEE Robotics and Automation Letters (RA-L)", "summary": "Vision-based locomotion in outdoor environments presents significant\nchallenges for quadruped robots. Accurate environmental prediction and\neffective handling of depth sensor noise during real-world deployment remain\ndifficult, severely restricting the outdoor applications of such algorithms. To\naddress these deployment challenges in vision-based motion control, this letter\nproposes the Redundant Estimator Network (RENet) framework. The framework\nemploys a dual-estimator architecture that ensures robust motion performance\nwhile maintaining deployment stability during onboard vision failures. Through\nan online estimator adaptation, our method enables seamless transitions between\nestimation modules when handling visual perception uncertainties. Experimental\nvalidation on a real-world robot demonstrates the framework's effectiveness in\ncomplex outdoor environments, showing particular advantages in scenarios with\ndegraded visual perception. This framework demonstrates its potential as a\npractical solution for reliable robotic deployment in challenging field\nconditions. Project website: https://RENet-Loco.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5197\u4f59\u4f30\u8ba1\u7f51\u7edc\uff08RENet\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u6237\u5916\u73af\u5883\u4e2d\u57fa\u4e8e\u89c6\u89c9\u7684\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6df1\u5ea6\u4f20\u611f\u5668\u566a\u58f0\u548c\u73af\u5883\u9884\u6d4b\u65b9\u9762\u7684\u56f0\u96be\u3002", "motivation": "\u5728\u6237\u5916\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u9762\u4e34\u73af\u5883\u9884\u6d4b\u4e0d\u51c6\u786e\u548c\u6df1\u5ea6\u4f20\u611f\u5668\u566a\u58f0\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u53cc\u4f30\u8ba1\u5668\u67b6\u6784\u548c\u5728\u7ebf\u4f30\u8ba1\u5668\u81ea\u9002\u5e94\uff0cRENet\u6846\u67b6\u5728\u89c6\u89c9\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u65f6\u5b9e\u73b0\u6a21\u5757\u95f4\u7684\u65e0\u7f1d\u5207\u6362\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u611f\u77e5\u9000\u5316\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "RENet\u6846\u67b6\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u89c6\u89c9\u63a7\u5236\u7684\u53ef\u9760\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2509.09282", "pdf": "https://arxiv.org/pdf/2509.09282", "abs": "https://arxiv.org/abs/2509.09282", "authors": ["Leonardo M\u00f6rlein", "Dirk Manteuffel"], "title": "On the Relation of Characteristic Modes of Different Conducting Structures", "categories": ["eess.SP"], "comment": null, "summary": "A formalism is derived to analyze the scattering of a conducting structure\nbased on the characteristic modes of another structure whose surface is a\nsuperset of the first structure. This enables the analysis and comparison of\ndifferent structures using a common basis of characteristic modes.\nAdditionally, it is shown that the scattering matrices and perturbation\nmatrices are no longer diagonal in these cases. Based on this, a modal\ntransformation matrix is defined to describe the mapping between the\ncharacteristic fields and the weighting coefficients of the two structures.\nThis matrix enables the conversion of the perturbation matrices in different\nbases. Finally, two examples are provided along with a discussion of some\naspects of the theory. The first example aims to validate and illustrate the\nformalism. The second example shows how the formalism can be applied in the\ndesign process of an antenna element that is gradually modified, starting from\na base structure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u6a21\u5f0f\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u5bfc\u4f53\u7684\u6563\u5c04\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6a21\u6001\u8f6c\u6362\u77e9\u9635\u5b9e\u73b0\u4e0d\u540c\u7ed3\u6784\u4e4b\u95f4\u7684\u6bd4\u8f83\u548c\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u5bfc\u4f53\u7ed3\u6784\u7684\u6563\u5c04\u7279\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7279\u5f81\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6a21\u6001\u8f6c\u6362\u77e9\u9635\u7b80\u5316\u4e86\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u6a21\u6001\u8f6c\u6362\u77e9\u9635\uff0c\u5c06\u4e0d\u540c\u7ed3\u6784\u7684\u7279\u5f81\u573a\u4e0e\u6743\u91cd\u7cfb\u6570\u6620\u5c04\u8d77\u6765\uff0c\u4ece\u800c\u5206\u6790\u6563\u5c04\u77e9\u9635\u548c\u6270\u52a8\u77e9\u9635\u7684\u975e\u5bf9\u89d2\u6027\u8d28\u3002", "result": "\u7406\u8bba\u5f97\u5230\u4e86\u4e24\u4e2a\u4f8b\u5b50\u7684\u9a8c\u8bc1\uff1a\u4e00\u662f\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u4e8c\u662f\u5c55\u793a\u5982\u4f55\u5728\u5929\u7ebf\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u5e94\u7528\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u5f0f\u4e3b\u4e49\u65b9\u6cd5\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u7406\u8bba\u7684\u53ef\u884c\u6027\uff0c\u8fd8\u4e3a\u5929\u7ebf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2509.09332", "pdf": "https://arxiv.org/pdf/2509.09332", "abs": "https://arxiv.org/abs/2509.09332", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yuzheng Zhuang", "Bowen Yang", "He Zhu", "Lingfeng Zhang", "Pengwei Xie", "David Gamaliel Arcos Bravo", "Yingxue Zhang", "Jianye Hao", "Xingyue Quan"], "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io", "AI": {"tldr": "OmniEVA\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u76843D\u63a5\u5730\u673a\u5236\u548c\u4f53\u73b0\u611f\u77e5\u63a8\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u9002\u5e94\u6027\u548c\u4f53\u73b0\u9650\u5236\u4e0a\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u4f53\u73b0\u63a8\u7406\u548c\u4efb\u52a1\u89c4\u5212\u3002", "motivation": "\u5f53\u524dMLLM-based systems\u5b58\u5728\u51e0\u4f55\u9002\u5e94\u6027\u5dee\u8ddd\u548c\u4f53\u73b0\u7ea6\u675f\u5dee\u8ddd\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faTask-Adaptive 3D Grounding\u673a\u5236\u548cEmbodiment-Aware Reasoning\u6846\u67b6\uff0c\u5206\u522b\u4f18\u53163D\u7a7a\u95f4\u7684\u9002\u5e94\u6027\u6574\u5408\u548c\u4efb\u52a1\u89c4\u5212\u7684\u53ef\u6267\u884c\u6027\u3002", "result": "OmniEVA\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5f00\u521b\u4e86\u4f53\u73b0\u63a8\u7406\u7684\u65b0\u6807\u6746\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u548c\u901a\u7528\u7684\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "OmniEVA\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f53\u73b0\u667a\u80fd\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.09373", "pdf": "https://arxiv.org/pdf/2509.09373", "abs": "https://arxiv.org/abs/2509.09373", "authors": ["Huayan Guo", "Jichen Zhang", "Junhui Rao", "Ross Murch", "Vincent K. N. Lau"], "title": "Channel Estimation and Analog Precoding for Pixel-based Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems", "categories": ["eess.SP"], "comment": "13 pages, 12 figures", "summary": "Pixel-based fluid antennas provide enhanced multiplexing gains and quicker\nradiation pattern switching than traditional designs. However, this innovation\nintroduces challenges for channel estimation and analog precoding due to the\nstate-non-separable channel response problem. This paper explores a multiuser\nMIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements\nfrom a real-world prototype. We present a sparse channel recovery framework for\nuplink channel sounding, employing an approximate separable channel response\nmodel with DNN-based antenna radiation functions. We then propose two\nlow-complexity channel estimation algorithms that leverage orthogonal matching\npursuit and variational Bayesian inference to accurately recover channel\nresponses across various scattering cluster angles. These estimations enable\nthe prediction of composite channels for all fluid antenna states, leading to\nan analog precoding scheme that optimally selects switching states for\ndifferent antennas. Our simulation results indicate that the proposed approach\nsignificantly outperforms several baseline methods, especially in high\nsignal-to-noise ratio environments with numerous users.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u50cf\u7d20\u7684\u6d41\u4f53\u5929\u7ebf\u5728MIMO-OFDM\u7cfb\u7edf\u4e2d\u7684\u4fe1\u9053\u4f30\u8ba1\u548c\u6a21\u62df\u9884\u7f16\u7801\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7a00\u758f\u4fe1\u9053\u6062\u590d\u6846\u67b6\u548c\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6d41\u4f53\u5929\u7ebf\u5e26\u6765\u7684\u4fe1\u9053\u54cd\u5e94\u4e0d\u53ef\u5206\u79bb\u95ee\u9898\uff0c\u4f18\u5316\u4fe1\u9053\u4f30\u8ba1\u548c\u6a21\u62df\u9884\u7f16\u7801\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8fd1\u4f3c\u53ef\u5206\u79bb\u4fe1\u9053\u54cd\u5e94\u6a21\u578b\u548c\u57fa\u4e8eDNN\u7684\u5929\u7ebf\u8f90\u5c04\u51fd\u6570\uff0c\u63d0\u51fa\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a\u548c\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u7b97\u6cd5\u3002", "result": "\u5728\u9ad8\u4fe1\u566a\u6bd4\u548c\u591a\u7528\u6237\u573a\u666f\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7a00\u758f\u4fe1\u9053\u6062\u590d\u6846\u67b6\u548c\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u6d41\u4f53\u5929\u7ebf\u7684\u6311\u6218\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.09364", "pdf": "https://arxiv.org/pdf/2509.09364", "abs": "https://arxiv.org/abs/2509.09364", "authors": ["Grzegorz Ficht", "Luis Denninger", "Sven Behnke"], "title": "AGILOped: Agile Open-Source Humanoid Robot for Research", "categories": ["cs.RO"], "comment": "10th IEEE International Conference on Advanced Robotics and\n  Mechatronics (ARM), Portsmouth, UK, August 2025", "summary": "With academic and commercial interest for humanoid robots peaking, multiple\nplatforms are being developed. Through a high level of customization, they\nshowcase impressive performance. Most of these systems remain closed-source or\nhave high acquisition and maintenance costs, however. In this work, we present\nAGILOped - an open-source humanoid robot that closes the gap between high\nperformance and accessibility. Our robot is driven by off-the-shelf\nbackdrivable actuators with high power density and uses standard electronic\ncomponents. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be\noperated without a gantry by a single person. Experiments in walking, jumping,\nimpact mitigation and getting-up demonstrate its viability for use in research.", "AI": {"tldr": "AGILOped\u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u586b\u8865\u4e86\u9ad8\u6027\u80fd\u4e0e\u6613\u7528\u6027\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9ad8\u6027\u80fd\u4eba\u5f62\u673a\u5668\u4eba\u5c01\u95ed\u6e90\u4ee3\u7801\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u73b0\u6210\u7684\u53ef\u9006\u9a71\u52a8\u5668\u548c\u6807\u51c6\u7535\u5b50\u7ec4\u4ef6\u3002", "result": "AGILOped\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u884c\u8d70\u3001\u8df3\u8dc3\u7b49\u7814\u7a76\u573a\u666f\u3002", "conclusion": "AGILOped\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u4e14\u7ecf\u6d4e\u7684\u9ad8\u6027\u80fd\u5e73\u53f0\u3002"}}
{"id": "2509.09606", "pdf": "https://arxiv.org/pdf/2509.09606", "abs": "https://arxiv.org/abs/2509.09606", "authors": ["Sajjad Hussain"], "title": "A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction in UAV-Assisted mmWave Radio Networks", "categories": ["eess.SP"], "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Accurate pathloss prediction is essential for the design and optimization of\nUAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches\nhave shown strong potential, their generalization across diverse environments,\nrobustness to noisy inputs, and sensitivity to UAV altitude remain\nunderexplored. To address these challenges, we propose a UNet-based deep\nlearning architecture that combines multi-scale feature extraction,\nconvolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP)\nbottleneck for efficient context aggregation. The model predicts pathloss maps\nfrom log-distance, line-of-sight (LOS) mask, and building mask inputs. In\naddition, we develop a fully vectorized LOS mask computation algorithm that\nsignificantly accelerates pre-processing and enables large-scale dataset\ngeneration. Extensive evaluations on both in-house ray-tracing data and the\nRadioMapSeer benchmark demonstrate that the proposed model outperforms several\nstate-of-the-art baselines in accuracy and efficiency. All source code is\npublicly released to support reproducibility and future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUNet\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u9884\u6d4b\u65e0\u4eba\u673a\u8f85\u52a9\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u7684\u8def\u5f84\u635f\u8017\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u7684\u8def\u5f84\u635f\u8017\u9884\u6d4b\u5bf9\u4e8e\u65e0\u4eba\u673a\u8f85\u52a9\u6beb\u7c73\u6ce2\u7f51\u7edc\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6837\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3001\u6297\u566a\u8f93\u5165\u80fd\u529b\u548c\u5bf9\u65e0\u4eba\u673a\u9ad8\u5ea6\u7684\u654f\u611f\u6027\u65b9\u9762\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u5377\u79ef\u7279\u5f81\u878d\u5408\u548cASPP\u74f6\u9888\u7684UNet\u67b6\u6784\uff0c\u8f93\u5165\u5305\u62ec\u5bf9\u6570\u8ddd\u79bb\u3001\u89c6\u7ebf\u63a9\u7801\u548c\u5efa\u7b51\u7269\u63a9\u7801\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u5411\u91cf\u5316\u89c6\u7ebf\u63a9\u7801\u8ba1\u7b97\u7b97\u6cd5\uff0c\u52a0\u901f\u9884\u5904\u7406\u3002", "result": "\u5728\u5ba4\u5185\u5c04\u7ebf\u8ffd\u8e2a\u6570\u636e\u548cRadioMapSeer\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u5f84\u635f\u8017\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u516c\u5f00\u4e86\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u590d\u73b0\u548c\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.09372", "pdf": "https://arxiv.org/pdf/2509.09372", "abs": "https://arxiv.org/abs/2509.09372", "authors": ["Yihao Wang", "Pengxiang Ding", "Lingxiao Li", "Can Cui", "Zirui Ge", "Xinyang Tong", "Wenxuan Song", "Han Zhao", "Wei Zhao", "Pengxu Hou", "Siteng Huang", "Yifan Tang", "Wenhui Wang", "Ru Zhang", "Jianyi Liu", "Donglin Wang"], "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.", "AI": {"tldr": "VLA-Adapter\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u6865\u63a5\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u51cf\u5c11\u5bf9\u5927\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u5927\u91cf\u9884\u8bad\u7ec3\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u9ad8\u6548\u6865\u63a5\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u4e0e\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ee5\u51cf\u5c11\u73b0\u6709Vision-Language-Action\u6a21\u578b\u5bf9\u5927\u6a21\u578b\u548c\u9ad8\u9884\u8bad\u7ec3\u6210\u672c\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faVLA-Adapter\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5173\u952e\u6761\u4ef6\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u5757\uff08Bridge Attention\uff09\uff0c\u81ea\u4e3b\u6ce8\u5165\u6700\u4f18\u6761\u4ef6\u5230\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLA-Adapter\u4e0d\u4ec5\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4e14\u4ec5\u97000.5B\u53c2\u6570\u4e3b\u5e72\uff0c\u65e0\u9700\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u5355\u6d88\u8d39\u7ea7GPU\u4e0a8\u5c0f\u65f6\u5b8c\u6210\u8bad\u7ec3\u3002", "conclusion": "VLA-Adapter\u901a\u8fc7\u9ad8\u6548\u6865\u63a5\u8303\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u90e8\u7f72VLA\u6a21\u578b\u7684\u95e8\u69db\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u5feb\u901f\u63a8\u7406\u3002"}}
{"id": "2509.08956", "pdf": "https://arxiv.org/pdf/2509.08956", "abs": "https://arxiv.org/abs/2509.08956", "authors": ["Luke Snow", "Vikram Krishnamurthy"], "title": "Multi-Agent Inverse Reinforcement Learning for Identifying Pareto-Efficient Coordination -- A Distributionally Robust Approach", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": null, "summary": "Multi-agent inverse reinforcement learning (IRL) aims to identify\nPareto-efficient behavior in a multi-agent system, and reconstruct utility\nfunctions of the individual agents. Motivated by the problem of detecting UAV\ncoordination, how can we construct a statistical detector for Pareto-efficient\nbehavior given noisy measurements of the decisions of a multi-agent system?\nThis paper approaches this IRL problem by deriving necessary and sufficient\nconditions for a dataset of multi-agent system dynamics to be consistent with\nPareto-efficient coordination, and providing algorithms for recovering utility\nfunctions which are consistent with the system dynamics. We derive an optimal\nstatistical detector for determining Pareto-efficient coordination from noisy\nsystem measurements, which minimizes Type-I statistical detection error. Then,\nwe provide a utility estimation algorithm which minimizes the worst-case\nestimation error over a statistical ambiguity set centered at empirical\nobservations; this min-max solution achieves distributionally robust IRL, which\nis crucial in adversarial strategic interactions. We illustrate these results\nin a detailed example for detecting Pareto-efficient coordination among\nmultiple UAVs given noisy measurement recorded at a radar. We then reconstruct\nthe utility functions of the UAVs in a distributionally robust sense.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u95ee\u9898\uff0c\u65e8\u5728\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u68c0\u6d4bPareto\u6709\u6548\u7684\u534f\u8c03\u884c\u4e3a\uff0c\u5e76\u91cd\u5efa\u667a\u80fd\u4f53\u7684\u6548\u7528\u51fd\u6570\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u65e0\u4eba\u673a\uff08UAV\uff09\u534f\u8c03\u884c\u4e3a\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u5373\u5982\u4f55\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u566a\u58f0\u6d4b\u91cf\u6570\u636e\u4e0b\uff0c\u6784\u5efa\u4e00\u4e2a\u7edf\u8ba1\u68c0\u6d4b\u5668\u6765\u8bc6\u522bPareto\u6709\u6548\u7684\u534f\u8c03\u884c\u4e3a\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff1a\u9996\u5148\u63a8\u5bfc\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u52a8\u6001\u6570\u636e\u4e0ePareto\u6709\u6548\u534f\u8c03\u884c\u4e3a\u4e00\u81f4\u7684\u5fc5\u8981\u548c\u5145\u5206\u6761\u4ef6\uff1b\u7136\u540e\u8bbe\u8ba1\u4e86\u6700\u4f18\u7edf\u8ba1\u68c0\u6d4b\u5668\uff0c\u6700\u5c0f\u5316\u7edf\u8ba1\u68c0\u6d4b\u7684Type-I\u9519\u8bef\uff1b\u5e76\u63d0\u4f9b\u4e00\u4e2a\u6548\u7528\u4f30\u8ba1\u7b97\u6cd5\uff0c\u6700\u5c0f\u5316\u5728\u7edf\u8ba1\u6a21\u7cca\u96c6\u4e0b\u7684\u6700\u574f\u4f30\u8ba1\u8bef\u5dee\uff0c\u5b9e\u73b0\u5206\u5e03\u9c81\u68d2\u7684IRL\u3002", "result": "\u8bba\u6587\u5b9e\u73b0\u4e86\u5728\u566a\u58f0\u96f7\u8fbe\u6d4b\u91cf\u4e0b\u68c0\u6d4b\u65e0\u4eba\u673aPareto\u6709\u6548\u534f\u8c03\u884c\u4e3a\u7684\u793a\u4f8b\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u7684\u65b9\u5f0f\u91cd\u5efa\u4e86\u65e0\u4eba\u673a\u7684\u6548\u7528\u51fd\u6570\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53IRL\u4e2dPareto\u6709\u6548\u884c\u4e3a\u7684\u68c0\u6d4b\u548c\u6548\u7528\u51fd\u6570\u91cd\u5efa\u95ee\u9898\uff0c\u4e3a\u5bf9\u6297\u6027\u6218\u7565\u4e92\u52a8\u63d0\u4f9b\u4e86\u5206\u5e03\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09404", "pdf": "https://arxiv.org/pdf/2509.09404", "abs": "https://arxiv.org/abs/2509.09404", "authors": ["Tongshun Chen", "Zezhou Sun", "Yanhan Sun", "Yuhao Wang", "Dezhen Song", "Ke Wu"], "title": "A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness", "categories": ["cs.RO"], "comment": null, "summary": "Cable-driven continuum robots offer high flexibility and lightweight design,\nmaking them well-suited for tasks in constrained and unstructured environments.\nHowever, prolonged use can induce mechanical fatigue from plastic deformation\nand material degradation, compromising performance and risking structural\nfailure. In the state of the art, fatigue estimation of continuum robots\nremains underexplored, limiting long-term operation. To address this, we\npropose a fatigue-aware continuum robot with three key innovations: (1) a\nHybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and\nbending: passive revolute joints in the BendBeam mitigate stress concentration,\nwhile TwistBeam's limited torsional deformation reduces BendBeam stress\nmagnitude, enhancing durability; (2) a Passive Stopper that safely constrains\nmotion via mechanical constraints and employs motor torque sensing to detect\ncorresponding limit torque, ensuring safety and enabling data collection; and\n(3) a real-time fatigue-awareness method that estimates stiffness from motor\ntorque at the limit pose, enabling online fatigue estimation without additional\nsensors. Experiments show that the proposed design reduces fatigue accumulation\nby about 49% compared with a conventional design, while passive mechanical\nlimiting combined with motor-side sensing allows accurate estimation of\nstructural fatigue and damage. These results confirm the effectiveness of the\nproposed architecture for safe and reliable long-term operation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u75b2\u52b3\u611f\u77e5\u7684\u7535\u7f06\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6df7\u5408\u94f0\u94fe-\u6881\u7ed3\u6784\u548c\u88ab\u52a8\u9650\u4f4d\u5668\u964d\u4f4e\u75b2\u52b3\u79ef\u7d2f49%\uff0c\u5e76\u5b9e\u65f6\u4f30\u8ba1\u75b2\u52b3\u3002", "motivation": "\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5728\u7ea6\u675f\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u957f\u671f\u4f7f\u7528\u4f1a\u5bfc\u81f4\u673a\u68b0\u75b2\u52b3\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u5176\u75b2\u52b3\u4f30\u8ba1\u4e0d\u8db3\uff0c\u5f71\u54cd\u957f\u671f\u64cd\u4f5c\u3002", "method": "\u8bbe\u8ba1\u4e86\u6df7\u5408\u94f0\u94fe-\u6881\u7ed3\u6784\uff08\u89e3\u8026\u626d\u8f6c\u548c\u5f2f\u66f2\uff09\u3001\u88ab\u52a8\u9650\u4f4d\u5668\uff08\u673a\u68b0\u7ea6\u675f\u548c\u626d\u77e9\u68c0\u6d4b\uff09\u3001\u5b9e\u65f6\u75b2\u52b3\u4f30\u8ba1\u65b9\u6cd5\uff08\u901a\u8fc7\u7535\u673a\u626d\u77e9\u4f30\u8ba1\u521a\u5ea6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u8bbe\u8ba1\u51cf\u5c11\u75b2\u52b3\u79ef\u7d2f49%\uff0c\u5e76\u80fd\u51c6\u786e\u4f30\u8ba1\u7ed3\u6784\u75b2\u52b3\u548c\u635f\u4f24\uff0c\u63d0\u5347\u957f\u671f\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u957f\u671f\u64cd\u4f5c\u4e2d\u7684\u75b2\u52b3\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.09053", "pdf": "https://arxiv.org/pdf/2509.09053", "abs": "https://arxiv.org/abs/2509.09053", "authors": ["Julian Oelhaf", "Georg Kordowich", "Mehran Pashaei", "Christian Bergler", "Andreas Maier", "Johann J\u00e4ger", "Siming Bayer"], "title": "A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "The integration of renewable and distributed energy resources reshapes modern\npower systems, challenging conventional protection schemes. This scoping review\nsynthesizes recent literature on machine learning (ML) applications in power\nsystem protection and disturbance management, following the PRISMA for Scoping\nReviews framework. Based on over 100 publications, three key objectives are\naddressed: (i) assessing the scope of ML research in protection tasks; (ii)\nevaluating ML performance across diverse operational scenarios; and (iii)\nidentifying methods suitable for evolving grid conditions. ML models often\ndemonstrate high accuracy on simulated datasets; however, their performance\nunder real-world conditions remains insufficiently validated. The existing\nliterature is fragmented, with inconsistencies in methodological rigor, dataset\nquality, and evaluation metrics. This lack of standardization hampers the\ncomparability of results and limits the generalizability of findings. To\naddress these challenges, this review introduces a ML-oriented taxonomy for\nprotection tasks, resolves key terminological inconsistencies, and advocates\nfor standardized reporting practices. It further provides guidelines for\ncomprehensive dataset documentation, methodological transparency, and\nconsistent evaluation protocols, aiming to improve reproducibility and enhance\nthe practical relevance of research outcomes. Critical gaps remain, including\nthe scarcity of real-world validation, insufficient robustness testing, and\nlimited consideration of deployment feasibility. Future research should\nprioritize public benchmark datasets, realistic validation methods, and\nadvanced ML architectures. These steps are essential to move ML-based\nprotection from theoretical promise to practical deployment in increasingly\ndynamic and decentralized power systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u5728\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u4fdd\u62a4\u548c\u6270\u52a8\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86100\u591a\u7bc7\u6587\u732e\uff0c\u63d0\u51fa\u6807\u51c6\u5316\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u96c6\u6210\u53ef\u518d\u751f\u80fd\u6e90\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u5bf9\u4f20\u7edf\u4fdd\u62a4\u65b9\u6848\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528PRISMA\u6846\u67b6\u8fdb\u884c\u8303\u56f4\u7efc\u8ff0\uff0c\u5206\u6790\u673a\u5668\u5b66\u4e60\u5728\u4fdd\u62a4\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u5206\u7c7b\u6cd5\u548c\u6807\u51c6\u5316\u5efa\u8bae\u3002", "result": "\u673a\u5668\u5b66\u4e60\u5728\u6a21\u62df\u6570\u636e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u6548\u679c\u4e0d\u8db3\uff0c\u6587\u732e\u4e2d\u5b58\u5728\u65b9\u6cd5\u4e0d\u4e00\u81f4\u548c\u6570\u636e\u96c6\u8d28\u91cf\u53c2\u5dee\u7684\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u9700\u5173\u6ce8\u516c\u5f00\u6570\u636e\u96c6\u3001\u5b9e\u9645\u9a8c\u8bc1\u548c\u5148\u8fdb\u67b6\u6784\uff0c\u4ee5\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u5728\u52a8\u6001\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.09484", "pdf": "https://arxiv.org/pdf/2509.09484", "abs": "https://arxiv.org/abs/2509.09484", "authors": ["Peng Zhou", "Jiaming Qi", "Hongmin Wu", "Chen Wang", "Yizhou Chen", "Zeqing Zhang"], "title": "BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Bagging tasks, commonly found in industrial scenarios, are challenging\nconsidering deformable bags' complicated and unpredictable nature. This paper\npresents an automated bagging system from the proposed adaptive\nStructure-of-Interest (SOI) manipulation strategy for dual robot arms. The\nsystem dynamically adjusts its actions based on real-time visual feedback,\nremoving the need for pre-existing knowledge of bag properties. Our framework\nincorporates Gaussian Mixture Models (GMM) for estimating SOI states,\noptimization techniques for SOI generation, motion planning via Constrained\nBidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination\nusing Model Predictive Control (MPC). Extensive experiments validate the\ncapability of our system to perform precise and robust bagging across various\nobjects, showcasing its adaptability. This work offers a new solution for\nrobotic deformable object manipulation (DOM), particularly in automated bagging\ntasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u9002\u5e94SOI\u7b56\u7565\u7684\u53cc\u673a\u5668\u4eba\u81c2\u81ea\u52a8\u88c5\u888b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\u52a8\u6001\u8c03\u6574\u52a8\u4f5c\uff0c\u65e0\u9700\u9884\u5148\u4e86\u89e3\u888b\u5b50\u7279\u6027\u3002", "motivation": "\u9488\u5bf9\u5de5\u4e1a\u573a\u666f\u4e2d\u53ef\u53d8\u5f62\u5bb9\u5668\u7684\u590d\u6742\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u81ea\u9002\u5e94\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u88c5\u888b\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408GMM\u4f30\u8ba1SOI\u72b6\u6001\u3001\u4f18\u5316SOI\u751f\u6210\u3001CBiRRT\u8fd0\u52a8\u89c4\u5212\u548cMPC\u53cc\u81c2\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u4e0d\u540c\u7269\u4f53\u88c5\u888b\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u53ef\u53d8\u5f62\u4f53\u64cd\u4f5c\uff08DOM\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u5316\u88c5\u888b\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2509.09149", "pdf": "https://arxiv.org/pdf/2509.09149", "abs": "https://arxiv.org/abs/2509.09149", "authors": ["Yufan Qian", "Tianshu Qu", "Xihong Wu"], "title": "Automotive sound field reproduction using deep optimization with spatial domain constraint", "categories": ["eess.AS", "eess.SP"], "comment": "41 pages, 9 figures, Revised and submitted to The Journal of the\n  Acoustical Society of America (JASA)", "summary": "Sound field reproduction with undistorted sound quality and precise spatial\nlocalization is desirable for automotive audio systems. However, the complexity\nof automotive cabin acoustic environment often necessitates a trade-off between\nsound quality and spatial accuracy. To overcome this limitation, we propose\nSpatial Power Map Net (SPMnet), a learning-based sound field reproduction\nmethod that improves both sound quality and spatial localization in complex\nenvironments. We introduce a spatial power map (SPM) constraint, which\ncharacterizes the angular energy distribution of the reproduced field using\nbeamforming. This constraint guides energy toward the intended direction to\nenhance spatial localization, and is integrated into a multi-channel\nequalization framework to also improve sound quality under reverberant\nconditions. To address the resulting non-convexity, deep optimization that use\nneural networks to solve optimization problems is employed for filter design.\nBoth in situ objective and subjective evaluations confirm that our method\nenhances sound quality and improves spatial localization within the automotive\ncabin. Furthermore, we analyze the influence of different audio materials and\nthe arrival angles of the virtual sound source in the reproduced sound field,\ninvestigating the potential underlying factors affecting these results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u58f0\u573a\u91cd\u73b0\u65b9\u6cd5SPMnet\uff0c\u65e8\u5728\u89e3\u51b3\u6c7d\u8f66\u97f3\u54cd\u7cfb\u7edf\u4e2d\u58f0\u97f3\u8d28\u91cf\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u7684\u6743\u8861\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u529f\u7387\u56fe\u7ea6\u675f\u5e76\u7ed3\u5408\u591a\u901a\u9053\u5747\u8861\u6280\u672f\uff0c\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u6539\u5584\u4e86\u58f0\u97f3\u8d28\u91cf\u548c\u7a7a\u95f4\u5b9a\u4f4d\u3002", "motivation": "\u6c7d\u8f66\u5ea7\u8231\u7684\u590d\u6742\u58f0\u5b66\u73af\u5883\u5bfc\u81f4\u58f0\u97f3\u8d28\u91cf\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u63d0\u5347\u4e24\u8005\u3002", "method": "\u63d0\u51faSPMnet\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a7a\u95f4\u529f\u7387\u56fe\u7ea6\u675f\u548c\u591a\u901a\u9053\u5747\u8861\u6280\u672f\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4ee5\u8bbe\u8ba1\u6ee4\u6ce2\u5668\u3002", "result": "\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6c7d\u8f66\u5ea7\u8231\u5185\u7684\u58f0\u97f3\u8d28\u91cf\u548c\u7a7a\u95f4\u5b9a\u4f4d\u6548\u679c\u3002", "conclusion": "SPMnet\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u58f0\u97f3\u8d28\u91cf\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u97f3\u9891\u6750\u6599\u548c\u865a\u62df\u58f0\u6e90\u89d2\u5ea6\uff0c\u63a2\u8ba8\u4e86\u5f71\u54cd\u7ed3\u679c\u7684\u56e0\u7d20\u3002"}}
{"id": "2509.09509", "pdf": "https://arxiv.org/pdf/2509.09509", "abs": "https://arxiv.org/abs/2509.09509", "authors": ["Pedro Miguel Bastos Soares", "Ali Tourani", "Miguel Fernandez-Cortizas", "Asier Bikandi Noya", "Jose Luis Sanchez-Lopez", "Holger Voos"], "title": "SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking", "categories": ["cs.RO"], "comment": "12 pages, 6 figures, 5 tables", "summary": "Advancing research in fields like Simultaneous Localization and Mapping\n(SLAM) and autonomous navigation critically depends on reliable and\nreproducible multimodal datasets. While several influential datasets have\ndriven progress in these domains, they often suffer from limitations in sensing\nmodalities, environmental diversity, and the reproducibility of the underlying\nhardware setups. To address these challenges, this paper introduces SMapper, a\nnovel open-hardware, multi-sensor platform designed explicitly for, though not\nlimited to, SLAM research. The device integrates synchronized LiDAR,\nmulti-camera, and inertial sensing, supported by a robust calibration and\nsynchronization pipeline that ensures precise spatio-temporal alignment across\nmodalities. Its open and replicable design allows researchers to extend its\ncapabilities and reproduce experiments across both handheld and robot-mounted\nscenarios. To demonstrate its practicality, we additionally release\nSMapper-light, a publicly available SLAM dataset containing representative\nindoor and outdoor sequences. The dataset includes tightly synchronized\nmultimodal data and ground-truth trajectories derived from offline LiDAR-based\nSLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.\nFurthermore, the paper contains benchmarking results on state-of-the-art LiDAR\nand visual SLAM frameworks using the SMapper-light dataset. By combining\nopen-hardware design, reproducible data collection, and comprehensive\nbenchmarking, SMapper establishes a robust foundation for advancing SLAM\nalgorithm development, evaluation, and reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMapper\u7684\u5f00\u6e90\u591a\u4f20\u611f\u5668\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3SLAM\u7814\u7a76\u4e2d\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u5e03\u4e86SMapper-light\u6570\u636e\u96c6\u7528\u4e8e\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684SLAM\u548c\u591a\u6a21\u6001\u6570\u636e\u96c6\u5728\u611f\u77e5\u591a\u6837\u6027\u3001\u73af\u5883\u591a\u6837\u6027\u548c\u786c\u4ef6\u53ef\u91cd\u590d\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5236\u7ea6\u4e86\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u8bba\u6587\u8bbe\u8ba1\u4e86SMapper\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u540c\u6b65\u7684LiDAR\u3001\u591a\u6444\u50cf\u5934\u548c\u60ef\u6027\u4f20\u611f\uff0c\u5e76\u901a\u8fc7\u6821\u51c6\u548c\u540c\u6b65\u7ba1\u9053\u786e\u4fdd\u6570\u636e\u7684\u65f6\u95f4\u7a7a\u95f4\u5bf9\u9f50\u3002\u540c\u65f6\uff0c\u53d1\u5e03\u4e86SMapper-light\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7SMapper-light\u6570\u636e\u96c6\u5c55\u793a\u4e86\u8be5\u5e73\u53f0\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u9a8c\u8bc1\u5176\u652f\u6301SLAM\u7b97\u6cd5\u5f00\u53d1\u548c\u8bc4\u4f30\u7684\u80fd\u529b\u3002", "conclusion": "SMapper\u901a\u8fc7\u5f00\u6e90\u786c\u4ef6\u8bbe\u8ba1\u3001\u53ef\u91cd\u590d\u6570\u636e\u6536\u96c6\u548c\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3aSLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2509.09343", "pdf": "https://arxiv.org/pdf/2509.09343", "abs": "https://arxiv.org/abs/2509.09343", "authors": ["Mohammed M. H. Qazzaz", "Abdelaziz Salama", "Maryam Hafeez", "Syed A. R. Zaidi"], "title": "Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Open Radio Access Network (O-RAN) architecture provides an intrinsic\ncapability to exploit key performance monitoring (KPM) within Radio\nIntelligence Controller (RIC) to derive network optimisation through xApps.\nThese xApps can leverage KPM knowledge to dynamically switch on/off the\nassociated RUs where such a function is supported over the E2 interface.\nSeveral existing studies employ artificial intelligence (AI)/Machine Learning\n(ML) based approaches to realise such dynamic sleeping for increased energy\nefficiency (EE). Nevertheless, most of these approaches rely upon offloading\nuser equipment (UE) to carve out a sleeping opportunity. Such an approach\ninherently creates load imbalance across the network. Such load imbalance may\nimpact the throughput performance of offloaded UEs as they might be allocated a\nlower number of physical resource blocks (PRBs). Maintaining the same PRB\nallocation while addressing the EE at the network level is a challenging task.\nTo that end, in this article, we present a comprehensive ML-based framework for\njoint optimisation of load balancing and EE for ORAN deployments. We formulate\nthe problem as a multi-class classification system that predictively evaluates\npotential RU configurations before optimising the EE, mapping network\nconditions to three load balance categories (Well Balanced, Moderately\nBalanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,\nAggressive) accommodates different operational priorities between energy\nsavings and performance assurance. Experimental evaluation using 4.26 million\nreal network measurements from simulations demonstrates that our Random Forest\nmodel achieves 98.3% F1-macro performance, representing 195% improvement over\ntraditional baseline strategies.", "AI": {"tldr": "O-RAN\u67b6\u6784\u5229\u7528xApps\u548cKPM\u5b9e\u73b0\u7f51\u7edc\u4f18\u5316\uff0c\u901a\u8fc7AI/ML\u65b9\u6cd5\u52a8\u6001\u5173\u95edRU\u4ee5\u63d0\u5347\u80fd\u6548\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8d1f\u8f7d\u4e0d\u5747\u8861\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cdML\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u8d1f\u8f7d\u5747\u8861\u548c\u80fd\u6548\uff0c\u901a\u8fc7\u591a\u9608\u503c\u5206\u7c7b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u52a8\u6001\u7761\u7720\u65b9\u6cd5\u56e0UE\u5378\u8f7d\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u7f51\u7edc\u80fd\u6548\u3002", "method": "\u91c7\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u7684\u591a\u5206\u7c7b\u7cfb\u7edf\uff0c\u9884\u6d4bRU\u914d\u7f6e\u5e76\u4f18\u5316\u80fd\u6548\uff0c\u901a\u8fc7\u591a\u9608\u503c\u7b56\u7565\u5e73\u8861\u8282\u80fd\u4e0e\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u7684F1-macro\u6027\u80fd\u8fbe98.3%\uff0c\u8f83\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347195%\u3002", "conclusion": "\u63d0\u51fa\u7684ML\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d1f\u8f7d\u4e0d\u5747\u8861\u4e0e\u80fd\u6548\u4f18\u5316\u7684\u77db\u76fe\uff0c\u4e3aO-RAN\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2509.09546", "pdf": "https://arxiv.org/pdf/2509.09546", "abs": "https://arxiv.org/abs/2509.09546", "authors": ["Yanhui Lu", "Zeyu Deng", "Stephen J. Redmond", "Efi Psomopoulou", "Benjamin Ward-Cherrier"], "title": "A Neuromorphic Incipient Slip Detection System using Papillae Morphology", "categories": ["cs.RO"], "comment": "7 pages, 12 figures. Submitted to IEEE Robotics and Automation\n  Letters (RAL), under review", "summary": "Detecting incipient slip enables early intervention to prevent object\nslippage and enhance robotic manipulation safety. However, deploying such\nsystems on edge platforms remains challenging, particularly due to energy\nconstraints. This work presents a neuromorphic tactile sensing system based on\nthe NeuroTac sensor with an extruding papillae-based skin and a spiking\nconvolutional neural network (SCNN) for slip-state classification. The SCNN\nmodel achieves 94.33% classification accuracy across three classes (no slip,\nincipient slip, and gross slip) in slip conditions induced by sensor motion.\nUnder the dynamic gravity-induced slip validation conditions, after temporal\nsmoothing of the SCNN's final-layer spike counts, the system detects incipient\nslip at least 360 ms prior to gross slip across all trials, consistently\nidentifying incipient slip before gross slip occurs. These results demonstrate\nthat this neuromorphic system has stable and responsive incipient slip\ndetection capability.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u4f20\u611f\u7cfb\u7edf\u548cSCNN\u7684\u6ed1\u79fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u7269\u4f53\u6ed1\u79fb\u524d360\u6beb\u79d2\u68c0\u6d4b\u5230\u521d\u671f\u6ed1\u79fb\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u6ed1\u79fb\u53ef\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u5b89\u5168\u6027\uff0c\u4f46\u76ee\u524d\u8fb9\u7f18\u5e73\u53f0\u7684\u80fd\u8017\u95ee\u9898\u9650\u5236\u4e86\u8fd9\u7c7b\u7cfb\u7edf\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528NeuroTac\u4f20\u611f\u5668\u548cSCNN\u6a21\u578b\u5bf9\u4e09\u79cd\u6ed1\u79fb\u72b6\u6001\uff08\u65e0\u6ed1\u79fb\u3001\u521d\u671f\u6ed1\u79fb\u3001\u5b8c\u5168\u6ed1\u79fb\uff09\u8fdb\u884c\u5206\u7c7b\u3002", "result": "SCNN\u6a21\u578b\u5728\u52a8\u6001\u91cd\u529b\u8bf1\u5bfc\u7684\u6ed1\u79fb\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u7a33\u5b9a\u5730\u5728\u5b8c\u5168\u6ed1\u79fb\u524d360\u6beb\u79d2\u68c0\u6d4b\u5230\u521d\u671f\u6ed1\u79fb\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u6709\u7a33\u5b9a\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u521d\u671f\u6ed1\u79fb\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.09506", "pdf": "https://arxiv.org/pdf/2509.09506", "abs": "https://arxiv.org/abs/2509.09506", "authors": ["Philipp del Hougne"], "title": "Frozen differential scattering in reconfigurable complex media", "categories": ["physics.optics", "eess.SP", "physics.app-ph"], "comment": "13 pages, 5 figures", "summary": "The sensitivity of transmission to the input wavefront is a hallmark feature\nof complex media and the basis for wavefront shaping techniques. Yet,\nintriguing special cases exist in which the output wavefront is \"frozen\"\n(agnostic to the input wavefront). This happens when special structure in the\ncomplex medium collapses the rank of its transmission matrix to unity. Here, we\nunveil that an analogous phenomenon exists more universally for differential\nscattering (including reflection) in reconfigurable complex media.\nSpecifically, for a localized perturbation, the differential scattering matrix\nof any complex medium has rank one. One consequence is that the differential\noutput signal is perfectly coherent irrespective of the input wavefront's\ncoherence. Moreover, the thermal noise emitted into the frozen differential\noutput mode has a particular structure that can be exploited for thermal noise\nmanagement. We experimentally evidence frozen differential scattering in a\nrich-scattering wireless link parametrized by a programmable meta-atom. Then,\nwe demonstrate \"customized freezing\" by optimizing the configuration of\nadditional programmable meta-atoms that parametrize the wireless link, as\nenvisioned for 6G networks. We impose particular shapes of the frozen\ndifferential output mode, and maximize its signal-to-thermal-noise ratio.\nPotential applications include filtering and stabilization of differential\nwavefronts, as well as imaging, sensing, and communication in complex media.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u53ef\u91cd\u6784\u590d\u6742\u4ecb\u8d28\u4e2d\u666e\u904d\u5b58\u5728\u7684\u201c\u51bb\u7ed3\u201d\u5fae\u5206\u6563\u5c04\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u57286G\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u590d\u6742\u4ecb\u8d28\u4e2d\u4f20\u8f93\u6ce2\u524d\u4e0e\u8f93\u5165\u6ce2\u524d\u65e0\u5173\u7684\u7279\u6b8a\u73b0\u8c61\uff0c\u63a2\u7d22\u5176\u5728\u5fae\u5206\u6563\u5c04\uff08\u5305\u62ec\u53cd\u5c04\uff09\u4e2d\u7684\u666e\u904d\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u590d\u6742\u4ecb\u8d28\u7684\u4f20\u8f93\u77e9\u9635\u548c\u5fae\u5206\u6563\u5c04\u77e9\u9635\uff0c\u53d1\u73b0\u5176\u79e9\u4e3a\u4e00\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u53ef\u7f16\u7a0b\u8d85\u6784\u539f\u5b50\u53c2\u6570\u5316\u7684\u65e0\u7ebf\u94fe\u8def\u4e2d\u7684\u51bb\u7ed3\u73b0\u8c61\u3002", "result": "\u5fae\u5206\u8f93\u51fa\u4fe1\u53f7\u5177\u6709\u5b8c\u7f8e\u76f8\u5e72\u6027\uff0c\u4e14\u70ed\u566a\u58f0\u8f93\u51fa\u6a21\u5f0f\u5177\u6709\u7279\u5b9a\u7ed3\u6784\uff1b\u901a\u8fc7\u4f18\u5316\u8d85\u6784\u539f\u5b50\u914d\u7f6e\u5b9e\u73b0\u4e86\u201c\u5b9a\u5236\u5316\u51bb\u7ed3\u201d\u3002", "conclusion": "\u8fd9\u4e00\u73b0\u8c61\u5728\u590d\u6742\u4ecb\u8d28\u4e2d\u7684\u6210\u50cf\u3001\u4f20\u611f\u548c\u901a\u4fe1\u7b49\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.09594", "pdf": "https://arxiv.org/pdf/2509.09594", "abs": "https://arxiv.org/abs/2509.09594", "authors": ["Sourav Garg", "Dustin Craggs", "Vineeth Bhat", "Lachlan Mares", "Stefan Podgorski", "Madhava Krishna", "Feras Dayoub", "Ian Reid"], "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "CoRL 2025; 23 pages including appendix", "summary": "Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u7269\u4f53\u76f8\u5bf9\u201d\u63a7\u5236\u7684\u89c6\u89c9\u5bfc\u822a\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5bf9\u8c61\u7ea7\u8868\u793a\u514b\u670d\u4e86\u56fe\u50cf\u7ea7\u8868\u793a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8de8\u5177\u8eab\u4f53\u9a8c\u548c\u4efb\u52a1\u4e2d\u7684\u9ad8\u4e0d\u53d8\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u50cf\u76f8\u5bf9\u63a7\u5236\u5728\u89c6\u89c9\u5bfc\u822a\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u56fe\u50cf\u4e25\u683c\u4f9d\u8d56\u4e8e\u4ee3\u7406\u7684\u4f4d\u59ff\u548c\u4f53\u73b0\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5bf9\u8c61\u76f8\u5bf9\u63a7\u5236\u7684\u4f18\u52bf\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5177\u4e0d\u53d8\u6027\u548c\u7075\u6d3b\u6027\u7684\u5bfc\u822a\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u201c\u76f8\u5bf9\u201d3D\u573a\u666f\u56fe\u7684\u62d3\u6251\u56fe\u8868\u793a\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u672c\u5730\u63a7\u5236\u5668\u201cObjectReact\u201d\uff0c\u76f4\u63a5\u57fa\u4e8e\u9ad8\u7ea7\u201cWayObject Costmap\u201d\u8868\u793a\u8fdb\u884c\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u8c61\u76f8\u5bf9\u63a7\u5236\u5728\u4f20\u611f\u5668\u9ad8\u5ea6\u53d8\u5316\u548c\u591a\u79cd\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u56fe\u50cf\u76f8\u5bf9\u63a7\u5236\uff0c\u4e14\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u3002", "conclusion": "\u5bf9\u8c61\u76f8\u5bf9\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u66f4\u5177\u4e0d\u53d8\u6027\u7684\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8de8\u5177\u8eab\u4f53\u9a8c\u548c\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2509.09644", "pdf": "https://arxiv.org/pdf/2509.09644", "abs": "https://arxiv.org/abs/2509.09644", "authors": ["Chunjie Wang", "Xuhui Zhang", "Wenchao Liu", "Jinke Ren", "Shuqiang Wang", "Yanyan Shen", "Kejiang Ye", "Kim Fung Tsang"], "title": "RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "This manuscript has been submitted to IEEE", "summary": "This paper investigates the data collection enhancement problem in a\nreconfigurable intelligent surface (RIS)-empowered intelligent consumer\ntransportation system (ICTS). We propose a novel framework where a data center\n(DC) provides energy to pre-configured roadside unit (RSU) pairs during the\ndownlink stage. While in the uplink stage, these RSU pairs utilize a hybrid\nrate-splitting multiple access (RSMA) and time-division multiple access (TDMA)\nprotocol to transmit the processed data to the DC, while simultaneously\nperforming local data processing using the harvested energy. Our objective is\nto maximize the minimal processed data volume of the RSU pairs by jointly\noptimizing the RIS downlink and uplink phase shifts, the transmit power of the\nDC and RSUs, the RSU computation resource allocation, and the time slot\nallocation. To address the formulated non-convex problem, we develop an\nefficient iterative algorithm integrating alternating optimization and\nsequential rank-one constraint relaxation methods. Extensive simulations\ndemonstrate that the proposed algorithm significantly outperforms baseline\nschemes under diverse scenarios, validating its effectiveness in enhancing the\ndata processing performance for intelligent transportation applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u667a\u80fd\u6d88\u8d39\u8005\u4ea4\u901a\u7cfb\u7edf\u4e2dRIS\u589e\u5f3a\u7684\u6570\u636e\u6536\u96c6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u8fed\u4ee3\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u6570\u636e\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u6570\u636e\u6536\u96c6\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u6570\u636e\u5904\u7406\u6548\u7387\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df7\u5408RSMA\u548cTDMA\u534f\u8bae\uff0c\u4f18\u5316RIS\u914d\u7f6e\u3001\u529f\u7387\u5206\u914d\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u5206\u914d\uff0c\u5f00\u53d1\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6848\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u6570\u636e\u5904\u7406\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.09613", "pdf": "https://arxiv.org/pdf/2509.09613", "abs": "https://arxiv.org/abs/2509.09613", "authors": ["Taisei Mogi", "Mari Saito", "Yoshihiro Nakata"], "title": "MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements", "categories": ["cs.RO"], "comment": null, "summary": "Robots for therapy and social interaction are often intended to evoke\n\"animacy\" in humans. While many robots imitate appearance and joint movements,\nlittle attention has been given to whole-body expansion-contraction,\nvolume-changing movements observed in living organisms, and their effect on\nanimacy perception. We developed a mobile robot called \"MOFU (Morphing Fluffy\nUnit),\" capable of whole-body expansion-contraction with a single motor and\ncovered with a fluffy exterior. MOFU employs a \"Jitterbug\" structure, a\ngeometric transformation mechanism that enables smooth volume change in\ndiameter from 210 to 280 mm using one actuator. It is also equipped with a\ndifferential two-wheel drive mechanism for locomotion. To evaluate the effect\nof expansion-contraction movements, we conducted an online survey using videos\nof MOFU's behavior. Participants rated impressions with the Godspeed\nQuestionnaire Series. First, we compared videos of MOFU in a stationary state\nwith and without expansion-contraction and turning, finding that\nexpansion-contraction significantly increased perceived animacy. Second, we\nhypothesized that presenting two MOFUs would increase animacy compared with a\nsingle robot; however, this was not supported, as no significant difference\nemerged. Exploratory analyses further compared four dual-robot motion\nconditions. Third, when expansion-contraction was combined with locomotion,\nanimacy ratings were higher than locomotion alone. These results suggest that\nvolume-changing movements such as expansion and contraction enhance perceived\nanimacy in robots and should be considered an important design element in\nfuture robot development aimed at shaping human impressions.", "AI": {"tldr": "MOFU\u673a\u5668\u4eba\u901a\u8fc7\u5168\u8eab\u81a8\u80c0-\u6536\u7f29\u8fd0\u52a8\u63d0\u5347\u4eba\u7c7b\u5bf9\u5176\u7684\u751f\u673a\u611f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u8fd0\u52a8\u663e\u8457\u589e\u5f3a\u611f\u77e5\u751f\u673a\u611f\uff0c\u4f46\u53cc\u673a\u5668\u4eba\u5e76\u672a\u8fdb\u4e00\u6b65\u589e\u52a0\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u5168\u8eab\u4f53\u79ef\u53d8\u5316\u8fd0\u52a8\u5bf9\u673a\u5668\u4eba\u611f\u77e5\u751f\u673a\u611f\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u8bbe\u8ba1\u4e2d\u5bf9\u8be5\u8fd0\u52a8\u7684\u5ffd\u89c6\u3002", "method": "\u5f00\u53d1MOFU\u673a\u5668\u4eba\uff0c\u91c7\u7528Jitterbug\u7ed3\u6784\u548c\u53cc\u8f6e\u9a71\u52a8\uff0c\u901a\u8fc7\u5728\u7ebf\u89c6\u9891\u8c03\u67e5\u8bc4\u4f30\u81a8\u80c0-\u6536\u7f29\u8fd0\u52a8\u7684\u6548\u679c\u3002", "result": "\u81a8\u80c0-\u6536\u7f29\u8fd0\u52a8\u663e\u8457\u63d0\u5347\u751f\u673a\u611f\uff0c\u53cc\u673a\u5668\u4eba\u65e0\u989d\u5916\u6548\u679c\uff0c\u7ed3\u5408\u8fd0\u52a8\u65f6\u751f\u673a\u611f\u66f4\u5f3a\u3002", "conclusion": "\u4f53\u79ef\u53d8\u5316\u8fd0\u52a8\u662f\u63d0\u5347\u673a\u5668\u4eba\u611f\u77e5\u751f\u673a\u611f\u7684\u91cd\u8981\u8bbe\u8ba1\u5143\u7d20\u3002"}}
{"id": "2509.09651", "pdf": "https://arxiv.org/pdf/2509.09651", "abs": "https://arxiv.org/abs/2509.09651", "authors": ["Zakaria El Kassimi", "Fares Fourati", "Mohamed-Slim Alouini"], "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "eess.SP"], "comment": null, "summary": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG.", "AI": {"tldr": "\u7814\u7a76\u5728\u65e0\u7ebf\u7535\u6cd5\u89c4\u9886\u57df\u7684\u95ee\u9898\u56de\u7b54\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7535\u4fe1\u7279\u5b9a\u7684RAG\u7ba1\u9053\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u591a\u9009\u62e9\u9898\u8bc4\u4f30\u96c6\u3002", "motivation": "\u65e0\u7ebf\u7535\u6cd5\u89c4\u662f\u4e00\u4e2a\u6cd5\u5f8b\u654f\u611f\u4e14\u9ad8\u98ce\u9669\u7684\u9886\u57df\uff0c\u9700\u8981\u6709\u6548\u7684\u95ee\u7b54\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7535\u4fe1\u7279\u5b9a\u7684RAG\u7ba1\u9053\uff0c\u7ed3\u5408\u81ea\u52a8\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\u6784\u5efa\u8bc4\u4f30\u96c6\uff0c\u5e76\u5b9a\u4e49\u9886\u57df\u7279\u5b9a\u7684\u68c0\u7d22\u6307\u6807\u3002", "result": "\u68c0\u7d22\u51c6\u786e\u7387\u8fbe97%\uff0c\u751f\u6210\u51c6\u786e\u7387\u63d0\u534712%\u3002", "conclusion": "\u76ee\u6807\u660e\u786e\u7684RAG\u7ba1\u9053\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9886\u57df\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09671", "pdf": "https://arxiv.org/pdf/2509.09671", "abs": "https://arxiv.org/abs/2509.09671", "authors": ["Sirui Xu", "Yu-Wei Chao", "Liuyu Bian", "Arsalan Mousavian", "Yu-Xiong Wang", "Liang-Yan Gui", "Wei Yang"], "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration", "categories": ["cs.RO", "cs.CV"], "comment": "CoRL 2025", "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation.", "AI": {"tldr": "Dexplore\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u5355\u5faa\u73af\u4f18\u5316\u65b9\u6cd5\uff0c\u76f4\u63a5\u4eceMoCap\u6570\u636e\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\uff0c\u907f\u514d\u4e86\u73b0\u6709\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u566a\u58f0\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673aMoCap\u6570\u636e\u4f7f\u7528\u5b58\u5728\u6f14\u793a\u4e0d\u51c6\u786e\u548c\u4eba\u4f53\u4e0e\u673a\u5668\u4eba\u624b\u5f62\u6001\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6570\u636e\u5229\u7528\u4e0d\u8db3\u548c\u8bef\u5dee\u7d2f\u79ef\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "Dexplore\u901a\u8fc7\u7edf\u4e00\u7684\u5355\u5faa\u73af\u4f18\u5316\u8054\u5408\u6267\u884c\u91cd\u5b9a\u5411\u548c\u8ddf\u8e2a\uff0c\u5c06\u6f14\u793a\u4f5c\u4e3a\u8f6f\u6307\u5bfc\uff0c\u4ece\u539f\u59cb\u8f68\u8ff9\u4e2d\u63d0\u53d6\u81ea\u9002\u5e94\u7a7a\u95f4\u8303\u56f4\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u6f14\u793a\u610f\u56fe\uff0c\u652f\u6301\u673a\u5668\u4eba\u7279\u5b9a\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\uff0c\u6700\u540e\u751f\u6210\u652f\u6301\u8de8\u5bf9\u8c61\u6cdb\u5316\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u89c6\u89c9\u63a7\u5236\u5668\u3002", "conclusion": "Dexplore\u4e3a\u5c06\u4e0d\u5b8c\u7f8e\u7684MoCap\u6570\u636e\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u7075\u5de7\u64cd\u4f5c\u8bad\u7ec3\u4fe1\u53f7\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09674", "pdf": "https://arxiv.org/pdf/2509.09674", "abs": "https://arxiv.org/abs/2509.09674", "authors": ["Haozhan Li", "Yuxin Zuo", "Jiale Yu", "Yuhao Zhang", "Zhaohui Yang", "Kaiyan Zhang", "Xuekai Zhu", "Yuchen Zhang", "Tianxing Chen", "Ganqu Cui", "Dehui Wang", "Dingxiang Luo", "Yuchen Fan", "Youbang Sun", "Jia Zeng", "Jiangmiao Pang", "Shanghang Zhang", "Yu Wang", "Yao Mu", "Bowen Zhou", "Ning Ding"], "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL", "AI": {"tldr": "SimpleVLA-RL\u662f\u4e00\u79cd\u9488\u5bf9VLA\u6a21\u578b\u7684\u9ad8\u6548RL\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8d85\u8d8aSFT\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u5927\u89c4\u6a21\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u7a00\u7f3a\u548c\u4efb\u52a1\u5206\u5e03\u504f\u79fb\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86SimpleVLA-RL\u6846\u67b6\uff0c\u7ed3\u5408\u8f68\u8ff9\u91c7\u6837\u3001\u5e76\u884c\u5316\u3001\u591a\u73af\u5883\u6e32\u67d3\u548c\u4f18\u5316\u635f\u5931\u8ba1\u7b97\u7b49\u7b56\u7565\u3002", "result": "\u5728LIBERO\u548cRoboTwin 1.0&2.0\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u5e76\u53d1\u73b0RL\u8bad\u7ec3\u4e2d\u7684'pushcut'\u73b0\u8c61\u3002", "conclusion": "SimpleVLA-RL\u51cf\u5c11\u4e86\u6570\u636e\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2509.09210", "pdf": "https://arxiv.org/pdf/2509.09210", "abs": "https://arxiv.org/abs/2509.09210", "authors": ["Xing Gao", "Zherui Huang", "Weiyao Lin", "Xiao Sun"], "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Accurate motion prediction of surrounding agents is crucial for the safe\nplanning of autonomous vehicles. Recent advancements have extended prediction\ntechniques from individual agents to joint predictions of multiple interacting\nagents, with various strategies to address complex interactions within future\nmotions of agents. However, these methods overlook the evolving nature of these\ninteractions. To address this limitation, we propose a novel progressive\nmulti-scale decoding strategy, termed ProgD, with the help of dynamic\nheterogeneous graph-based scenario modeling. In particular, to explicitly and\ncomprehensively capture the evolving social interactions in future scenarios,\ngiven their inherent uncertainty, we design a progressive modeling of scenarios\nwith dynamic heterogeneous graphs. With the unfolding of such dynamic\nheterogeneous graphs, a factorized architecture is designed to process the\nspatio-temporal dependencies within future scenarios and progressively\neliminate uncertainty in future motions of multiple agents. Furthermore, a\nmulti-scale decoding procedure is incorporated to improve on the future\nscenario modeling and consistent prediction of agents' future motion. The\nproposed ProgD achieves state-of-the-art performance on the INTERACTION\nmulti-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2\nmulti-world forecasting benchmark.", "AI": {"tldr": "ProgD\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u5c3a\u5ea6\u6e10\u8fdb\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u5f02\u6784\u56fe\u5efa\u6a21\u672a\u6765\u573a\u666f\u4e2d\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u95f4\u4e92\u52a8\u7684\u52a8\u6001\u6f14\u53d8\uff0c\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u590d\u6742\u573a\u666f\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u3002", "method": "\u63d0\u51faProgD\u7b56\u7565\uff0c\u7ed3\u5408\u52a8\u6001\u5f02\u6784\u56fe\u9010\u6b65\u5efa\u6a21\u672a\u6765\u573a\u666f\uff0c\u5e76\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u89e3\u7801\u51cf\u5c11\u8fd0\u52a8\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728INTERACTION\u548cArgoverse 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "ProgD\u901a\u8fc7\u52a8\u6001\u6e10\u8fdb\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u89e3\u7801\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.09238", "pdf": "https://arxiv.org/pdf/2509.09238", "abs": "https://arxiv.org/abs/2509.09238", "authors": ["Thorbj\u00f8rn Mosekj\u00e6r Iversen", "Lars Car\u00f8e S\u00f8rensen", "Simon Faarvang Mathiesen", "Henrik Gordon Petersen"], "title": "Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation", "categories": ["stat.ML", "cs.LG", "cs.RO"], "comment": null, "summary": "Many optimization problems in robotics involve the optimization of\ntime-expensive black-box functions, such as those involving complex simulations\nor evaluation of real-world experiments. Furthermore, these functions are often\nstochastic as repeated experiments are subject to unmeasurable disturbances.\nBayesian optimization can be used to optimize such methods in an efficient\nmanner by deploying a probabilistic function estimator to estimate with a given\nconfidence so that regions of the search space can be pruned away.\nConsequently, the success of the Bayesian optimization depends on the function\nestimator's ability to provide informative confidence bounds. Existing function\nestimators require many function evaluations to infer the underlying confidence\nor depend on modeling of the disturbances. In this paper, it is shown that the\nconfidence bounds provided by the Wilson Score Kernel Density Estimator\n(WS-KDE) are applicable as excellent bounds to any stochastic function with an\noutput confined to the closed interval [0;1] regardless of the distribution of\nthe output. This finding opens up the use of WS-KDE for stable global\noptimization on a wider range of cost functions. The properties of WS-KDE in\nthe context of Bayesian optimization are demonstrated in simulation and applied\nto the problem of automated trap design for vibrational part feeders.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWilson Score\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff08WS-KDE\uff09\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8f93\u51fa\u5728[0,1]\u533a\u95f4\u5185\u7684\u968f\u673a\u51fd\u6570\u4f18\u5316\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7f6e\u4fe1\u8fb9\u754c\u63d0\u9ad8\u4e86\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5b66\u4e2d\uff0c\u8bb8\u591a\u4f18\u5316\u95ee\u9898\u6d89\u53ca\u8017\u65f6\u4e14\u968f\u673a\u7684\u9ed1\u7bb1\u51fd\u6570\uff08\u5982\u590d\u6742\u6a21\u62df\u6216\u771f\u5b9e\u5b9e\u9a8c\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u6216\u566a\u58f0\u5efa\u6a21\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u91c7\u7528Wilson Score\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff08WS-KDE\uff09\u4f5c\u4e3a\u6982\u7387\u51fd\u6570\u4f30\u8ba1\u5668\uff0c\u63d0\u51fa\u5176\u7f6e\u4fe1\u8fb9\u754c\u9002\u7528\u4e8e\u4efb\u4f55\u8f93\u51fa\u5728[0,1]\u533a\u95f4\u5185\u7684\u968f\u673a\u51fd\u6570\u3002", "result": "WS-KDE\u7684\u7f6e\u4fe1\u8fb9\u754c\u88ab\u8bc1\u660e\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u968f\u673a\u51fd\u6570\uff0c\u65e0\u9700\u4f9d\u8d56\u8f93\u51fa\u5206\u5e03\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u632f\u52a8\u9001\u6599\u5668\u81ea\u52a8\u9677\u9631\u8bbe\u8ba1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "WS-KDE\u4e3a\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6210\u672c\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6548\u7387\u3002"}}
{"id": "2509.09297", "pdf": "https://arxiv.org/pdf/2509.09297", "abs": "https://arxiv.org/abs/2509.09297", "authors": ["Spyridon Loukovitis", "Anastasios Arsenos", "Vasileios Karampinis", "Athanasios Voulodimos"], "title": "Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Open-set detection is crucial for robust UAV autonomy in air-to-air object\ndetection under real-world conditions. Traditional closed-set detectors degrade\nsignificantly under domain shifts and flight data corruption, posing risks to\nsafety-critical applications. We propose a novel, model-agnostic open-set\ndetection framework designed specifically for embedding-based detectors. The\nmethod explicitly handles unknown object rejection while maintaining robustness\nagainst corrupted flight data. It estimates semantic uncertainty via entropy\nmodeling in the embedding space and incorporates spectral normalization and\ntemperature scaling to enhance open-set discrimination. We validate our\napproach on the challenging AOT aerial benchmark and through extensive\nreal-world flight tests. Comprehensive ablation studies demonstrate consistent\nimprovements over baseline methods, achieving up to a 10\\% relative AUROC gain\ncompared to standard YOLO-based detectors. Additionally, we show that\nbackground rejection further strengthens robustness without compromising\ndetection accuracy, making our solution particularly well-suited for reliable\nUAV perception in dynamic air-to-air environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5d4c\u5165\u68c0\u6d4b\u5668\u7684\u5f00\u96c6\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u5efa\u6a21\u548c\u5149\u8c31\u5f52\u4e00\u5316\u7b49\u6280\u672f\u63d0\u5347\u5bf9\u672a\u77e5\u76ee\u6807\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9645\u98de\u884c\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u95ed\u96c6\u68c0\u6d4b\u5668\u5728\u57df\u504f\u79fb\u548c\u98de\u884c\u6570\u636e\u635f\u574f\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5f00\u96c6\u68c0\u6d4b\u5bf9\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u4e2d\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u71b5\u5efa\u6a21\u548c\u5149\u8c31\u5f52\u4e00\u5316\u589e\u5f3a\u5f00\u96c6\u533a\u5206\u80fd\u529b\uff0c\u7ed3\u5408\u6e29\u5ea6\u7f29\u653e\u7b49\u6280\u672f\u3002", "result": "\u5728AOT\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u98de\u884c\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u5bf9YOLO\u68c0\u6d4b\u5668AUROC\u63d0\u534710%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u52a8\u6001\u7a7a\u5bf9\u7a7a\u73af\u5883\u4e2d\u7684\u611f\u77e5\u53ef\u9760\u6027\uff0c\u5c24\u5176\u9002\u5408\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2509.09349", "pdf": "https://arxiv.org/pdf/2509.09349", "abs": "https://arxiv.org/abs/2509.09349", "authors": ["Ian Nell", "Shane Gilroy"], "title": "Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.RO", "eess.IV"], "comment": null, "summary": "Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behavior classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviors such as excessive lateral movement and erratic trajectory patterns by\nimplementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioral analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u65b0\u578b\u9a7e\u9a76\u5458\u884c\u4e3a\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5916\u90e8\u89c2\u5bdf\u6280\u672f\u68c0\u6d4b\u5206\u5fc3\u548c\u9152\u9a7e\u7b49\u884c\u4e3a\uff0c\u65e0\u9700\u8f66\u8f86\u95f4\u901a\u4fe1\u5373\u53ef\u5206\u6790\u975e\u8054\u7f51\u8f66\u8f86\u7684\u884c\u4e3a\u3002", "motivation": "\u9053\u8def\u4ea4\u901a\u4e8b\u6545\u662f\u5168\u7403\u91cd\u5927\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u9a7e\u9a76\u5458\u5206\u5fc3\u548c\u9152\u9a7e\u7b49\u4eba\u4e3a\u9519\u8bef\u3002", "method": "\u91c7\u7528\u5148\u8fdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u5305\u62ec\u5b9e\u65f6\u76ee\u6807\u8ddf\u8e2a\u3001\u6a2a\u5411\u4f4d\u79fb\u5206\u6790\u548c\u8f66\u9053\u4f4d\u7f6e\u76d1\u6d4b\uff0c\u7ed3\u5408YOLO\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u548c\u81ea\u5b9a\u4e49\u8f66\u9053\u4f30\u8ba1\u7b97\u6cd5\u3002", "result": "\u7cfb\u7edf\u5728\u4e0d\u540c\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u9ad8\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u4e0d\u5b89\u5168\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "conclusion": "\u8be5\u89c6\u89c9\u9a71\u52a8\u7684\u65b9\u6cd5\u4e3a\u5206\u6790\u975e\u8054\u7f51\u8f66\u8f86\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.09356", "pdf": "https://arxiv.org/pdf/2509.09356", "abs": "https://arxiv.org/abs/2509.09356", "authors": ["Abdel Hakim Drid", "Vincenzo Suriani", "Daniele Nardi", "Abderrezzak Debilou"], "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning", "categories": ["cs.AI", "cs.RO"], "comment": "The 19th International Conference on Intelligent Autonomous Systems\n  (IAS 19), 2025, Genoa", "summary": "Navigating and understanding complex and unknown environments autonomously\ndemands more than just basic perception and movement from embodied agents.\nTruly effective exploration requires agents to possess higher-level cognitive\nabilities, the ability to reason about their surroundings, and make more\ninformed decisions regarding exploration strategies. However, traditional RL\napproaches struggle to balance efficient exploration and semantic understanding\ndue to limited cognitive capabilities embedded in the small policies for the\nagents, leading often to human drivers when dealing with semantic exploration.\nIn this paper, we address this challenge by presenting a novel Deep\nReinforcement Learning (DRL) architecture that is specifically designed for\nresource efficient semantic exploration. A key methodological contribution is\nthe integration of a Vision-Language Model (VLM) common-sense through a layered\nreward function. The VLM query is modeled as a dedicated action, allowing the\nagent to strategically query the VLM only when deemed necessary for gaining\nexternal guidance, thereby conserving resources. This mechanism is combined\nwith a curriculum learning strategy designed to guide learning at different\nlevels of complexity to ensure robust and stable learning. Our experimental\nevaluation results convincingly demonstrate that our agent achieves\nsignificantly enhanced object discovery rates and develops a learned capability\nto effectively navigate towards semantically rich regions. Furthermore, it also\nshows a strategic mastery of when to prompt for external environmental\ninformation. By demonstrating a practical and scalable method for embedding\ncommon-sense semantic reasoning with autonomous agents, this research provides\na novel approach to pursuing a fully intelligent and self-guided exploration in\nrobotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5206\u5c42\u5956\u52b1\u51fd\u6570\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfRL\u5728\u8bed\u4e49\u63a2\u7d22\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edfRL\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u8bed\u4e49\u63a2\u7d22\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u9ad8\u5c42\u8ba4\u77e5\u80fd\u529b\uff0c\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408VLM\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u5f00\u53d1\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u8bed\u4e49\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5f15\u5165VLM\u4f5c\u4e3a\u4e00\u79cd\u4e13\u7528\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42\u5956\u52b1\u51fd\u6570\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u6218\u7565\u6027\u5229\u7528\u5916\u90e8\u6307\u5bfc\uff0c\u540c\u65f6\u786e\u4fdd\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u8c61\u53d1\u73b0\u7387\uff0c\u5e76\u5b66\u4f1a\u6709\u6548\u5bfc\u822a\u81f3\u8bed\u4e49\u4e30\u5bcc\u7684\u533a\u57df\uff0c\u540c\u65f6\u638c\u63e1\u4f55\u65f6\u8bf7\u6c42\u5916\u90e8\u4fe1\u606f\u7684\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u5b9e\u73b0\u5b8c\u5168\u667a\u80fd\u548c\u81ea\u5bfc\u822a\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.09584", "pdf": "https://arxiv.org/pdf/2509.09584", "abs": "https://arxiv.org/abs/2509.09584", "authors": ["Lingdong Kong", "Dongyue Lu", "Ao Liang", "Rong Li", "Yuhao Dong", "Tianshuai Hu", "Lai Xing Ng", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "Visual Grounding from Event Cameras", "categories": ["cs.CV", "cs.RO"], "comment": "Abstract Paper (Non-Archival) @ ICCV 2025 NeVi Workshop", "summary": "Event cameras capture changes in brightness with microsecond precision and\nremain reliable under motion blur and challenging illumination, offering clear\nadvantages for modeling highly dynamic scenes. Yet, their integration with\nnatural language understanding has received little attention, leaving a gap in\nmultimodal perception. To address this, we introduce Talk2Event, the first\nlarge-scale benchmark for language-driven object grounding using event data.\nBuilt on real-world driving scenarios, Talk2Event comprises 5,567 scenes,\n13,458 annotated objects, and more than 30,000 carefully validated referring\nexpressions. Each expression is enriched with four structured attributes --\nappearance, status, relation to the viewer, and relation to surrounding objects\n-- that explicitly capture spatial, temporal, and relational cues. This\nattribute-centric design supports interpretable and compositional grounding,\nenabling analysis that moves beyond simple object recognition to contextual\nreasoning in dynamic environments. We envision Talk2Event as a foundation for\nadvancing multimodal and temporally-aware perception, with applications\nspanning robotics, human-AI interaction, and so on.", "AI": {"tldr": "\u63d0\u51faTalk2Event\uff0c\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u8bed\u8a00\u9a71\u52a8\u5bf9\u8c61\u5b9a\u4f4d\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u6db5\u76d6\u4e30\u5bcc\u573a\u666f\u548c\u6807\u6ce8\uff0c\u652f\u6301\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4e0e\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u7ed3\u5408\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u591a\u6a21\u6001\u611f\u77e5\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efaTalk2Event\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u6570\u636e\u3001\u5927\u91cf\u6807\u6ce8\u5bf9\u8c61\u548c\u9a8c\u8bc1\u8fc7\u7684\u5f15\u7528\u8868\u8fbe\u5f0f\uff0c\u8bbe\u8ba1\u4e86\u56db\u4e2a\u7ed3\u6784\u5316\u5c5e\u6027\u4ee5\u652f\u6301\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "result": "Talk2Event\u5305\u542b5,567\u4e2a\u573a\u666f\u300113,458\u4e2a\u6807\u6ce8\u5bf9\u8c61\u548c30,000\u591a\u4e2a\u5f15\u7528\u8868\u8fbe\u5f0f\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u7684\u7ec4\u5408\u5f0f\u5bf9\u8c61\u5b9a\u4f4d\u3002", "conclusion": "Talk2Event\u4e3a\u591a\u6a21\u6001\u548c\u65f6\u95f4\u611f\u77e5\u7684\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u3001\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u3002"}}
