{"id": "2510.26803", "pdf": "https://arxiv.org/pdf/2510.26803", "abs": "https://arxiv.org/abs/2510.26803", "authors": ["Hang Lin", "Liuxun Xue", "Shu Sun", "Ruifeng Gao", "Jue Wang", "Tengjiao Wang"], "title": "Investigation of Superdirectivity in Planar Holographic Arrays", "categories": ["eess.SP", "cs.ET", "cs.IT", "math.IT"], "comment": "in Chinese language", "summary": "This paper studies the superdirectivity characteristics of uniform\nrectangular arrays (URAs) for holographic multiple-input multiple-output\nsystems. By establishing a mathematical directivity model for the URA, an\nanalytical expression for the maximum directivity is derived. Accordingly,\nsystematic analysis is performed in conjunction with numerical simulations.\nResults show that the directivity can be significantly enhanced via rational\nutilization of coupling effects. However, this enhancement yields diminishing\nreturns when antenna spacings transition to deep sub-wavelength scales. This\nstudy provides a theoretical basis for the design of superdirective URAs and\noffers valuable insights for holographic array optimization in 5G/6G\ncommunication systems."}
{"id": "2510.26822", "pdf": "https://arxiv.org/pdf/2510.26822", "abs": "https://arxiv.org/abs/2510.26822", "authors": ["Yuanhang Qian", "Xueqin Luo", "Jilu Jin", "Gongping Huang", "Jingdong Chen", "Jacob Benesty"], "title": "Joint optimization of microphone array geometry, sensor directivity pattern, and beamforming parameters for linear superarrays", "categories": ["eess.SP"], "comment": null, "summary": "Linear superarrays (LSAs) have been proposed to address the limited steering\ncapability of conventional linear differential microphone arrays (LDMAs) by\nintegrating omnidirectional and directional microphones, enabling more flexible\nbeamformer designs. However, existing approaches remain limited because array\ngeometry and element directivity, both critical to beamforming performance, are\nnot jointly optimized. This paper presents a generalized LSA optimization\nframework that simultaneously optimizes array geometry, element directivity,\nand the beamforming filter to minimize the approximation error between the\ndesigned beampattern and an ideal directivity pattern (IDP) over the full\nfrequency band and all steering directions within the region of interest. The\nbeamformer is derived by approximating the IDP using a Jacobi-Anger series\nexpansion, while the array geometry and element directivity are optimized via a\ngenetic algorithm. Simulation results show that the proposed optimized array\nachieves lower approximation error than conventional LSAs across the target\nfrequency band and steering range. Additionally, its directivity factor and\nwhite noise gain demonstrate more stable and improved performance across\nfrequencies and steering angles."}
{"id": "2510.27040", "pdf": "https://arxiv.org/pdf/2510.27040", "abs": "https://arxiv.org/abs/2510.27040", "authors": ["Dian Chen", "Yunkai Chen", "Tong Lin", "Sijie Chen", "Xiaolin Cheng"], "title": "GeoPep: A geometry-aware masked language model for protein-peptide binding site prediction", "categories": ["eess.SP", "cs.LG"], "comment": "11 pages, 5 figures", "summary": "Multimodal approaches that integrate protein structure and sequence have\nachieved remarkable success in protein-protein interface prediction. However,\nextending these methods to protein-peptide interactions remains challenging due\nto the inherent conformational flexibility of peptides and the limited\navailability of structural data that hinder direct training of structure-aware\nmodels. To address these limitations, we introduce GeoPep, a novel framework\nfor peptide binding site prediction that leverages transfer learning from ESM3,\na multimodal protein foundation model. GeoPep fine-tunes ESM3's rich\npre-learned representations from protein-protein binding to address the limited\navailability of protein-peptide binding data. The fine-tuned model is further\nintegrated with a parameter-efficient neural network architecture capable of\nlearning complex patterns from sparse data. Furthermore, the model is trained\nusing distance-based loss functions that exploit 3D structural information to\nenhance binding site prediction. Comprehensive evaluations demonstrate that\nGeoPep significantly outperforms existing methods in protein-peptide binding\nsite prediction by effectively capturing sparse and heterogeneous binding\npatterns."}
{"id": "2510.27043", "pdf": "https://arxiv.org/pdf/2510.27043", "abs": "https://arxiv.org/abs/2510.27043", "authors": ["Hao Jiang", "Xiaojun Yuan", "Yinuo Huang", "Qinghua Guo"], "title": "Blind MIMO Semantic Communication via Parallel Variational Diffusion: A Completely Pilot-Free Approach", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we propose a novel blind multi-input multi-output (MIMO)\nsemantic communication (SC) framework named Blind-MIMOSC that consists of a\ndeep joint source-channel coding (DJSCC) transmitter and a diffusion-based\nblind receiver. The DJSCC transmitter aims to compress and map the source data\ninto the transmitted signal by exploiting the structural characteristics of the\nsource data, while the diffusion-based blind receiver employs a parallel\nvariational diffusion (PVD) model to simultaneously recover the channel and the\nsource data from the received signal without using any pilots. The PVD model\nleverages two pre-trained score networks to characterize the prior information\nof the channel and the source data, operating in a plug-and-play manner during\ninference. This design allows only the affected network to be retrained when\nchannel conditions or source datasets change, avoiding the complicated\nfull-network retraining required by end-to-end methods. This work presents the\nfirst fully pilot-free solution for joint channel estimation and source\nrecovery in block-fading MIMO systems. Extensive experiments show that\nBlind-MIMOSC with PVD achieves superior channel and source recovery accuracy\ncompared to state-of-the-art approaches, with drastically reduced channel\nbandwidth ratio."}
{"id": "2510.26837", "pdf": "https://arxiv.org/pdf/2510.26837", "abs": "https://arxiv.org/abs/2510.26837", "authors": ["Conor K. Trygstad", "Nestor O. Perez-Arancibia"], "title": "Force Characterization of Insect-Scale Aquatic Propulsion Based on Fluid-Structure Interaction", "categories": ["cs.RO", "physics.flu-dyn"], "comment": "To be presented at ICAR 2025 in San Juan, Argentina", "summary": "We present force characterizations of two newly developed insect-scale\npropulsors--one single-tailed and one double-tailed--for microrobotic swimmers\nthat leverage fluid-structure interaction (FSI) to generate thrust. The designs\nof these two devices were inspired by anguilliform swimming and are driven by\nsoft tails excited by high-work-density (HWD) actuators powered by shape-memory\nalloy (SMA) wires. While these propulsors have been demonstrated to be suitable\nfor microrobotic aquatic locomotion and controllable with simple architectures\nfor trajectory tracking in the two-dimensional (2D) space, the characteristics\nand magnitudes of the associated forces have not been studied systematically.\nIn the research presented here, we adopted a theoretical framework based on the\nnotion of reactive forces and obtained experimental data for characterization\nusing a custom-built micro-N-resolution force sensor. We measured maximum and\ncycle-averaged force values with multi-test means of respectively 0.45 mN and\n2.97 micro-N, for the tested single-tail propulsor. For the dual-tail\npropulsor, we measured maximum and cycle-averaged force values with multi-test\nmeans of 0.61 mN and 22.6 micro-N, respectively. These results represent the\nfirst measurements of the instantaneous thrust generated by insect-scale\npropulsors of this type and provide insights into FSI for efficient\nmicrorobotic propulsion."}
{"id": "2510.27069", "pdf": "https://arxiv.org/pdf/2510.27069", "abs": "https://arxiv.org/abs/2510.27069", "authors": ["Mohammad Hossein Shokouhi", "Vincent W. S. Wong"], "title": "Distributed Precoding for Cell-free Massive MIMO in O-RAN: A Multi-agent Deep Reinforcement Learning Framework", "categories": ["eess.SP"], "comment": null, "summary": "Cell-free massive multiple-input multiple-output (MIMO) is a key technology\nfor next-generation wireless systems. The integration of cell-free massive MIMO\nwithin the open radio access network (O-RAN) architecture addresses the growing\nneed for decentralized, scalable, and high-capacity networks that can support\ndifferent use cases. Precoding is a crucial step in the operation of cell-free\nmassive MIMO, where O-RUs steer their beams towards the intended users while\nmitigating interference to other users. Current precoding schemes for cell-free\nmassive MIMO are either fully centralized or fully distributed. Centralized\nschemes are not scalable, whereas distributed schemes may lead to a high\ninter-O-RU interference. In this paper, we propose a distributed and scalable\nprecoding framework for cell-free massive MIMO that uses limited information\nexchange among precoding agents to mitigate interference. We formulate an\noptimization problem for precoding that maximizes the aggregate throughput\nwhile guaranteeing the minimum data rate requirements of users. The formulated\nproblem is nonconvex. We propose a multi-timescale framework that combines\nmulti-agent deep reinforcement learning (DRL) with expert insights from an\niterative algorithm to determine the precoding matrices efficiently. We conduct\nsimulations and compare the proposed framework with the centralized precoding\nand distributed precoding methods for different numbers of O-RUs, users, and\ntransmit antennas. The results show that the proposed framework achieves a\nhigher aggregate throughput than the distributed regularized zero-forcing\n(D-RZF) scheme and the weighted minimum mean square error (WMMSE) algorithm.\nWhen compared with the centralized regularized zero-forcing (C-RZF) scheme, the\nproposed framework achieves similar aggregate throughput performance but with a\nlower signaling overhead."}
{"id": "2510.26855", "pdf": "https://arxiv.org/pdf/2510.26855", "abs": "https://arxiv.org/abs/2510.26855", "authors": ["Reihaneh Mirjalili"], "title": "Leveraging Foundation Models for Enhancing Robot Perception and Action", "categories": ["cs.RO", "cs.AI"], "comment": "Doctoral thesis", "summary": "This thesis investigates how foundation models can be systematically\nleveraged to enhance robotic capabilities, enabling more effective\nlocalization, interaction, and manipulation in unstructured environments. The\nwork is structured around four core lines of inquiry, each addressing a\nfundamental challenge in robotics while collectively contributing to a cohesive\nframework for semantics-aware robotic intelligence."}
{"id": "2510.27078", "pdf": "https://arxiv.org/pdf/2510.27078", "abs": "https://arxiv.org/abs/2510.27078", "authors": ["Meles Weldegebriel", "Zihan Li", "Greg Hellbourg", "Ning Zhang", "Neal Patwari"], "title": "RFI Detection and Identification at OVRO Using Pseudonymetry", "categories": ["eess.SP"], "comment": null, "summary": "Protecting radio astronomy observatories from unintended interference is\ncritical as wireless transmissions increases near protected bands. While\ndatabase-driven coordination frameworks and radio quiet zones exist, they\ncannot rapidly identify or suppress specific interfering transmitters,\nespecially at low signal-to-noise ratio (SNR) levels. This paper presents the\nfirst over-the-air field demonstration of Pseudonymetry at the Owens Valley\nRadio Observatory (OVRO), illustrating cooperative spectrum sharing between\nheterogeneous wireless systems. In our experiment, a narrow-band secondary\ntransmitter embeds a pseudonym watermark into its signal, while the wide-band\nradio telescope passively extracts the watermark from spectrogram data. Results\nshow that interference can be reliably detected and the interfering device\nuniquely identified even at low SNR where conventional demodulation is\ninfeasible. These findings validate that passive scientific receivers can\nparticipate in a lightweight feedback loop to trigger shutdown of harmful\ntransmissions, demonstrating the potential of Pseudonymetry as a complementary\nenforcement tool for protecting radio astronomy environments."}
{"id": "2510.26900", "pdf": "https://arxiv.org/pdf/2510.26900", "abs": "https://arxiv.org/abs/2510.26900", "authors": ["Jahir Argote-Gerald", "Genki Miyauchi", "Julian Rau", "Paul Trodden", "Roderich Gross"], "title": "Design for One, Deploy for Many: Navigating Tree Mazes with Multiple Agents", "categories": ["cs.RO", "cs.MA"], "comment": "7 pages, 7 figures, to be published in MRS 2025", "summary": "Maze-like environments, such as cave and pipe networks, pose unique\nchallenges for multiple robots to coordinate, including communication\nconstraints and congestion. To address these challenges, we propose a\ndistributed multi-agent maze traversal algorithm for environments that can be\nrepresented by acyclic graphs. It uses a leader-switching mechanism where one\nagent, assuming a head role, employs any single-agent maze solver while the\nother agents each choose an agent to follow. The head role gets transferred to\nneighboring agents where necessary, ensuring it follows the same path as a\nsingle agent would. The multi-agent maze traversal algorithm is evaluated in\nsimulations with groups of up to 300 agents, various maze sizes, and multiple\nsingle-agent maze solvers. It is compared against strategies that are na\\\"ive,\nor assume either global communication or full knowledge of the environment. The\nalgorithm outperforms the na\\\"ive strategy in terms of makespan and\nsum-of-fuel. It is superior to the global-communication strategy in terms of\nmakespan but is inferior to it in terms of sum-of-fuel. The findings suggest it\nis asymptotically equivalent to the full-knowledge strategy with respect to\neither metric. Moreover, real-world experiments with up to 20 Pi-puck robots\nconfirm the feasibility of the approach."}
{"id": "2510.27110", "pdf": "https://arxiv.org/pdf/2510.27110", "abs": "https://arxiv.org/abs/2510.27110", "authors": ["Gal Shtendel", "Ayush Bhandari"], "title": "Unlimited Sampling of Multiband Signals: Single-Channel Acquisition and Recovery", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "IEEE Signal Processing Letters (in press)", "summary": "In this paper, we address the problem of reconstructing multiband signals\nfrom modulo-folded, pointwise samples within the Unlimited Sensing Framework\n(USF). Focusing on a low-complexity, single-channel acquisition setup, we\nestablish recovery guarantees demonstrating that sub-Nyquist sampling is\nachievable under the USF paradigm. In doing so, we also tighten the previous\nsampling theorem for bandpass signals. Our recovery algorithm demonstrates up\nto a 13x dynamic range improvement in hardware experiments with up to 6\nspectral bands. These results enable practical high-dynamic-range multiband\nacquisition in scenarios previously limited by dynamic range and excessive\noversampling."}
{"id": "2510.26909", "pdf": "https://arxiv.org/pdf/2510.26909", "abs": "https://arxiv.org/abs/2510.26909", "authors": ["Tim Windecker", "Manthan Patel", "Moritz Reuss", "Richard Schwarzkopf", "Cesar Cadena", "Rudolf Lioutikov", "Marco Hutter", "Jonas Frey"], "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models", "categories": ["cs.RO"], "comment": "9 pages, 6 figures, under review at IEEE conference", "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/."}
{"id": "2510.27192", "pdf": "https://arxiv.org/pdf/2510.27192", "abs": "https://arxiv.org/abs/2510.27192", "authors": ["Haoran Yin", "Yanqun Tang", "Jun Xiong", "Fan Liu", "Yuanhan Ni", "Qu Luo", "Roberto Bomfin", "Marwa Chafii", "Marios Kountouris", "Christos Masouros"], "title": "From OFDM to AFDM: Enabling Adaptive Integrated Sensing and Communication in High-Mobility Scenarios", "categories": ["eess.SP"], "comment": "Magazine paper submitted to IEEE", "summary": "Integrated sensing and communication (ISAC) is a key feature of\nnext-generation wireless networks, enabling a wide range of emerging\napplications such as vehicle-to-everything (V2X) and unmanned aerial vehicles\n(UAVs), which operate in high-mobility scenarios. Notably, the wireless\nchannels within these applications typically exhibit severe delay and Doppler\nspreads. The latter causes serious communication performance degradation in the\nOrthogonal Frequency-Division Multiplexing (OFDM) waveform that is widely\nadopted in current wireless networks. To address this challenge, the recently\nproposed Doppler-resilient affine frequency division multiplexing (AFDM)\nwaveform, which uses flexible chirp signals as subcarriers, shows great\npotential for achieving adaptive ISAC in high-mobility scenarios. This article\nprovides a comprehensive overview of AFDM-ISAC. We begin by presenting the\nfundamentals of AFDM-ISAC, highlighting its inherent frequency-modulated\ncontinuous-wave (FMCW)-like characteristics. Then, we explore its ISAC\nperformance limits by analyzing its diversity order, ambiguity function (AF),\nand Cramer-Rao Bound (CRB). Finally, we present several effective sensing\nalgorithms and opportunities for AFDM-ISAC, with the aim of sparking new ideas\nin this emerging field."}
{"id": "2510.26915", "pdf": "https://arxiv.org/pdf/2510.26915", "abs": "https://arxiv.org/abs/2510.26915", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Ankit Prabhu", "Jason Hughes", "Varun Murali", "Camillo Taylor", "George J. Pappas", "Vijay Kumar"], "title": "Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Heterogeneous robot teams operating in realistic settings often must\naccomplish complex missions requiring collaboration and adaptation to\ninformation acquired online. Because robot teams frequently operate in\nunstructured environments -- uncertain, open-world settings without prior maps\n-- subtasks must be grounded in robot capabilities and the physical world.\nWhile heterogeneous teams have typically been designed for fixed\nspecifications, generative intelligence opens the possibility of teams that can\naccomplish a wide range of missions described in natural language. However,\ncurrent large language model (LLM)-enabled teaming methods typically assume\nwell-structured and known environments, limiting deployment in unstructured\nenvironments. We present SPINE-HT, a framework that addresses these limitations\nby grounding the reasoning abilities of LLMs in the context of a heterogeneous\nrobot team through a three-stage process. Given language specifications\ndescribing mission goals and team capabilities, an LLM generates grounded\nsubtasks which are validated for feasibility. Subtasks are then assigned to\nrobots based on capabilities such as traversability or perception and refined\ngiven feedback collected during online operation. In simulation experiments\nwith closed-loop perception and control, our framework achieves nearly twice\nthe success rate compared to prior LLM-enabled heterogeneous teaming\napproaches. In real-world experiments with a Clearpath Jackal, a Clearpath\nHusky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an\n87\\% success rate in missions requiring reasoning about robot capabilities and\nrefining subtasks with online feedback. More information is provided at\nhttps://zacravichandran.github.io/SPINE-HT."}
{"id": "2510.27217", "pdf": "https://arxiv.org/pdf/2510.27217", "abs": "https://arxiv.org/abs/2510.27217", "authors": ["Boxuan Xie", "Lauri Mela", "Alexis A. Dowhuszko", "Yu Bai", "Zehui Xiong", "Zhu Han", "Dusit Niyato", "Riku Jäntti"], "title": "Joint Visible Light and Backscatter Communications for Proximity-Based Indoor Asset Tracking Enabled by Energy-Neutral Devices", "categories": ["eess.SP", "cs.ET"], "comment": "14 pages, 14 figures, 4 tables", "summary": "In next-generation wireless systems, providing location-based mobile\ncomputing services for energy-neutral devices has become a crucial objective\nfor the provision of sustainable Internet of Things (IoT). Visible light\npositioning (VLP) has gained great research attention as a complementary method\nto radio frequency (RF) solutions since it can leverage ubiquitous lighting\ninfrastructure. However, conventional VLP receivers often rely on\nphotodetectors or cameras that are power-hungry, complex, and expensive. To\naddress this challenge, we propose a hybrid indoor asset tracking system that\nintegrates visible light communication (VLC) and backscatter communication (BC)\nwithin a simultaneous lightwave information and power transfer (SLIPT)\nframework. We design a low-complexity and energy-neutral IoT node, namely\nbackscatter device (BD) which harvests energy from light-emitting diode (LED)\naccess points, and then modulates and reflects ambient RF carriers to indicate\nits location within particular VLC cells. We present a multi-cell VLC\ndeployment with frequency division multiplexing (FDM) method that mitigates\ninterference among LED access points by assigning them distinct frequency pairs\nbased on a four-color map scheduling principle. We develop a lightweight\nparticle filter (PF) tracking algorithm at an edge RF reader, where the fusion\nof proximity reports and the received backscatter signal strength are employed\nto track the BD. Experimental results show that this approach achieves the\npositioning error of 0.318 m at 50th percentile and 0.634 m at 90th percentile,\nwhile avoiding the use of complex photodetectors and active RF synthesizing\ncomponents at the energy-neutral IoT node. By demonstrating robust performance\nin multiple indoor trajectories, the proposed solution enables scalable,\ncost-effective, and energy-neutral indoor tracking for pervasive and\nedge-assisted IoT applications."}
{"id": "2510.26935", "pdf": "https://arxiv.org/pdf/2510.26935", "abs": "https://arxiv.org/abs/2510.26935", "authors": ["Yunhao Yang", "Neel P. Bhatt", "Pranay Samineni", "Rohan Siva", "Zhanyang Wang", "Ufuk Topcu"], "title": "RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.FL"], "comment": "Code and data are available at: https://repv-project.github.io/", "summary": "As AI systems migrate to safety-critical domains, verifying that their\nactions comply with well-defined rules remains a challenge. Formal methods\nprovide provable guarantees but demand hand-crafted temporal-logic\nspecifications, offering limited expressiveness and accessibility. Deep\nlearning approaches enable evaluation of plans against natural-language\nconstraints, yet their opaque decision process invites misclassifications with\npotentially severe consequences. We introduce RepV, a neurosymbolic verifier\nthat unifies both views by learning a latent space where safe and unsafe plans\nare linearly separable. Starting from a modest seed set of plans labeled by an\noff-the-shelf model checker, RepV trains a lightweight projector that embeds\neach plan, together with a language model-generated rationale, into a\nlow-dimensional space; a frozen linear boundary then verifies compliance for\nunseen natural-language rules in a single forward pass.\n  Beyond binary classification, RepV provides a probabilistic guarantee on the\nlikelihood of correct verification based on its position in the latent space.\nThis guarantee enables a guarantee-driven refinement of the planner, improving\nrule compliance without human annotations. Empirical evaluations show that RepV\nimproves compliance prediction accuracy by up to 15% compared to baseline\nmethods while adding fewer than 0.2M parameters. Furthermore, our refinement\nframework outperforms ordinary fine-tuning baselines across various planning\ndomains. These results show that safety-separable latent spaces offer a\nscalable, plug-and-play primitive for reliable neurosymbolic plan verification.\nCode and data are available at: https://repv-project.github.io/."}
{"id": "2510.27270", "pdf": "https://arxiv.org/pdf/2510.27270", "abs": "https://arxiv.org/abs/2510.27270", "authors": ["Yida Zhang", "Qiuyan Liu", "Yuqi Xia", "Guoxu Xia", "Qiang Wang"], "title": "SIM-Assisted End-to-End Co-Frequency Co-Time Full-Duplex System", "categories": ["eess.SP"], "comment": null, "summary": "To further suppress the inherent self-interference (SI) in co-frequency and\nco-time full-duplex (CCFD) systems, we propose integrating a stacked\nintelligent metasurface (SIM) into the RF front-end to enhance signal\nprocessing in the wave domain. Furthermore, an end-to-end (E2E) learning-based\nsignal processing method is adopted to control the metasurface. Specifically,\nthe real metasurface is abstracted as hidden layers of a network, thereby\nconstructing an electromagnetic neural network (EMNN) to enable driving control\nof the real communication system. Traditional communication tasks, such as\nchannel coding, modulation, precoding, combining, demodulation, and channel\ndecoding, are synchronously carried out during the electromagnetic (EM) forward\npropagation through the metasurface. Simulation results show that, benefiting\nfrom the additional wave-domain processing capability of the SIM, the\nSIM-assisted CCFD system achieves significantly reduced bit error rate (BER)\ncompared with conventional CCFD systems. Our study fully demonstrates the\npotential applications of EMNN and SIM-assisted E2E CCFD systems in\nnext-generation transceiver design."}
{"id": "2510.27010", "pdf": "https://arxiv.org/pdf/2510.27010", "abs": "https://arxiv.org/abs/2510.27010", "authors": ["William E. Heap", "Yimeng Qin", "Kai Hammond", "Anish Bayya", "Haonon Kong", "Allison M. Okamura"], "title": "A Hermetic, Transparent Soft Growing Vine Robot System for Pipe Inspection", "categories": ["cs.RO"], "comment": "8 pages, 7 figures", "summary": "Rehabilitation of aging pipes requires accurate condition assessment and\nmapping far into the pipe interiors. Soft growing vine robot systems are\nparticularly promising for navigating confined, sinuous paths such as in pipes,\nbut are currently limited by complex subsystems and a lack of validation in\nreal-world industrial settings. In this paper, we introduce the concept and\nimplementation of a hermetic and transparent vine robot system for visual\ncondition assessment and mapping within non-branching pipes. This design\nencloses all mechanical and electrical components within the vine robot's soft,\nairtight, and transparent body, protecting them from environmental interference\nwhile enabling visual sensing. Because this approach requires an enclosed\nmechanism for transporting sensors, we developed, modeled, and tested a\npassively adapting enclosed tip mount. Finally, we validated the hermetic and\ntransparent vine robot system concept through a real-world condition assessment\nand mapping task in a wastewater pipe. This work advances the use of\nsoft-growing vine robots in pipe inspection by developing and demonstrating a\nrobust, streamlined, field-validated system suitable for continued development\nand deployment."}
{"id": "2510.27345", "pdf": "https://arxiv.org/pdf/2510.27345", "abs": "https://arxiv.org/abs/2510.27345", "authors": ["Anders Malthe Westerkam", "Amélia Struyf", "Dimitri Lederer", "Troels Pedersen", "François Quitin"], "title": "Variational Bayesian Estimation of Low Earth Orbits for Satellite Communication", "categories": ["eess.SP"], "comment": null, "summary": "Low-earth-orbit (LEO) satellite communication systems that use\nmillimeter-wave frequencies rely on large antenna arrays with hybrid\nanalog-digital architectures for rapid beam steering. LEO satellites are only\nvisible from the ground for short periods of times (a few tens of minutes) due\nto their high orbital speeds. This paper presents a variational message passing\nalgorithm for joint localization and beam tracking of a LEO satellite from a\nground station equipped with a hybrid transceiver architecture. The algorithm\nrelies on estimating the parameters of the orbit, which is modelled as\ncircular. Angles are then obtained from the orbit in a straightforward manner.\nSimulation results show that the proposed method is highly resilient to missed\ndetections, enables reliable satellite tracking even near the horizon, and\neffectively alleviates the ambiguities inherent in hybrid architectures."}
{"id": "2510.27033", "pdf": "https://arxiv.org/pdf/2510.27033", "abs": "https://arxiv.org/abs/2510.27033", "authors": ["Simindokht Jahangard", "Mehrzad Mohammadi", "Abhinav Dhall", "Hamid Rezatofighi"], "title": "A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Visual reasoning, particularly spatial reasoning, is a challenging cognitive\ntask that requires understanding object relationships and their interactions\nwithin complex environments, especially in robotics domain. Existing\nvision_language models (VLMs) excel at perception tasks but struggle with\nfine-grained spatial reasoning due to their implicit, correlation-driven\nreasoning and reliance solely on images. We propose a novel neuro_symbolic\nframework that integrates both panoramic-image and 3D point cloud information,\ncombining neural perception with symbolic reasoning to explicitly model spatial\nand logical relationships. Our framework consists of a perception module for\ndetecting entities and extracting attributes, and a reasoning module that\nconstructs a structured scene graph to support precise, interpretable queries.\nEvaluated on the JRDB-Reasoning dataset, our approach demonstrates superior\nperformance and reliability in crowded, human_built environments while\nmaintaining a lightweight design suitable for robotics and embodied AI\napplications."}
{"id": "2510.27371", "pdf": "https://arxiv.org/pdf/2510.27371", "abs": "https://arxiv.org/abs/2510.27371", "authors": ["Sagar Dutta", "Banani Basu", "Fazal Ahmed Talukdar"], "title": "Classification of Lower Limb Activities Based on Discrete Wavelet Transform Using On-Body Creeping Wave Propagation", "categories": ["eess.SP"], "comment": null, "summary": "This article investigates how the creeping wave propagation around the human\nthigh could be used to monitor the leg movements. The propagation path around\nthe human thigh gives information regarding leg motions that can be used for\nthe classification of activities. The variation of the transmission coefficient\nis measured between two on-body polyethylene terephthalate (PET) flexible\nantennas for six different leg-based activities that exhibit unique\ntime-varying signatures. A discrete wavelet transform (DWT) along with\ndifferent classifiers, such as support vector machine (SVM), decision trees,\nnaive Bayes, and K-nearest neighbors (KNN), is applied for feature extraction\nand classification to evaluate the efficiency for classifying different\nactivity signals. Additional algorithms, such as dynamic time warping (DTW) and\ndeep convolutional neural network (DCNN), have also been implemented, and in\neach case, SVM with DWT outperforms the others. Simulation to evaluate a\nspecific absorption rate (SAR) is carried out as the antenna is positioned on\nthe human thigh leaving no gap. The results show that the SAR is within the\nthreshold as per the Federal Communications Commission (FCC) standard."}
{"id": "2510.27048", "pdf": "https://arxiv.org/pdf/2510.27048", "abs": "https://arxiv.org/abs/2510.27048", "authors": ["Eric T. Chang", "Peter Ballentine", "Zhanpeng He", "Do-Gon Kim", "Kai Jiang", "Hua-Hsuan Liang", "Joaquin Palacios", "William Wang", "Pedro Piacenza", "Ioannis Kymissis", "Matei Ciocarlie"], "title": "SpikeATac: A Multimodal Tactile Finger with Taxelized Dynamic Sensing for Dexterous Manipulation", "categories": ["cs.RO"], "comment": "9 pages, 8 figures, under review", "summary": "In this work, we introduce SpikeATac, a multimodal tactile finger combining a\ntaxelized and highly sensitive dynamic response (PVDF) with a static\ntransduction method (capacitive) for multimodal touch sensing. Named for its\n`spiky' response, SpikeATac's 16-taxel PVDF film sampled at 4 kHz provides\nfast, sensitive dynamic signals to the very onset and breaking of contact. We\ncharacterize the sensitivity of the different modalities, and show that\nSpikeATac provides the ability to stop quickly and delicately when grasping\nfragile, deformable objects. Beyond parallel grasping, we show that SpikeATac\ncan be used in a learning-based framework to achieve new capabilities on a\ndexterous multifingered robot hand. We use a learning recipe that combines\nreinforcement learning from human feedback with tactile-based rewards to\nfine-tune the behavior of a policy to modulate force. Our hardware platform and\nlearning pipeline together enable a difficult dexterous and contact-rich task\nthat has not previously been achieved: in-hand manipulation of fragile objects.\nVideos are available at\n\\href{https://roamlab.github.io/spikeatac/}{roamlab.github.io/spikeatac}."}
{"id": "2510.27382", "pdf": "https://arxiv.org/pdf/2510.27382", "abs": "https://arxiv.org/abs/2510.27382", "authors": ["Sagar Dutta", "Banani Basu", "Fazal Ahmed Talukdar"], "title": "Classification of Induction Motor Fault and Imbalance Based on Vibration Signal Using Single Antenna's Reactive Near Field", "categories": ["eess.SP"], "comment": null, "summary": "Early fault diagnosis is imperative for the proper functioning of rotating\nmachines. It can reduce economic losses in the industry due to unexpected\nfailures. Existing fault analysis methods are either expensive or demand\nexpertise for the installation of the sensors. This article proposes a novel\nmethod for the detection of bearing faults and imbalance in induction motors\nusing an antenna as the sensor, which is noninvasive and cost-efficient.\nTime-varying S11 is measured using an omnidirectional antenna, and it is seen\nthat the spectrogram of S11 shows unique characteristics for different fault\nconditions. The experimental setup has analytically evaluated the vibration\nfrequencies due to fault and validated the characteristic fault frequency by\napplying FFT analysis on the captured S11 data. This article has evaluated the\naverage power content of the detected signals at normal and different fault\nconditions. A deep learning model is used to classify the faults based on the\nreflection coefficient ( S11). It is found that classification accuracy of\n98.2% is achieved using both magnitude and phase of S11, 96% using the\nmagnitude of S11 and 92.1% using the phase of S11. The classification accuracy\nfor different operating frequencies, antenna location, and time windows are\nalso investigated."}
{"id": "2510.27114", "pdf": "https://arxiv.org/pdf/2510.27114", "abs": "https://arxiv.org/abs/2510.27114", "authors": ["Dohyeok Lee", "Jung Min Lee", "Munkyung Kim", "Seokhun Ju", "Jin Woo Koo", "Kyungjae Lee", "Dohyeong Kim", "TaeHyun Cho", "Jungwoo Lee"], "title": "Learning Generalizable Visuomotor Policy through Dynamics-Alignment", "categories": ["cs.RO", "cs.LG"], "comment": "9 pages, 6 figures", "summary": "Behavior cloning methods for robot learning suffer from poor generalization\ndue to limited data support beyond expert demonstrations. Recent approaches\nleveraging video prediction models have shown promising results by learning\nrich spatiotemporal representations from large-scale datasets. However, these\nmodels learn action-agnostic dynamics that cannot distinguish between different\ncontrol inputs, limiting their utility for precise manipulation tasks and\nrequiring large pretraining datasets. We propose a Dynamics-Aligned Flow\nMatching Policy (DAP) that integrates dynamics prediction into policy learning.\nOur method introduces a novel architecture where policy and dynamics models\nprovide mutual corrective feedback during action generation, enabling\nself-correction and improved generalization. Empirical validation demonstrates\ngeneralization performance superior to baseline methods on real-world robotic\nmanipulation tasks, showing particular robustness in OOD scenarios including\nvisual distractions and lighting variations."}
{"id": "2510.27394", "pdf": "https://arxiv.org/pdf/2510.27394", "abs": "https://arxiv.org/abs/2510.27394", "authors": ["Yuhao Zhang", "Guangjin Pan", "Musa Furkan Keskin", "Ossi Kaltiokallio", "Mikko Valkama", "Henk Wymeersch"], "title": "UNILocPro: Unified Localization Integrating Model-Based Geometry and Channel Charting", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we propose a unified localization framework (called UNILocPro)\nthat integrates model-based localization and channel charting (CC) for mixed\nline-of-sight (LoS)/non-line-of-sight (NLoS) scenarios. Specifically, based on\nLoS/NLoS identification, an adaptive activation between the model-based and\nCC-based methods is conducted. Aiming for unsupervised learning, information\nobtained from the model-based method is utilized to train the CC model, where a\npairwise distance loss (involving a new dissimilarity metric design), a triplet\nloss (if timestamps are available), a LoS-based loss, and an optimal transport\n(OT)-based loss are jointly employed such that the global geometry can be well\npreserved. To reduce the training complexity of UNILocPro, we propose a\nlow-complexity implementation (called UNILoc), where the CC model is trained\nwith self-generated labels produced by a single pre-training OT transformation,\nwhich avoids iterative Sinkhorn updates involved in the OT-based loss\ncomputation. Extensive numerical experiments demonstrate that the proposed\nunified frameworks achieve significantly improved positioning accuracy compared\nto both model-based and CC-based methods. Notably, UNILocPro with timestamps\nattains performance on par with fully-supervised fingerprinting despite\noperating without labelled training data. It is also shown that the\nlow-complexity UNILoc can substantially reduce training complexity with only\nmarginal performance degradation."}
{"id": "2510.27151", "pdf": "https://arxiv.org/pdf/2510.27151", "abs": "https://arxiv.org/abs/2510.27151", "authors": ["Xueliang Cheng", "Kanzhong Yao", "Andrew West", "Ognjen Marjanovic", "Barry Lennox", "Keir Groves"], "title": "Confined Space Underwater Positioning Using Collaborative Robots", "categories": ["cs.RO", "/"], "comment": "31 pages including appendix, 24 figures", "summary": "Positioning of underwater robots in confined and cluttered spaces remains a\nkey challenge for field operations. Existing systems are mostly designed for\nlarge, open-water environments and struggle in industrial settings due to poor\ncoverage, reliance on external infrastructure, and the need for feature-rich\nsurroundings. Multipath effects from continuous sound reflections further\ndegrade signal quality, reducing accuracy and reliability. Accurate and easily\ndeployable positioning is essential for repeatable autonomous missions;\nhowever, this requirement has created a technological bottleneck limiting\nunderwater robotic deployment. This paper presents the Collaborative Aquatic\nPositioning (CAP) system, which integrates collaborative robotics and sensor\nfusion to overcome these limitations. Inspired by the \"mother-ship\" concept,\nthe surface vehicle acts as a mobile leader to assist in positioning a\nsubmerged robot, enabling localization even in GPS-denied and highly\nconstrained environments. The system is validated in a large test tank through\nrepeatable autonomous missions using CAP's position estimates for real-time\ntrajectory control. Experimental results demonstrate a mean Euclidean distance\n(MED) error of 70 mm, achieved in real time without requiring fixed\ninfrastructure, extensive calibration, or environmental features. CAP leverages\nadvances in mobile robot sensing and leader-follower control to deliver a step\nchange in accurate, practical, and infrastructure-free underwater localization."}
{"id": "2510.27408", "pdf": "https://arxiv.org/pdf/2510.27408", "abs": "https://arxiv.org/abs/2510.27408", "authors": ["Nelson Mattié", "Arturo Sanchez-Azofeifa", "Pablo Crespo-Peremarch", "Juan-Ygnacio López-Hernández"], "title": "Estimation of aboveground biomass in a tropical dry forest: An intercomparison of airborne, unmanned, and space laser scanning", "categories": ["eess.SP", "cs.LG"], "comment": "32 pages, 17 figures, research paper", "summary": "According to the Paris Climate Change Agreement, all nations are required to\nsubmit reports on their greenhouse gas emissions and absorption every two years\nby 2024. Consequently, forests play a crucial role in reducing carbon\nemissions, which is essential for meeting these obligations. Recognizing the\nsignificance of forest conservation in the global battle against climate\nchange, Article 5 of the Paris Agreement emphasizes the need for high-quality\nforest data. This study focuses on enhancing methods for mapping aboveground\nbiomass in tropical dry forests. Tropical dry forests are considered one of the\nleast understood tropical forest environments; therefore, there is a need for\naccurate approaches to estimate carbon pools. We employ a comparative analysis\nof AGB estimates, utilizing different discrete and full-waveform laser scanning\ndatasets in conjunction with Ordinary Least Squares and Bayesian approaches\nSVM. Airborne Laser Scanning, Unmanned Laser Scanning, and Space Laser Scanning\nwere used as independent variables for extracting forest metrics. Variable\nselection, SVM regression tuning, and cross-validation via a machine-learning\napproach were applied to account for overfitting and underfitting. The results\nindicate that six key variables primarily related to tree height: Elev.minimum,\nElev.L3, lev.MAD.mode, Elev.mode, Elev.MAD.median, and Elev.skewness, are\nimportant for AGB estimation using ALSD and ULSD , while Leaf Area Index,\ncanopy coverage and height, terrain elevation, and full-waveform signal energy\nemerged as the most vital variables. AGB values estimated from ten permanent\ntropical dry forest plots in Costa Rica Guanacaste province ranged from 26.02\nMg/ha to 175.43 Mg/ha . The SVM regressions demonstrated a 17.89 error across\nall laser scanning systems, with SLSF W exhibiting the lowest error 17.07 in\nestimating total biomass per plot."}
{"id": "2510.27178", "pdf": "https://arxiv.org/pdf/2510.27178", "abs": "https://arxiv.org/abs/2510.27178", "authors": ["Xuan-Thuan Nguyen", "Khac Nam Nguyen", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Hoang Hiep Ly", "Tung D. Ta"], "title": "MobiDock: Design and Control of A Modular Self Reconfigurable Bimanual Mobile Manipulator via Robotic Docking", "categories": ["cs.RO"], "comment": "ICRA2026 submited", "summary": "Multi-robot systems, particularly mobile manipulators, face challenges in\ncontrol coordination and dynamic stability when working together. To address\nthis issue, this study proposes MobiDock, a modular self-reconfigurable mobile\nmanipulator system that allows two independent robots to physically connect and\nform a unified mobile bimanual platform. This process helps transform a complex\nmulti-robot control problem into the management of a simpler, single system.\nThe system utilizes an autonomous docking strategy based on computer vision\nwith AprilTag markers and a new threaded screw-lock mechanism. Experimental\nresults show that the docked configuration demonstrates better performance in\ndynamic stability and operational efficiency compared to two independently\ncooperating robots. Specifically, the unified system has lower Root Mean Square\n(RMS) Acceleration and Jerk values, higher angular precision, and completes\ntasks significantly faster. These findings confirm that physical\nreconfiguration is a powerful design principle that simplifies cooperative\ncontrol, improving stability and performance for complex tasks in real-world\nenvironments."}
{"id": "2510.27503", "pdf": "https://arxiv.org/pdf/2510.27503", "abs": "https://arxiv.org/abs/2510.27503", "authors": ["Anubhab Ghosh", "Yonina C. Eldar", "Saikat Chatterjee"], "title": "pDANSE: Particle-based Data-driven Nonlinear State Estimation from Nonlinear Measurements", "categories": ["eess.SP", "cs.LG"], "comment": "11 pages, 10 figures, under review at IEEE Transactions on Signal\n  Processing", "summary": "We consider the problem of designing a data-driven nonlinear state estimation\n(DANSE) method that uses (noisy) nonlinear measurements of a process whose\nunderlying state transition model (STM) is unknown. Such a process is referred\nto as a model-free process. A recurrent neural network (RNN) provides\nparameters of a Gaussian prior that characterize the state of the model-free\nprocess, using all previous measurements at a given time point. In the case of\nDANSE, the measurement system was linear, leading to a closed-form solution for\nthe state posterior. However, the presence of a nonlinear measurement system\nrenders a closed-form solution infeasible. Instead, the second-order statistics\nof the state posterior are computed using the nonlinear measurements observed\nat the time point. We address the nonlinear measurements using a\nreparameterization trick-based particle sampling approach, and estimate the\nsecond-order statistics of the state posterior. The proposed method is referred\nto as particle-based DANSE (pDANSE). The RNN of pDANSE uses sequential\nmeasurements efficiently and avoids the use of computationally intensive\nsequential Monte-Carlo (SMC) and/or ancestral sampling. We describe the\nsemi-supervised learning method for pDANSE, which transitions to unsupervised\nlearning in the absence of labeled data. Using a stochastic Lorenz-$63$ system\nas a benchmark process, we experimentally demonstrate the state estimation\nperformance for four nonlinear measurement systems. We explore cubic\nnonlinearity and a camera-model nonlinearity where unsupervised learning is\nused; then we explore half-wave rectification nonlinearity and\nCartesian-to-spherical nonlinearity where semi-supervised learning is used. The\nperformance of state estimation is shown to be competitive vis-\\`a-vis particle\nfilters that have complete knowledge of the STM of the Lorenz-$63$ system."}
{"id": "2510.27184", "pdf": "https://arxiv.org/pdf/2510.27184", "abs": "https://arxiv.org/abs/2510.27184", "authors": ["Hoang Hiep Ly", "Cong-Nhat Nguyen", "Doan-Quang Tran", "Quoc-Khanh Dang", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Xuan-Thuan Nguyen", "Tung D. Ta"], "title": "Hybrid Gripper Finger Enabling In-Grasp Friction Modulation Using Inflatable Silicone Pockets", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "Grasping objects with diverse mechanical properties, such as heavy, slippery,\nor fragile items, remains a significant challenge in robotics. Conventional\ngrippers often rely on applying high normal forces, which can cause damage to\nobjects. To address this limitation, we present a hybrid gripper finger that\ncombines a rigid structural shell with a soft, inflatable silicone pocket. The\ngripper finger can actively modulate its surface friction by controlling the\ninternal air pressure of the silicone pocket. Results from fundamental\nexperiments indicate that increasing the internal pressure results in a\nproportional increase in the effective coefficient of friction. This enables\nthe gripper to stably lift heavy and slippery objects without increasing the\ngripping force and to handle fragile or deformable objects, such as eggs,\nfruits, and paper cups, with minimal damage by increasing friction rather than\napplying excessive force. The experimental results demonstrate that the hybrid\ngripper finger with adaptable friction provides a robust and safer alternative\nto relying solely on high normal forces, thereby enhancing the gripper\nflexibility in handling delicate, fragile, and diverse objects."}
{"id": "2510.27576", "pdf": "https://arxiv.org/pdf/2510.27576", "abs": "https://arxiv.org/abs/2510.27576", "authors": ["Leatile Marata", "Mariona Jaramillo-Civill", "Tales Imbiriba", "Petri Välisuo", "Heidi Kuusniemi", "Elena Simona Lohan", "Pau Closas"], "title": "Trends and Challenges in Next-Generation GNSS Interference Management", "categories": ["eess.SP"], "comment": "Submitted to AESM", "summary": "The global navigation satellite system (GNSS) continues to evolve in order to\nmeet the demands of emerging applications such as autonomous driving and smart\nenvironmental monitoring. However, these advancements are accompanied by a rise\nin interference threats, which can significantly compromise the reliability and\nsafety of GNSS. Such interference problems are typically addressed through\nsignal-processing techniques that rely on physics-based mathematical models.\nUnfortunately, solutions of this nature can often fail to fully capture the\ncomplex forms of interference. To address this, artificial intelligence\n(AI)-inspired solutions are expected to play a key role in future interference\nmanagement solutions, thanks to their ability to exploit data in addition to\nphysics-based models. This magazine paper discusses the main challenges and\ntasks required to secure GNSS and present a research vision on how AI can be\nleveraged towards achieving more robust GNSS-based positioning."}
{"id": "2510.27191", "pdf": "https://arxiv.org/pdf/2510.27191", "abs": "https://arxiv.org/abs/2510.27191", "authors": ["Marcus Hoerger", "Muhammad Sudrajat", "Hanna Kurniawati"], "title": "Vectorized Online POMDP Planning", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 3 figures. Submitted to ICRA 2026", "summary": "Planning under partial observability is an essential capability of autonomous\nrobots. The Partially Observable Markov Decision Process (POMDP) provides a\npowerful framework for planning under partial observability problems, capturing\nthe stochastic effects of actions and the limited information available through\nnoisy observations. POMDP solving could benefit tremendously from massive\nparallelization of today's hardware, but parallelizing POMDP solvers has been\nchallenging. They rely on interleaving numerical optimization over actions with\nthe estimation of their values, which creates dependencies and synchronization\nbottlenecks between parallel processes that can quickly offset the benefits of\nparallelization. In this paper, we propose Vectorized Online POMDP Planner\n(VOPP), a novel parallel online solver that leverages a recent POMDP\nformulation that analytically solves part of the optimization component,\nleaving only the estimation of expectations for numerical computation. VOPP\nrepresents all data structures related to planning as a collection of tensors\nand implements all planning steps as fully vectorized computations over this\nrepresentation. The result is a massively parallel solver with no dependencies\nand synchronization bottlenecks between parallel computations. Experimental\nresults indicate that VOPP is at least 20X more efficient in computing\nnear-optimal solutions compared to an existing state-of-the-art parallel online\nsolver."}
{"id": "2510.26908", "pdf": "https://arxiv.org/pdf/2510.26908", "abs": "https://arxiv.org/abs/2510.26908", "authors": ["Mohammad Eskandari", "Mojtaba Joodaki"], "title": "Electromagnetic Investigation of Crosstalk in Bent Microstrip Lines with Partial and Apertured Shielding: Simulations and Measurements", "categories": ["physics.app-ph", "cs.ET", "eess.SP"], "comment": null, "summary": "This paper presents an electromagnetic investigation of the crosstalk between\ntwo bent microstrip lines (MLs) separated by a perforated planar shield. As an\nextension of our previous study, the effects of various discontinuities in\neither the MLs or the shield along the coupling path are analyzed through\nnumerical simulations and validated by measurements. The underlying\nelectromagnetic mechanisms are also discussed. Furthermore, multimodal wave\ntheory in a rectangular waveguide is applied to predict crosstalk behavior when\nthe shield contains an aperture. This study aims to conceptually elucidate\ncomplex crosstalk phenomena that are difficult to model using circuit theory,\nand successful predictions of crosstalk behavior are presented for different\nproblem cases."}
{"id": "2510.27327", "pdf": "https://arxiv.org/pdf/2510.27327", "abs": "https://arxiv.org/abs/2510.27327", "authors": ["Robert Pommeranz", "Kevin Tebbe", "Ralf Heynicke", "Gerd Scholl"], "title": "A Modular and Scalable System Architecture for Heterogeneous UAV Swarms Using ROS 2 and PX4-Autopilot", "categories": ["cs.RO"], "comment": null, "summary": "In this paper a modular and scalable architecture for heterogeneous\nswarm-based Counter Unmanned Aerial Systems (C-UASs) built on PX4-Autopilot and\nRobot Operating System 2 (ROS 2) framework is presented. The proposed\narchitecture emphasizes seamless integration of hardware components by\nintroducing independent ROS 2 nodes for each component of a Unmanned Aerial\nVehicle (UAV). Communication between swarm participants is abstracted in\nsoftware, allowing the use of various technologies without architectural\nchanges. Key functionalities are supported, e.g. leader following and formation\nflight to maneuver the swarm. The system also allows computer vision algorithms\nto be integrated for the detection and tracking of UAVs. Additionally, a ground\nstation control is integrated for the coordination of swarm operations.\nSwarm-based Unmanned Aerial System (UAS) architecture is verified within a\nGazebo simulation environment but also in real-world demonstrations."}
{"id": "2510.26985", "pdf": "https://arxiv.org/pdf/2510.26985", "abs": "https://arxiv.org/abs/2510.26985", "authors": ["Mostafa Darvishi"], "title": "Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies", "categories": ["cs.AR", "eess.SP"], "comment": "5 figures, 3 tables", "summary": "This paper presents an in-depth analysis of timing closure challenges and\nconstraints in Field Programmable Gate Arrays (FPGAs) and Application Specific\nIntegrated Circuits (ASICs). We examine core timing principles, architectural\ndistinctions, and design methodologies influencing timing behavior in both\ntechnologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA\n(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance\ntrade-offs. Experimental results show ASICs achieve superior timing of 45ps\nsetup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and\n120ps hold times, validating their suitability for high-performance designs."}
{"id": "2510.27333", "pdf": "https://arxiv.org/pdf/2510.27333", "abs": "https://arxiv.org/abs/2510.27333", "authors": ["Hao Cheng", "Yanbo Jiang", "Qingyuan Shi", "Qingwen Meng", "Keyu Chen", "Wenhao Yu", "Jianqiang Wang", "Sifa Zheng"], "title": "Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict", "categories": ["cs.RO"], "comment": null, "summary": "Effective, reliable, and efficient evaluation of autonomous driving safety is\nessential to demonstrate its trustworthiness. Criticality metrics provide an\nobjective means of assessing safety. However, as existing metrics primarily\ntarget longitudinal conflicts, accurately quantifying the risks of lateral\nconflicts - prevalent in urban settings - remains challenging. This paper\nproposes the Modified-Emergency Index (MEI), a metric designed to quantify\nevasive effort in lateral conflicts. Compared to the original Emergency Index\n(EI), MEI refines the estimation of the time available for evasive maneuvers,\nenabling more precise risk quantification. We validate MEI on a public lateral\nconflict dataset based on Argoverse-2, from which we extract over 1,500\nhigh-quality AV conflict cases, including more than 500 critical events. MEI is\nthen compared with the well-established ACT and the widely used PET metrics.\nResults show that MEI consistently outperforms them in accurately quantifying\ncriticality and capturing risk evolution. Overall, these findings highlight MEI\nas a promising metric for evaluating urban conflicts and enhancing the safety\nassessment framework for autonomous driving. The open-source implementation is\navailable at https://github.com/AutoChengh/MEI."}
{"id": "2510.27090", "pdf": "https://arxiv.org/pdf/2510.27090", "abs": "https://arxiv.org/abs/2510.27090", "authors": ["Sina Javadzadeh", "Rahil Soroushmojdehi", "S. Alireza Seyyed Mousavi", "Mehrnaz Asadi", "Sumiko Abe", "Terence D. Sanger"], "title": "Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions", "categories": ["cs.LG", "eess.SP"], "comment": "Submitted to ICLR 2026", "summary": "Aggregating intracranial recordings across subjects is challenging since\nelectrode count, placement, and covered regions vary widely. Spatial\nnormalization methods like MNI coordinates offer a shared anatomical reference,\nbut often fail to capture true functional similarity, particularly when\nlocalization is imprecise; even at matched anatomical coordinates, the targeted\nbrain region and underlying neural dynamics can differ substantially between\nindividuals. We propose a scalable representation-learning framework that (i)\nlearns a subject-agnostic functional identity for each electrode from\nmulti-region local field potentials using a Siamese encoder with contrastive\nobjectives, inducing an embedding geometry that is locality-sensitive to\nregion-specific neural signatures, and (ii) tokenizes these embeddings for a\ntransformer that models inter-regional relationships with a variable number of\nchannels. We evaluate this framework on a 20-subject dataset spanning basal\nganglia-thalamic regions collected during flexible rest/movement recording\nsessions with heterogeneous electrode layouts. The learned functional space\nsupports accurate within-subject discrimination and forms clear,\nregion-consistent clusters; it transfers zero-shot to unseen channels. The\ntransformer, operating on functional tokens without subject-specific heads or\nsupervision, captures cross-region dependencies and enables reconstruction of\nmasked channels, providing a subject-agnostic backbone for downstream decoding.\nTogether, these results indicate a path toward large-scale, cross-subject\naggregation and pretraining for intracranial neural data where strict task\nstructure and uniform sensor placement are unavailable."}
{"id": "2510.27420", "pdf": "https://arxiv.org/pdf/2510.27420", "abs": "https://arxiv.org/abs/2510.27420", "authors": ["Roman Freiberg", "Alexander Qualmann", "Ngo Anh Vien", "Gerhard Neumann"], "title": "Towards a Multi-Embodied Grasping Agent", "categories": ["cs.RO", "I.2.9"], "comment": "9 pages, 3 figures", "summary": "Multi-embodiment grasping focuses on developing approaches that exhibit\ngeneralist behavior across diverse gripper designs. Existing methods often\nlearn the kinematic structure of the robot implicitly and face challenges due\nto the difficulty of sourcing the required large-scale data. In this work, we\npresent a data-efficient, flow-based, equivariant grasp synthesis architecture\nthat can handle different gripper types with variable degrees of freedom and\nsuccessfully exploit the underlying kinematic model, deducing all necessary\ninformation solely from the gripper and scene geometry. Unlike previous\nequivariant grasping methods, we translated all modules from the ground up to\nJAX and provide a model with batching capabilities over scenes, grippers, and\ngrasps, resulting in smoother learning, improved performance and faster\ninference time. Our dataset encompasses grippers ranging from humanoid hands to\nparallel yaw grippers and includes 25,000 scenes and 20 million grasps."}
{"id": "2510.27108", "pdf": "https://arxiv.org/pdf/2510.27108", "abs": "https://arxiv.org/abs/2510.27108", "authors": ["Shuo Zhu", "Siyu Lin"], "title": "Analytical Model of NR-V2X Mode 2 with Re-Evaluation Mechanism", "categories": ["cs.NI", "eess.SP"], "comment": "6 pages, 7 figures, conference", "summary": "Massive message transmissions, unpredictable aperiodic messages, and\nhigh-speed moving vehicles contribute to the complex wireless environment,\nresulting in inefficient resource collisions in Vehicle to Everything (V2X). In\norder to achieve better medium access control (MAC) layer performance, 3GPP\nintroduced several new features in NR-V2X. One of the most important is the\nre-evaluation mechanism. It allows the vehicle to continuously sense resources\nbefore message transmission to avoid resource collisions. So far, only a few\narticles have studied the re-evaluation mechanism of NR-V2X, and they mainly\nfocus on network simulator that do not consider variable traffic, which makes\nanalysis and comparison difficult. In this paper, an analytical model of NR-V2X\nMode 2 is established, and a message generator is constructed by using discrete\ntime Markov chain (DTMC) to simulate the traffic pattern recommended by 3GPP\nadvanced V2X services. Our study shows that the re-evaluation mechanism\nimproves the reliability of NR-V2X transmission, but there are still local\nimprovements needed to reduce latency."}
{"id": "2510.27428", "pdf": "https://arxiv.org/pdf/2510.27428", "abs": "https://arxiv.org/abs/2510.27428", "authors": ["Hehui Zheng", "Bhavya Sukhija", "Chenhao Li", "Klemens Iten", "Andreas Krause", "Robert K. Katzschmann"], "title": "Learning Soft Robotic Dynamics with Active Exploration", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Soft robots offer unmatched adaptability and safety in unstructured\nenvironments, yet their compliant, high-dimensional, and nonlinear dynamics\nmake modeling for control notoriously difficult. Existing data-driven\napproaches often fail to generalize, constrained by narrowly focused task\ndemonstrations or inefficient random exploration. We introduce SoftAE, an\nuncertainty-aware active exploration framework that autonomously learns\ntask-agnostic and generalizable dynamics models of soft robotic systems. SoftAE\nemploys probabilistic ensemble models to estimate epistemic uncertainty and\nactively guides exploration toward underrepresented regions of the state-action\nspace, achieving efficient coverage of diverse behaviors without task-specific\nsupervision. We evaluate SoftAE on three simulated soft robotic platforms -- a\ncontinuum arm, an articulated fish in fluid, and a musculoskeletal leg with\nhybrid actuation -- and on a pneumatically actuated continuum soft arm in the\nreal world. Compared with random exploration and task-specific model-based\nreinforcement learning, SoftAE produces more accurate dynamics models, enables\nsuperior zero-shot control on unseen tasks, and maintains robustness under\nsensing noise, actuation delays, and nonlinear material effects. These results\ndemonstrate that uncertainty-driven active exploration can yield scalable,\nreusable dynamics models across diverse soft robotic morphologies, representing\na step toward more autonomous, adaptable, and data-efficient control in\ncompliant robots."}
{"id": "2510.27143", "pdf": "https://arxiv.org/pdf/2510.27143", "abs": "https://arxiv.org/abs/2510.27143", "authors": ["Takahiro Iwami", "Naohisa Inoue", "Akira Omoto"], "title": "Beamforming in the Reproducing Kernel Domain Based on Spatial Differentiation", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": null, "summary": "This paper proposes a novel beamforming framework in the reproducing kernel\ndomain, derived from a unified interpretation of directional response as\nspatial differentiation of the sound field. By representing directional\nresponse using polynomial differential operators, the proposed method enables\nthe formulation of arbitrary beam patterns including non-axisymmetric. The\nderivation of the reproducing kernel associated with the interior fields is\nmathematically supported by Hobson's theorem, which allows concise analytical\nexpressions. Furthermore, the proposed framework generalizes conventional\nspherical harmonic domain beamformers by reinterpreting them as spatial\ndifferential operators, thereby clarifying their theoretical structure and\nextensibility. Three numerical simulations conducted in two-dimensional space\nconfirm the validity of the method."}
{"id": "2510.27436", "pdf": "https://arxiv.org/pdf/2510.27436", "abs": "https://arxiv.org/abs/2510.27436", "authors": ["Tomoko Yonezawa", "Hirotake Yamazoe", "Atsuo Fujino", "Daigo Suhara", "Takaya Tamamoto", "Yuto Nishiguchi"], "title": "Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot", "categories": ["cs.RO", "cs.HC", "I.2.9; I.3.6"], "comment": "Workshop on Socially Aware and Cooperative Intelligent Systems in HAI\n  2025", "summary": "Human-robot interaction frequently involves physical proximity or contact. In\nhuman-human settings, people flexibly accept, reject, or tolerate such\napproaches depending on the relationship and context. We explore the design of\na robot's rejective internal state and corresponding avoidance behaviors, such\nas withdrawing or pushing away, when a person approaches. We model the\naccumulation and decay of discomfort as a function of interpersonal distance,\nand implement tolerance (endurance) and limit-exceeding avoidance driven by the\nDominance axis of the PAD affect model. The behaviors and their intensities are\nrealized on an arm robot. Results illustrate a coherent pipeline from internal\nstate parameters to graded endurance motions and, once a limit is crossed, to\navoidance actions."}
{"id": "2510.27211", "pdf": "https://arxiv.org/pdf/2510.27211", "abs": "https://arxiv.org/abs/2510.27211", "authors": ["Henry Pritchard", "Rahul Parhi"], "title": "Nonasymptotic Convergence Rates for Plug-and-Play Methods With MMSE Denoisers", "categories": ["math.OC", "eess.SP", "stat.ML"], "comment": null, "summary": "It is known that the minimum-mean-squared-error (MMSE) denoiser under\nGaussian noise can be written as a proximal operator, which suffices for\nasymptotic convergence of plug-and-play (PnP) methods but does not reveal the\nstructure of the induced regularizer or give convergence rates. We show that\nthe MMSE denoiser corresponds to a regularizer that can be written explicitly\nas an upper Moreau envelope of the negative log-marginal density, which in turn\nimplies that the regularizer is 1-weakly convex. Using this property, we derive\n(to the best of our knowledge) the first sublinear convergence guarantee for\nPnP proximal gradient descent with an MMSE denoiser. We validate the theory\nwith a one-dimensional synthetic study that recovers the implicit regularizer.\nWe also validate the theory with imaging experiments (deblurring and computed\ntomography), which exhibit the predicted sublinear behavior."}
{"id": "2510.27545", "pdf": "https://arxiv.org/pdf/2510.27545", "abs": "https://arxiv.org/abs/2510.27545", "authors": ["Travis Davies", "Yiqi Huang", "Alexi Gladstone", "Yunxin Liu", "Xiang Chen", "Heng Ji", "Huxian Liu", "Luhui Hu"], "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 6 figures, 4 tables", "summary": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts."}
{"id": "2510.27272", "pdf": "https://arxiv.org/pdf/2510.27272", "abs": "https://arxiv.org/abs/2510.27272", "authors": ["Vincent K. M. Cheung", "Pei-Cheng Shih", "Masato Hirano", "Masataka Goto", "Shinichi Furuya"], "title": "Inferring trust in recommendation systems from brain, behavioural, and physiological data", "categories": ["cs.HC", "eess.AS", "eess.SP"], "comment": null, "summary": "As people nowadays increasingly rely on artificial intelligence (AI) to\ncurate information and make decisions, assigning the appropriate amount of\ntrust in automated intelligent systems has become ever more important. However,\ncurrent measurements of trust in automation still largely rely on self-reports\nthat are subjective and disruptive to the user. Here, we take music\nrecommendation as a model to investigate the neural and cognitive processes\nunderlying trust in automation. We observed that system accuracy was directly\nrelated to users' trust and modulated the influence of recommendation cues on\nmusic preference. Modelling users' reward encoding process with a reinforcement\nlearning model further revealed that system accuracy, expected reward, and\nprediction error were related to oscillatory neural activity recorded via EEG\nand changes in pupil diameter. Our results provide a neurally grounded account\nof calibrating trust in automation and highlight the promises of a multimodal\napproach towards developing trustable AI systems."}
{"id": "2510.27558", "pdf": "https://arxiv.org/pdf/2510.27558", "abs": "https://arxiv.org/abs/2510.27558", "authors": ["Sushil Samuel Dinesh", "Shinkyu Park"], "title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents a framework that leverages pre-trained foundation models\nfor robotic manipulation without domain-specific training. The framework\nintegrates off-the-shelf models, combining multimodal perception from\nfoundation models with a general-purpose reasoning model capable of robust task\nsequencing. Scene graphs, dynamically maintained within the framework, provide\nspatial awareness and enable consistent reasoning about the environment. The\nframework is evaluated through a series of tabletop robotic manipulation\nexperiments, and the results highlight its potential for building robotic\nmanipulation systems directly on top of off-the-shelf foundation models."}
{"id": "2510.27666", "pdf": "https://arxiv.org/pdf/2510.27666", "abs": "https://arxiv.org/abs/2510.27666", "authors": ["Dong Heon Han", "Xiaohao Xu", "Yuxi Chen", "Yusheng Zhou", "Xinqi Zhang", "Jiaqi Wang", "Daniel Bruder", "Xiaonan Huang"], "title": "Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust Cross-Scale Grasping", "categories": ["cs.RO"], "comment": null, "summary": "Biological systems, such as the octopus, exhibit masterful cross-scale\nmanipulation by adaptively reconfiguring their entire form, a capability that\nremains elusive in robotics. Conventional soft grippers, while compliant, are\nmostly constrained by a fixed global morphology, and prior shape-morphing\nefforts have been largely confined to localized deformations, failing to\nreplicate this biological dexterity. Inspired by this natural exemplar, we\nintroduce the paradigm of collaborative, whole-body proprioceptive morphing,\nrealized in a modular soft gripper architecture. Our design is a distributed\nnetwork of modular self-sensing pneumatic actuators that enables the gripper to\nintelligently reconfigure its entire topology, achieving multiple morphing\nstates that are controllable to form diverse polygonal shapes. By integrating\nrich proprioceptive feedback from embedded sensors, our system can seamlessly\ntransition from a precise pinch to a large envelope grasp. We experimentally\ndemonstrate that this approach expands the grasping envelope and enhances\ngeneralization across diverse object geometries (standard and irregular) and\nscales (up to 10$\\times$), while also unlocking novel manipulation modalities\nsuch as multi-object and internal hook grasping. This work presents a low-cost,\neasy-to-fabricate, and scalable framework that fuses distributed actuation with\nintegrated sensing, offering a new pathway toward achieving biological levels\nof dexterity in robotic manipulation."}
{"id": "2510.26948", "pdf": "https://arxiv.org/pdf/2510.26948", "abs": "https://arxiv.org/abs/2510.26948", "authors": ["Lohitvel Gopikannan", "Shashi Ranjan Kumar", "Abhinav Sinha"], "title": "Cooperative Integrated Estimation-Guidance for Simultaneous Interception of Moving Targets", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "This paper proposes a cooperative integrated estimation-guidance framework\nfor simultaneous interception of a non-maneuvering target using a team of\nunmanned autonomous vehicles, assuming only a subset of vehicles are equipped\nwith dedicated sensors to measure the target's states. Unlike earlier\napproaches that focus solely on either estimation or guidance design, the\nproposed framework unifies both within a cooperative architecture. To\ncircumvent the limitation posed by heterogeneity in target observability,\nsensorless vehicles estimate the target's state by leveraging information\nexchanged with neighboring agents over a directed communication topology\nthrough a prescribed-time observer. The proposed approach employs true\nproportional navigation guidance (TPNG), which uses an exact time-to-go\nformulation and is applicable across a wide spectrum of target motions.\nFurthermore, prescribed-time observer and controller are employed to achieve\nconvergence to true target's state and consensus in time-to-go within set\npredefined times, respectively. Simulations demonstrate the effectiveness of\nthe proposed framework under various engagement scenarios."}
{"id": "2510.27133", "pdf": "https://arxiv.org/pdf/2510.27133", "abs": "https://arxiv.org/abs/2510.27133", "authors": ["Zhicong Sun", "Jacqueline Lo", "Jinxing Hu"], "title": "WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond", "categories": ["cs.CV", "cs.RO"], "comment": "This paper has been accepted by MMM 2026", "summary": "3D Gaussian splatting (3DGS) and its subsequent variants have led to\nremarkable progress in simultaneous localization and mapping (SLAM). While most\nrecent 3DGS-based SLAM works focus on small-scale indoor scenes, developing\n3DGS-based SLAM methods for large-scale forest scenes holds great potential for\nmany real-world applications, especially for wildfire emergency response and\nforest management. However, this line of research is impeded by the absence of\na comprehensive and high-quality dataset, and collecting such a dataset over\nreal-world scenes is costly and technically infeasible. To this end, we have\nbuilt a large-scale, comprehensive, and high-quality synthetic dataset for SLAM\nin wildfire and forest environments. Leveraging the Unreal Engine 5 Electric\nDreams Environment Sample Project, we developed a pipeline to easily collect\naerial and ground views, including ground-truth camera poses and a range of\nadditional data modalities from unmanned aerial vehicle. Our pipeline also\nprovides flexible controls on environmental factors such as light, weather, and\ntypes and conditions of wildfire, supporting the need for various tasks\ncovering forest mapping, wildfire emergency response, and beyond. The resulting\npilot dataset, WildfireX-SLAM, contains 5.5k low-altitude RGB-D aerial images\nfrom a large-scale forest map with a total size of 16 km2. On top of\nWildfireX-SLAM, a thorough benchmark is also conducted, which not only reveals\nthe unique challenges of 3DGS-based SLAM in the forest but also highlights\npotential improvements for future works. The dataset and code will be publicly\navailable. Project page: https://zhicongsun.github.io/wildfirexslam."}
{"id": "2510.27607", "pdf": "https://arxiv.org/pdf/2510.27607", "abs": "https://arxiv.org/abs/2510.27607", "authors": ["John Won", "Kyungmin Lee", "Huiwon Jang", "Dongyoung Kim", "Jinwoo Shin"], "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model", "categories": ["cs.CV", "cs.RO"], "comment": "20 pages, 10 figures", "summary": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while still\nenabling cross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and a decoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supports test-time scaling, where action and vision tokens evolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,\nwhile our test-time scaling approach provides an additional 2-5% boost. On\nreal-world tasks with the Franka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore, pre-training\non action-free videos from BridgeV2 yields significant transfer gains on\nRoboCasa, underscoring DUST's potential for large-scale VLA pretraining."}
