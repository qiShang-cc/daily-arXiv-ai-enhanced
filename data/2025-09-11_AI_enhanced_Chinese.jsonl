{"id": "2509.07990", "pdf": "https://arxiv.org/pdf/2509.07990", "abs": "https://arxiv.org/abs/2509.07990", "authors": ["Charan Gajjala Chenchu", "Kinam Kim", "Gao Lu", "Zia Ud Din"], "title": "Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Human-robot collaboration (HRC) in the construction industry depends on\nprecise and prompt recognition of human motion intentions and actions by robots\nto maximize safety and workflow efficiency. There is a research gap in\ncomparing data modalities, specifically signals and videos, for motion\nintention recognition. To address this, the study leverages deep learning to\nassess two different modalities in recognizing workers' motion intention at the\nearly stage of movement in drywall installation tasks. The Convolutional Neural\nNetwork - Long Short-Term Memory (CNN-LSTM) model utilizing surface\nelectromyography (sEMG) data achieved an accuracy of around 87% with an average\ntime of 0.04 seconds to perform prediction on a sample input. Meanwhile, the\npre-trained Video Swin Transformer combined with transfer learning harnessed\nvideo sequences as input to recognize motion intention and attained an accuracy\nof 94% but with a longer average time of 0.15 seconds for a similar prediction.\nThis study emphasizes the unique strengths and trade-offs of both data formats,\ndirecting their systematic deployments to enhance HRC in real-world\nconstruction projects.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4fe1\u53f7\uff08sEMG\uff09\u548c\u89c6\u9891\u4e24\u79cd\u6570\u636e\u6a21\u6001\u5728\u5e72\u5899\u5b89\u88c5\u4efb\u52a1\u4e2d\u8bc6\u522b\u5de5\u4eba\u8fd0\u52a8\u610f\u56fe\u7684\u6548\u679c\uff0c\u53d1\u73b0\u89c6\u9891\u6a21\u6001\u51c6\u786e\u7387\u66f4\u9ad8\uff0894% vs 87%\uff09\uff0c\u4f46\u4fe1\u53f7\u6a21\u6001\u9884\u6d4b\u901f\u5ea6\u66f4\u5feb\uff080.04\u79d2 vs 0.15\u79d2\uff09\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u534f\u4f5c\uff08HRC\uff09\u5728\u5efa\u7b51\u884c\u4e1a\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e0d\u540c\u6570\u636e\u6a21\u6001\u5728\u8fd0\u52a8\u610f\u56fe\u8bc6\u522b\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528CNN-LSTM\u6a21\u578b\u5904\u7406sEMG\u4fe1\u53f7\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u7684Video Swin Transformer\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u5904\u7406\u89c6\u9891\u5e8f\u5217\u3002", "result": "sEMG\u6a21\u6001\u51c6\u786e\u738787%\uff0c\u9884\u6d4b\u65f6\u95f40.04\u79d2\uff1b\u89c6\u9891\u6a21\u6001\u51c6\u786e\u738794%\uff0c\u9884\u6d4b\u65f6\u95f40.15\u79d2\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e24\u79cd\u6570\u636e\u6a21\u6001\u7684\u4f18\u52a3\u52bf\uff0c\u4e3a\u5b9e\u9645\u5efa\u7b51\u9879\u76ee\u4e2dHRC\u7684\u7cfb\u7edf\u6027\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.08142", "pdf": "https://arxiv.org/pdf/2509.08142", "abs": "https://arxiv.org/abs/2509.08142", "authors": ["Haoran Chang", "Mingzhe Chen", "Huaxia Wang", "Qianqian Zhang"], "title": "Privacy Preserving Semantic Communications Using Vision Language Models: A Segmentation and Generation Approach", "categories": ["eess.SP"], "comment": "6 pages, 6 figures, Accepted at IEEE MILCOM 2025", "summary": "Semantic communication has emerged as a promising paradigm for\nnext-generation wireless systems, improving the communication efficiency by\ntransmitting high-level semantic features. However, reliance on unimodal\nrepresentations can degrade reconstruction under poor channel conditions, and\nprivacy concerns of the semantic information attack also gain increasing\nattention. In this work, a privacy-preserving semantic communication framework\nis proposed to protect sensitive content of the image data. Leveraging a\nvision-language model (VLM), the proposed framework identifies and removes\nprivate content regions from input images prior to transmission. A shared\nprivacy database enables semantic alignment between the transmitter and\nreceiver to ensure consistent identification of sensitive entities. At the\nreceiver, a generative module reconstructs the masked regions using learned\nsemantic priors and conditioned on the received text embedding. Simulation\nresults show that generalizes well to unseen image processing tasks, improves\nreconstruction quality at the authorized receiver by over 10% using text\nembedding, and reduces identity leakage to the eavesdropper by more than 50%.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4fdd\u62a4\u56fe\u50cf\u6570\u636e\u4e2d\u7684\u654f\u611f\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u9690\u79c1\u6570\u636e\u5e93\u548c\u751f\u6210\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u4f9d\u8d56\u5355\u6a21\u6001\u8868\u793a\uff0c\u5728\u4fe1\u9053\u6761\u4ef6\u5dee\u65f6\u91cd\u5efa\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u8bed\u4e49\u4fe1\u606f\u653b\u51fb\u7684\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u5e76\u53bb\u9664\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u9690\u79c1\u5185\u5bb9\u533a\u57df\uff1b\u5171\u4eab\u9690\u79c1\u6570\u636e\u5e93\u786e\u4fdd\u53d1\u9001\u548c\u63a5\u6536\u7aef\u5bf9\u654f\u611f\u5b9e\u4f53\u7684\u4e00\u81f4\u8bc6\u522b\uff1b\u63a5\u6536\u7aef\u901a\u8fc7\u751f\u6210\u6a21\u5757\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u91cd\u5efa\u88ab\u63a9\u853d\u533a\u57df\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u5904\u7406\u4efb\u52a1\uff0c\u5728\u6388\u6743\u63a5\u6536\u7aef\u91cd\u5efa\u8d28\u91cf\u63d0\u5347\u8d85\u8fc710%\uff0c\u7a83\u542c\u8005\u8eab\u4efd\u6cc4\u9732\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u6539\u5584\u4e86\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2509.08272", "pdf": "https://arxiv.org/pdf/2509.08272", "abs": "https://arxiv.org/abs/2509.08272", "authors": ["Xiangying Li", "Jiankuan Li", "Yong Tang"], "title": "RTR: A Transformer-Based Lossless Crossover with Perfect Phase Alignment", "categories": ["eess.SP"], "comment": "ICASSP2025", "summary": "This paper proposes a transformer-based lossless crossover method, termed\nResonant Transformer Router (RTR), which achieves frequency separation while\nensuring perfect phase alignment between low-frequency (LF) and high-frequency\n(HF) channels at the crossover frequency. The core property of RTR is that its\nfrequency responses satisfy a linear complementary relation HLF(f)+HHF(f)=1. so\nthat the original signal can be perfectly reconstructed by linear summation of\nthe two channels. Theoretical derivation and circuit simulations demonstrate\nthat RTR provides superior energy efficiency, phase consistency, and robustness\nagainst component tolerances. Compared with conventional LC crossovers and\ndigital FIR/IIR filters, RTR offers a low-loss, low-latency hardware-assisted\nfiltering solution suitable for high-fidelity audio and communication\nfront-ends.\n  The core theory behind this paper's work, lossless crossover, is based on a\nChinese patent [CN116318117A] developed from the previous research of one of\nthe authors, Jianluan Li. We provide a comprehensive experimental validation of\nthis theory and propose a new extension.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65e0\u635f\u4ea4\u53c9\u65b9\u6cd5RTR\uff0c\u5b9e\u73b0\u4e86\u9891\u7387\u5206\u79bb\u548c\u76f8\u4f4d\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfLC\u4ea4\u53c9\u548c\u6570\u5b57\u6ee4\u6ce2\u5668\u5728\u80fd\u6548\u3001\u76f8\u4f4d\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86Resonant Transformer Router (RTR)\uff0c\u6ee1\u8db3\u9891\u7387\u54cd\u5e94\u7684\u7ebf\u6027\u4e92\u8865\u5173\u7cfbHLF(f)+HHF(f)=1\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u548c\u7535\u8def\u4eff\u771f\u8868\u660e\uff0cRTR\u5177\u6709\u9ad8\u80fd\u6548\u3001\u76f8\u4f4d\u4e00\u81f4\u6027\u548c\u5bf9\u5143\u4ef6\u5bb9\u5dee\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RTR\u4e3a\u9ad8\u4fdd\u771f\u97f3\u9891\u548c\u901a\u4fe1\u524d\u7aef\u63d0\u4f9b\u4e86\u4f4e\u635f\u8017\u3001\u4f4e\u5ef6\u8fdf\u7684\u786c\u4ef6\u8f85\u52a9\u6ee4\u6ce2\u65b9\u6848\u3002"}}
{"id": "2509.08294", "pdf": "https://arxiv.org/pdf/2509.08294", "abs": "https://arxiv.org/abs/2509.08294", "authors": ["Zheao Li", "Jiancheng An", "Chau Yuen"], "title": "Fundamental Trade-off in Wideband Stacked Intelligent Metasurface Assisted OFDMA Systems", "categories": ["eess.SP"], "comment": null, "summary": "Conventional digital beamforming for wideband multiuser orthogonal\nfrequency-division multiplexing (OFDM) demands numerous power-hungry\ncomponents, increasing hardware costs and complexity. By contrast, the stacked\nintelligent metasurfaces (SIM) can perform wave-based precoding at near-light\nspeed, drastically reducing baseband overhead. However, realizing SIM-enhanced\nfully-analog beamforming for wideband multiuser transmissions remains\nchallenging, as the SIM configuration has to handle interference across all\nsubcarriers. To address this, this paper proposes a flexible subcarrier\nallocation strategy to fully reap the SIM-assisted fully-analog beamforming\ncapability in an orthogonal frequency-division multiple access (OFDMA) system,\nwhere each subcarrier selectively serves one or more users to balance\ninterference mitigation and resource utilization of SIM. We propose an\niterative algorithm to jointly optimize the subcarrier assignment matrix and\nSIM transmission coefficients, approximating an interference-free channel for\nthose selected subcarriers. Results show that the proposed system has low\nfitting errors yet allows each user to exploit more subcarriers. Further\ncomparisons highlight a fundamental trade-off: our system achieves near-zero\ninterference and robust data reliability without incurring the hardware burdens\nof digital precoding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u8f7d\u6ce2\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8f7d\u6ce2\u5206\u914d\u77e9\u9635\u548c\u667a\u80fd\u8d85\u8868\u9762\u4f20\u8f93\u7cfb\u6570\uff0c\u5b9e\u73b0\u4f4e\u5e72\u6270\u7684\u5bbd\u9891\u591a\u7528\u6237\u5168\u6a21\u62df\u6ce2\u675f\u6210\u5f62\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u5728\u5bbd\u9891\u591a\u7528\u6237OFDM\u7cfb\u7edf\u4e2d\u529f\u8017\u9ad8\u4e14\u786c\u4ef6\u590d\u6742\uff0c\u800c\u667a\u80fd\u8d85\u8868\u9762\u53ef\u5b9e\u73b0\u9ad8\u901f\u6a21\u62df\u6ce2\u675f\u6210\u5f62\uff0c\u4f46\u9700\u89e3\u51b3\u8de8\u5b50\u8f7d\u6ce2\u7684\u5e72\u6270\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7075\u6d3b\u8f7d\u6ce2\u5206\u914d\u7b56\u7565\u548c\u8fed\u4ee3\u7b97\u6cd5\uff0c\u4f18\u5316\u8f7d\u6ce2\u5206\u914d\u4e0e\u8d85\u8868\u9762\u4f20\u8f93\u7cfb\u6570\uff0c\u5e73\u8861\u5e72\u6270\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "\u7cfb\u7edf\u62df\u5408\u8bef\u5dee\u4f4e\uff0c\u7528\u6237\u53ef\u5229\u7528\u66f4\u591a\u5b50\u8f7d\u6ce2\uff0c\u4e14\u65e0\u9700\u6570\u5b57\u9884\u7f16\u7801\u786c\u4ef6\u5373\u53ef\u5b9e\u73b0\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u5e72\u6270\u3002", "conclusion": "\u8be5\u65b9\u6848\u5728\u5bbd\u9891\u591a\u7528\u6237\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u6a21\u62df\u6ce2\u675f\u6210\u5f62\uff0c\u786c\u4ef6\u8d1f\u62c5\u5c0f\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2509.08002", "pdf": "https://arxiv.org/pdf/2509.08002", "abs": "https://arxiv.org/abs/2509.08002", "authors": ["Maria Mannone", "Mahathi Anand", "Peppino Fazio", "Abdalla Swikir"], "title": "A Novel Theoretical Approach on Micro-Nano Robotic Networks Based on Density Matrices and Swarm Quantum Mechanics", "categories": ["cs.RO", "quant-ph"], "comment": null, "summary": "In a robotic swarm, parameters such as position and proximity to the target\ncan be described in terms of probability amplitudes. This idea led to recent\nstudies on a quantum approach to the definition of the swarm, including a\nblock-matrix representation. Here, we propose an advancement of the idea,\ndefining a swarm as a mixed quantum state, to be described with a density\nmatrix, whose size does not change with the number of robots. We end the\narticle with some directions for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u673a\u5668\u4eba\u7fa4\u4f53\u5b9a\u4e49\u4e3a\u6df7\u5408\u91cf\u5b50\u6001\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u5bc6\u5ea6\u77e9\u9635\u8868\u793a\uff0c\u77e9\u9635\u5927\u5c0f\u4e0d\u968f\u673a\u5668\u4eba\u6570\u91cf\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u901a\u8fc7\u91cf\u5b50\u65b9\u6cd5\u63cf\u8ff0\u7fa4\u4f53\u673a\u5668\u4eba\u4e2d\u7684\u53c2\u6570\uff08\u5982\u4f4d\u7f6e\u548c\u76ee\u6807\u63a5\u8fd1\u5ea6\uff09\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u7fa4\u4f53\u8868\u793a\u5f62\u5f0f\u3002", "method": "\u5c06\u7fa4\u4f53\u5b9a\u4e49\u4e3a\u6df7\u5408\u91cf\u5b50\u6001\uff0c\u5e76\u4f7f\u7528\u5bc6\u5ea6\u77e9\u9635\u8868\u793a\uff0c\u786e\u4fdd\u77e9\u9635\u5927\u5c0f\u6052\u5b9a\uff0c\u4e0d\u968f\u673a\u5668\u4eba\u6570\u91cf\u589e\u52a0\u800c\u53d8\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u7fa4\u4f53\u8868\u793a\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7fa4\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u3002"}}
{"id": "2509.08432", "pdf": "https://arxiv.org/pdf/2509.08432", "abs": "https://arxiv.org/abs/2509.08432", "authors": ["Yingjie Wu", "Junshan Luo", "Weiyu Chen", "Shilian Wang", "Fanggang Wang", "Haiyang Ding"], "title": "Fluid-Antenna-aided AAV Secure Communications in Eavesdropper Uncertain Location", "categories": ["eess.SP"], "comment": null, "summary": "For autonomous aerial vehicle (AAV) secure communications, traditional\ndesigns based on fixed position antenna (FPA) lack sufficient spatial degrees\nof freedom (DoF), which leaves the line-of-sight-dominated AAV links vulnerable\nto eavesdropping. To overcome this problem, this paper proposes a framework\nthat effectively incorporates the fluid antenna (FA) and the artificial noise\n(AN) techniques. Specifically, the minimum secrecy rate (MSR) among multiple\neavesdroppers is maximized by jointly optimizing AAV deployment, signal and AN\nprecoders, and FA positions. In particular, the worst-case MSR is considered by\ntaking the channel uncertainties due to the uncertainty about eavesdropping\nlocations into account. To tackle the highly coupled optimization variables and\nthe channel uncertainties in the formulated problem, an efficient and robust\nalgorithm is proposed. Particularly, the uncertain regions of eavesdroppers,\nwhose shapes can be arbitrary, are disposed by constructing convex hull. In\naddition, two movement modes of FAs are considered, namely, free movement mode\nand zonal movement mode, for which different optimization techniques are\napplied, respectively. Numerical results show that, the proposed FA schemes\nboost security by exploiting additional spatial DoF rather than transmit power,\nwhile AN provides remarkable gains under high transmit power. Furthermore, the\nsynergy between FA and AN results in a secure advantage that exceeds the sum of\ntheir individual contributions, achieving a balance between security and\nreliability under limited resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6d41\u4f53\u5929\u7ebf\u548c\u4eba\u5de5\u566a\u58f0\u6280\u672f\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u81ea\u4e3b\u98de\u884c\u5668\u901a\u4fe1\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u90e8\u7f72\u548c\u5929\u7ebf\u4f4d\u7f6e\u6700\u5927\u5316\u4fdd\u5bc6\u7387\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u8bbe\u8ba1\u56e0\u7f3a\u4e4f\u7a7a\u95f4\u81ea\u7531\u5ea6\uff0c\u4f7f\u81ea\u4e3b\u98de\u884c\u5668\u7684\u901a\u4fe1\u6613\u53d7\u7a83\u542c\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316AAV\u90e8\u7f72\u3001\u4fe1\u53f7\u9884\u7f16\u7801\u3001\u4eba\u5de5\u566a\u58f0\u9884\u7f16\u7801\u53ca\u6d41\u4f53\u5929\u7ebf\u4f4d\u7f6e\u7684\u6846\u67b6\uff0c\u5e76\u8003\u8651\u4e86\u7a83\u542c\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6d41\u4f53\u5929\u7ebf\u65b9\u6848\u901a\u8fc7\u5229\u7528\u7a7a\u95f4\u81ea\u7531\u5ea6\u63d0\u5347\u4e86\u5b89\u5168\u6027\uff0c\u800c\u4eba\u5de5\u566a\u58f0\u5728\u9ad8\u529f\u7387\u4e0b\u8868\u73b0\u663e\u8457\uff0c\u4e8c\u8005\u534f\u540c\u4f5c\u7528\u66f4\u4f73\u3002", "conclusion": "\u6d41\u4f53\u5929\u7ebf\u4e0e\u4eba\u5de5\u566a\u58f0\u7684\u7ed3\u5408\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u7684\u5e73\u8861\uff0c\u8868\u73b0\u51fa\u8d85\u8d8a\u5355\u72ec\u4f7f\u7528\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.08017", "pdf": "https://arxiv.org/pdf/2509.08017", "abs": "https://arxiv.org/abs/2509.08017", "authors": ["Niharika Karnik", "Yash Bhangale", "Mohammad G. Abdo", "Andrei A. Klishin", "Joshua J. Cogliati", "Bingni W. Brunton", "J. Nathan Kutz", "Steven L. Brunton", "Krithika Manohar"], "title": "PySensors 2.0: A Python Package for Sparse Sensor Placement", "categories": ["cs.RO"], "comment": null, "summary": "PySensors is a Python package for selecting and placing a sparse set of\nsensors for reconstruction and classification tasks. In this major update to\n\\texttt{PySensors}, we introduce spatially constrained sensor placement\ncapabilities, allowing users to enforce constraints such as maximum or exact\nsensor counts in specific regions, incorporate predetermined sensor locations,\nand maintain minimum distances between sensors. We extend functionality to\nsupport custom basis inputs, enabling integration of any data-driven or\nspectral basis. We also propose a thermodynamic approach that goes beyond a\nsingle ``optimal'' sensor configuration and maps the complete landscape of\nsensor interactions induced by the training data. This comprehensive view\nfacilitates integration with external selection criteria and enables assessment\nof sensor replacement impacts. The new optimization technique also accounts for\nover- and under-sampling of sensors, utilizing a regularized least squares\napproach for robust reconstruction. Additionally, we incorporate noise-induced\nuncertainty quantification of the estimation error and provide visual\nuncertainty heat maps to guide deployment decisions. To highlight these\nadditions, we provide a brief description of the mathematical algorithms and\ntheory underlying these new capabilities. We demonstrate the usage of new\nfeatures with illustrative code examples and include practical advice for\nimplementation across various application domains. Finally, we outline a\nroadmap of potential extensions to further enhance the package's functionality\nand applicability to emerging sensing challenges.", "AI": {"tldr": "PySensors\u662f\u4e00\u4e2aPython\u5305\uff0c\u7528\u4e8e\u5728\u7a00\u758f\u4f20\u611f\u5668\u5e03\u7f6e\u4efb\u52a1\u4e2d\u9009\u62e9\u548c\u653e\u7f6e\u4f20\u611f\u5668\u3002\u672c\u6b21\u66f4\u65b0\u5f15\u5165\u4e86\u7a7a\u95f4\u7ea6\u675f\u529f\u80fd\u3001\u81ea\u5b9a\u4e49\u57fa\u7840\u8f93\u5165\u652f\u6301\u3001\u70ed\u529b\u5b66\u65b9\u6cd5\u4ee5\u53ca\u566a\u58f0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7b49\u65b0\u7279\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u4f20\u611f\u5668\u5e03\u7f6e\u65b9\u6848\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u7684\u9700\u6c42\uff0cPySensors\u8fdb\u884c\u4e86\u529f\u80fd\u6269\u5c55\u548c\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u7ea6\u675f\u3001\u81ea\u5b9a\u4e49\u57fa\u7840\u8f93\u5165\u3001\u70ed\u529b\u5b66\u65b9\u6cd5\u548c\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7b49\u6280\u672f\uff0c\u4f18\u5316\u4f20\u611f\u5668\u7684\u9009\u62e9\u548c\u5e03\u7f6e\u7b56\u7565\u3002", "result": "\u65b0\u529f\u80fd\u652f\u6301\u66f4\u591a\u6837\u5316\u7684\u4f20\u611f\u5668\u5e03\u7f6e\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PySensors\u7684\u66f4\u65b0\u663e\u8457\u589e\u5f3a\u4e86\u5176\u5728\u591a\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u8fdb\u4e00\u6b65\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.08434", "pdf": "https://arxiv.org/pdf/2509.08434", "abs": "https://arxiv.org/abs/2509.08434", "authors": ["Ahmet B. Kilic", "Ozgur B. Akan"], "title": "Information and Communication Theoretical Foundations of the Internet of Plants, Principles, Challenges, and Future Directions", "categories": ["eess.SP"], "comment": null, "summary": "Plants exchange information through multiple modalities, including chemical,\nelectrical, mycorrhizal, and acoustic signaling, which collectively support\nsurvival, defense, and adaptation. While these processes are well documented in\nbiology, their systematic analysis from an Information and Communication\nTechnology (ICT) perspective remains limited. To address this gap, this article\nis presented as a tutorial with survey elements. It provides the necessary\nbiological background, reformulates inter-plant signaling within ICT\nframeworks, and surveys empirical studies to guide future research and\napplications. First, the paper introduces the fundamental biological processes\nto establish a foundation for readers in communications and networking.\nBuilding on this foundation, existing models of emission, propagation, and\nreception are synthesized for each modality and reformulated in terms of\ntransmitter, channel, and receiver blocks. To complement theory, empirical\nstudies and state-of-the-art sensing approaches are critically examined.\nLooking forward, the paper identifies open challenges and outlines future\nresearch directions, with particular emphasis on the emerging vision of the\nInternet of Plants (IoP). This paradigm frames plants as interconnected nodes\nwithin ecological and technological networks, offering new opportunities for\napplications in precision agriculture, ecosystem monitoring, climate\nresilience, and bio-inspired communication systems. By integrating biological\ninsights with ICT frameworks and projecting toward the IoP, this article\nprovides a comprehensive tutorial on plant communication for the communications\nresearch community and establishes a foundation for interdisciplinary advances.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u690d\u7269\u901a\u8fc7\u5316\u5b66\u3001\u7535\u3001\u83cc\u6839\u548c\u58f0\u97f3\u4fe1\u53f7\u7b49\u591a\u6a21\u6001\u901a\u4fe1\u65b9\u5f0f\uff0c\u4ece\u4fe1\u606f\u4e0e\u901a\u4fe1\u6280\u672f\uff08ICT\uff09\u89d2\u5ea6\u91cd\u65b0\u5b9a\u4e49\u690d\u7269\u95f4\u901a\u4fe1\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u76ee\u524d\u5bf9\u690d\u7269\u901a\u4fe1\u7684\u7cfb\u7edf\u6027ICT\u5206\u6790\u6709\u9650\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u751f\u7269\u5b66\u80cc\u666f\u548cICT\u6846\u67b6\uff0c\u7efc\u8ff0\u73b0\u6709\u7814\u7a76\u5e76\u63d0\u51fa\u53d1\u5c04\u3001\u4fe1\u9053\u548c\u63a5\u6536\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u690d\u7269\u4e92\u8054\u7f51\uff08IoP\uff09\u6982\u5ff5\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u548c\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u7b49\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "conclusion": "\u8bba\u6587\u4e3a\u901a\u4fe1\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u690d\u7269\u901a\u4fe1\u7684\u5168\u9762\u6559\u7a0b\uff0c\u5e76\u4fc3\u8fdb\u4e86\u8de8\u5b66\u79d1\u8fdb\u5c55\u3002"}}
{"id": "2509.08069", "pdf": "https://arxiv.org/pdf/2509.08069", "abs": "https://arxiv.org/abs/2509.08069", "authors": ["Shiping Ma", "Haoming Zhang", "Marc Toussaint"], "title": "SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton", "categories": ["cs.RO"], "comment": null, "summary": "This letter introduces SVN-ICP, a novel Iterative Closest Point (ICP)\nalgorithm with uncertainty estimation that leverages Stein Variational Newton\n(SVN) on manifold. Designed specifically for fusing LiDAR odometry in\nmultisensor systems, the proposed method ensures accurate pose estimation and\nconsistent noise parameter inference, even in LiDAR-degraded environments. By\napproximating the posterior distribution using particles within the Stein\nVariational Inference framework, SVN-ICP eliminates the need for explicit noise\nmodeling or manual parameter tuning. To evaluate its effectiveness, we\nintegrate SVN-ICP into a simple error-state Kalman filter alongside an IMU and\ntest it across multiple datasets spanning diverse environments and robot types.\nExtensive experimental results demonstrate that our approach outperforms\nbest-in-class methods on challenging scenarios while providing reliable\nuncertainty estimates.", "AI": {"tldr": "SVN-ICP\u662f\u4e00\u79cd\u65b0\u9896\u7684ICP\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86Stein\u53d8\u5206\u725b\u987f\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u7684LiDAR\u91cc\u7a0b\u8ba1\u878d\u5408\uff0c\u80fd\u591f\u5728\u6076\u52a3\u73af\u5883\u4e0b\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u4f4d\u59ff\u4f30\u8ba1\u548c\u566a\u58f0\u53c2\u6570\u63a8\u65ad\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\uff0c\u5c24\u5176\u662f\u5728LiDAR\u6027\u80fd\u4e0b\u964d\u7684\u73af\u5883\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u4f4d\u59ff\u4f30\u8ba1\u548c\u566a\u58f0\u53c2\u6570\u63a8\u65ad\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528Stein\u53d8\u5206\u63a8\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u7c92\u5b50\u903c\u8fd1\u540e\u9a8c\u5206\u5e03\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u566a\u58f0\u5efa\u6a21\u6216\u624b\u52a8\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSVN-ICP\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u6700\u4f73\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u53ef\u9760\u7684\u566a\u58f0\u4f30\u8ba1\u3002", "conclusion": "SVN-ICP\u5728\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u73af\u5883\u548c\u673a\u5668\u4eba\u7c7b\u578b\u3002"}}
{"id": "2509.08504", "pdf": "https://arxiv.org/pdf/2509.08504", "abs": "https://arxiv.org/abs/2509.08504", "authors": ["Didem Aydogan", "Mohaned Chraiti", "Korkut Kaan Tokg\u00f6z"], "title": "On the Performance of ISAC over the D-Band in a Phase-Noise Aware OFDM Systems", "categories": ["eess.SP"], "comment": null, "summary": "Phase noise (PN) is a critical impairment at D-band frequencies (110 to 170\nGHz), which are widely investigated as promising candidates for beyond 5G/6G\nISAC systems. This paper evaluates OFDM based ISAC sensing performance under\nrealistic oscillator impairments using a hardware-tuned 3GPP PN model at 130\nGHz and FFT based radar processing. With a numerology of 480 kHz, results show\nthat PN introduces range RMSE floors of 0.04 to 0.05 m and velocity RMSE floors\nof 0.12 to 0.18 m/s. Doppler sidelobe metrics also saturate, with PSLR around\nminus 6 dB and ISLR around minus 4 dB. These findings confirm that range\naccuracy remains bandwidth limited, while velocity estimation and sidelobe\nsuppression are strongly PN-sensitive. The study highlights the importance of\nPN-aware waveform and numerology design for sub-THz ISAC and provides insights\nfor future multi-band transceivers. Communication metrics and PN mitigation\nstrategies such as PTRS and CPE tracking are left for future work.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86D\u6ce2\u6bb5\uff08110\u81f3170 GHz\uff09\u9891\u6bb5\u4e2d\u7684\u76f8\u4f4d\u566a\u58f0\uff08PN\uff09\u5bf9OFDM ISAC\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u786c\u4ef6\u8c03\u8c10\u76843GPP PN\u6a21\u578b\u548cFFT\u96f7\u8fbe\u5904\u7406\u5728130 GHz\u4e0b\u8fdb\u884c\u5206\u6790\u3002\u7ed3\u679c\u8868\u660e\uff0cPN\u4f1a\u5bfc\u81f4\u6d4b\u8ddd\u548c\u6d4b\u901f\u7684\u6027\u80fd\u9650\u5236\uff0c\u5e76\u5f71\u54cd\u591a\u666e\u52d2\u65c1\u74e3\u3002\u5f3a\u8c03\u4e86PN\u611f\u77e5\u7684\u6ce2\u5f62\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u53ef\u63a2\u7d22PN\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "D\u6ce2\u6bb5\u9891\u6bb5\u662f\u672a\u67655G/6G ISAC\u7cfb\u7edf\u7684\u6f5c\u5728\u5019\u9009\u9891\u6bb5\uff0c\u4f46\u76f8\u4f4d\u566a\u58f0\uff08PN\uff09\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30PN\u5bf9OFDM ISAC\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u591a\u9891\u6bb5\u6536\u53d1\u5668\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4f7f\u7528\u786c\u4ef6\u8c03\u8c10\u76843GPP PN\u6a21\u578b\u5728130 GHz\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u5408FFT\u96f7\u8fbe\u5904\u7406\uff0c\u5206\u6790\u4e86480 kHz\u8f7d\u6ce2\u9891\u7387\u4e0b\u7684ISAC\u611f\u77e5\u6027\u80fd\u3002", "result": "PN\u5bfc\u81f4\u6d4b\u8dddRMSE\u8d8b\u8fd1\u4e8e0.04-0.05\u7c73\uff0c\u6d4b\u901fRMSE\u8d8b\u8fd1\u4e8e0.12-0.18\u7c73/\u79d2\u3002\u591a\u666e\u52d2\u65c1\u74e3\u6307\u6807\u7684PSLR\u548cISLR\u5206\u522b\u8d8b\u8fd1\u4e8e-6 dB\u548c-4 dB\u3002", "conclusion": "\u6d4b\u8ddd\u7cbe\u5ea6\u4ecd\u53d7\u5e26\u5bbd\u9650\u5236\uff0c\u800c\u6d4b\u901f\u548c\u591a\u666e\u52d2\u65c1\u74e3\u5f3a\u70c8\u4f9d\u8d56\u4e8ePN\u3002\u7814\u7a76\u5f3a\u8c03\u4e86PN\u611f\u77e5\u7684\u6ce2\u5f62\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765PN\u7f13\u89e3\u7b56\u7565\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.08095", "pdf": "https://arxiv.org/pdf/2509.08095", "abs": "https://arxiv.org/abs/2509.08095", "authors": ["Lamiaa H. Zain", "Raafat E. Shalaby"], "title": "Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor Fusion", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Obstacle avoidance is a critical component of the navigation stack required\nfor mobile robots to operate effectively in complex and unknown environments.\nIn this research, three end-to-end Convolutional Neural Networks (CNNs) were\ntrained and evaluated offline and deployed on a differential-drive mobile robot\nfor real-time obstacle avoidance to generate low-level steering commands from\nsynchronized color and depth images acquired by an Intel RealSense D415 RGB-D\ncamera in diverse environments. Offline evaluation showed that the NetConEmb\nmodel achieved the best performance with a notably low MedAE of $0.58 \\times\n10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture adopted in this\nstudy, which reduces the number of trainable parameters by approximately 25\\%\nand converges faster, produced comparable results with an RMSE of $21.68 \\times\n10^{-3}$ rad/s, close to the $21.42 \\times 10^{-3}$ rad/s obtained by\nNetConEmb. Real-time navigation further confirmed NetConEmb's robustness,\nachieving a 100\\% success rate in both known and unknown environments, while\nNetEmb and NetGated succeeded only in navigating the known environment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u7aef\u5230\u7aef\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u907f\u969c\u3002\u5176\u4e2dNetConEmb\u8868\u73b0\u6700\u4f18\uff0c\u79bb\u7ebf\u8bc4\u4f30\u8bef\u5dee\u4f4e\uff0c\u4e14\u5728\u5b9e\u65f6\u5bfc\u822a\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u907f\u969c\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e86\u4e09\u79cdCNN\u6a21\u578b\uff08NetConEmb\u3001NetEmb\u3001NetGated\uff09\uff0c\u901a\u8fc7RGB-D\u6444\u50cf\u5934\u83b7\u53d6\u540c\u6b65\u56fe\u50cf\uff0c\u751f\u6210\u4f4e\u7ea7\u8f6c\u5411\u547d\u4ee4\u3002", "result": "NetConEmb\u79bb\u7ebf\u8bc4\u4f30\u8bef\u5dee\u6700\u4f4e\uff08MedAE\u4e3a0.58\u00d710\u22123 rad/s\uff09\uff0c\u5b9e\u65f6\u5bfc\u822a\u6210\u529f\u7387\u8fbe100%\uff1bNetEmb\u8868\u73b0\u63a5\u8fd1\u3002", "conclusion": "NetConEmb\u5728\u907f\u969c\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.08614", "pdf": "https://arxiv.org/pdf/2509.08614", "abs": "https://arxiv.org/abs/2509.08614", "authors": ["Yuxuan Duan", "Chenyang Yang"], "title": "Modular PE-Structured Learning for Cross-Task Wireless Communications", "categories": ["eess.SP"], "comment": "14 pages,7 figures", "summary": "Recent trends in learning wireless policies attempt to develop deep neural\nnetworks (DNNs) for handling multiple tasks with a single model. Existing\napproaches often rely on large models, which are hard to pre-train and\nfine-tune at the wireless edge. In this work, we challenge this paradigm by\nleveraging the structured knowledge of wireless problems -- specifically,\npermutation equivariant (PE) properties. We design three types of PE-aware\nmodules, two of which are Transformer-style sub-layers. These modules can serve\nas building blocks to assemble compact DNNs applicable to the wireless policies\nwith various PE properties. To guide the design, we analyze the hypothesis\nspace associated with each PE property, and show that the PE-structured module\nassembly can boost the learning efficiency. Inspired by the reusability of the\nmodules, we propose PE-MoFormer, a compositional DNN capable of learning a wide\nrange of wireless policies -- including but not limited to precoding,\ncoordinated beamforming, power allocation, and channel estimation -- with\nstrong generalizability, low sample and space complexity. Simulations\ndemonstrate that the proposed modular PE-based framework outperforms relevant\nlarge model in both learning efficiency and inference time, offering a new\ndirection for structured cross-task learning for wireless communications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u6362\u7b49\u53d8\uff08PE\uff09\u7279\u6027\u7684\u6a21\u5757\u5316\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6PE-MoFormer\uff0c\u7528\u4e8e\u9ad8\u6548\u5b66\u4e60\u591a\u79cd\u65e0\u7ebf\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u6a21\u578b\uff0c\u96be\u4ee5\u5728\u65e0\u7ebf\u8fb9\u7f18\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u800c\u901a\u8fc7\u5229\u7528\u65e0\u7ebf\u95ee\u9898\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\uff08PE\u7279\u6027\uff09\uff0c\u53ef\u4ee5\u8bbe\u8ba1\u66f4\u7d27\u51d1\u4e14\u9ad8\u6548\u7684DNN\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u7c7b\u578b\u7684PE\u611f\u77e5\u6a21\u5757\uff08\u5305\u62ec\u4e24\u79cdTransformer\u98ce\u683c\u7684\u5b50\u5c42\uff09\uff0c\u5206\u6790\u6bcf\u79cdPE\u7279\u6027\u7684\u5047\u8bbe\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u6a21\u5757\u7ec4\u88c5\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002\u63d0\u51fa\u7684PE-MoFormer\u6846\u67b6\u652f\u6301\u591a\u79cd\u65e0\u7ebf\u7b56\u7565\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u5757\u5316PE\u6846\u67b6\u5728\u5b66\u4e60\u548c\u63a8\u7406\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5927\u578b\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u9884\u7f16\u7801\u3001\u6ce2\u675f\u6210\u5f62\u3001\u529f\u7387\u5206\u914d\u548c\u4fe1\u9053\u4f30\u8ba1\u7b49\u4efb\u52a1\u3002", "conclusion": "\u57fa\u4e8ePE\u7684\u7ed3\u6784\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3a\u65e0\u7ebf\u901a\u4fe1\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.08117", "pdf": "https://arxiv.org/pdf/2509.08117", "abs": "https://arxiv.org/abs/2509.08117", "authors": ["Ruijie Du", "Ruoyu Lin", "Yanning Shen", "Magnus Egerstedt"], "title": "Online Learning and Coverage of Unknown Fields Using Random-Feature Gaussian Processes", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper proposes a framework for multi-robot systems to perform\nsimultaneous learning and coverage of the domain of interest characterized by\nan unknown and potentially time-varying density function. To overcome the\nlimitations of Gaussian Process (GP) regression, we employ Random Feature GP\n(RFGP) and its online variant (O-RFGP) that enables online and incremental\ninference. By integrating these with Voronoi-based coverage control and Upper\nConfidence Bound (UCB) sampling strategy, a team of robots can adaptively focus\non important regions while refining the learned spatial field for efficient\ncoverage. Under mild assumptions, we provide theoretical guarantees and\nevaluate the framework through simulations in time-invariant scenarios.\nFurthermore, its effectiveness in time-varying settings is demonstrated through\nadditional simulations and a physical experiment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u5b66\u4e60\u548c\u8986\u76d6\u672a\u77e5\u4e14\u53ef\u80fd\u65f6\u53d8\u7684\u5bc6\u5ea6\u51fd\u6570\u57df\uff0c\u7ed3\u5408\u4e86\u968f\u673a\u7279\u5f81\u9ad8\u65af\u8fc7\u7a0b\uff08RFGP\uff09\u548cVoronoi\u8986\u76d6\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5728\u5904\u7406\u672a\u77e5\u548c\u65f6\u53d8\u5bc6\u5ea6\u51fd\u6570\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u968f\u673a\u7279\u5f81\u9ad8\u65af\u8fc7\u7a0b\uff08RFGP\uff09\u53ca\u5176\u5728\u7ebf\u7248\u672c\uff08O-RFGP\uff09\uff0c\u7ed3\u5408Voronoi\u8986\u76d6\u63a7\u5236\u548cUCB\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u65f6\u95f4\u4e0d\u53d8\u548c\u65f6\u95f4\u53d8\u5316\u573a\u666f\u4e0b\u5747\u6709\u6548\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u80fd\u81ea\u9002\u5e94\u805a\u7126\u91cd\u8981\u533a\u57df\u5e76\u9ad8\u6548\u8986\u76d6\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u53ef\u884c\u6027\u3002"}}
{"id": "2509.08642", "pdf": "https://arxiv.org/pdf/2509.08642", "abs": "https://arxiv.org/abs/2509.08642", "authors": ["Hang Ruan", "Homa Nikbakht", "Ruizhi Zhang", "Honglei Chen", "Yonina C. Eldar"], "title": "RIS-Assisted Near-Field ISAC for Multi-Target Indication in NLoS Scenarios", "categories": ["eess.SP"], "comment": "5 pages, 3 figures; To be submitted to ICASSP 2026", "summary": "Enabling multi-target sensing in near-field integrated sensing and\ncommunication (ISAC) systems is a key challenge, particularly when\nline-of-sight paths are blocked. This paper proposes a beamforming framework\nthat leverages a reconfigurable intelligent surface (RIS) to achieve\nmulti-target indication. Our contribution is the extension of classic\nbeampattern gain and inter-target cross-correlation metrics to the near-field,\nleveraging both angle and distance information to discriminate between multiple\nusers and targets. We formulate a problem to maximize the worst-case sensing\nperformance by jointly designing the beamforming at the base station and the\nphase shifts at the RIS, while guaranteeing communication rates. The non-convex\nproblem is solved via an efficient alternating optimization (AO) algorithm that\nutilizes semidefinite relaxation (SDR). Simulations demonstrate that our\nRIS-assisted framework enables high-resolution sensing of co-angle targets in\nblocked scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u573aISAC\u7cfb\u7edf\u4e2d\u57fa\u4e8eRIS\u7684\u6ce2\u675f\u6210\u5f62\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u591a\u76ee\u6807\u611f\u77e5\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u89c6\u7ebf\u8def\u5f84\u53d7\u963b\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u8fd1\u573aISAC\u7cfb\u7edf\u4e2d\uff0c\u591a\u76ee\u6807\u611f\u77e5\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u89c6\u7ebf\u8def\u5f84\u53d7\u963b\u65f6\u96be\u4ee5\u533a\u5206\u591a\u4e2a\u76ee\u6807\u6216\u7528\u6237\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u7ecf\u5178\u6ce2\u675f\u589e\u76ca\u548c\u8de8\u76ee\u6807\u76f8\u5173\u6027\u6307\u6807\u81f3\u8fd1\u573a\uff0c\u5e76\u5229\u7528\u89d2\u5ea6\u548c\u8ddd\u79bb\u4fe1\u606f\uff0c\u63d0\u51fa\u4e86\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548cRIS\u76f8\u4f4d\u504f\u79fb\u7684\u6846\u67b6\uff0c\u4f7f\u7528AO\u7b97\u6cd5\u548cSDR\u89e3\u51b3\u975e\u51f8\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5RIS\u8f85\u52a9\u6846\u67b6\u80fd\u591f\u5728\u89c6\u7ebf\u53d7\u963b\u65f6\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u591a\u76ee\u6807\u611f\u77e5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8fd1\u573aISAC\u7cfb\u7edf\u4e2d\u7684\u591a\u76ee\u6807\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u89c6\u7ebf\u53d7\u963b\u573a\u666f\u4e0b\u3002"}}
{"id": "2509.08126", "pdf": "https://arxiv.org/pdf/2509.08126", "abs": "https://arxiv.org/abs/2509.08126", "authors": ["Houjian Yu", "Zheming Zhou", "Min Sun", "Omid Ghasemalizadeh", "Yuyin Sun", "Cheng-Hao Kuo", "Arnie Sen", "Changhyun Choi"], "title": "Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning", "categories": ["cs.RO"], "comment": "Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid\n  Robots", "summary": "Enabling robots to grasp objects specified through natural language is\nessential for effective human-robot interaction, yet it remains a significant\nchallenge. Existing approaches often struggle with open-form language\nexpressions and typically assume unambiguous target objects without duplicates.\nMoreover, they frequently rely on costly, dense pixel-wise annotations for both\nobject grounding and grasp configuration. We present Attribute-based Object\nGrounding and Robotic Grasping (OGRG), a novel framework that interprets\nopen-form language expressions and performs spatial reasoning to ground target\nobjects and predict planar grasp poses, even in scenes containing duplicated\nobject instances. We investigate OGRG in two settings: (1) Referring Grasp\nSynthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp\nAffordance (RGA) using weakly supervised learning with only single-pixel grasp\nannotations. Key contributions include a bi-directional vision-language fusion\nmodule and the integration of depth information to enhance geometric reasoning,\nimproving both grounding and grasping performance. Experiment results show that\nOGRG outperforms strong baselines in tabletop scenes with diverse spatial\nlanguage instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX\n2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential\ngrasping, while delivering superior grounding and grasp prediction accuracy\ncompared to all the baselines considered. Under the weakly supervised RGA\nsetting, OGRG also surpasses baseline grasp-success rates in both simulation\nand real-robot trials, underscoring the effectiveness of its spatial reasoning\ndesign. Project page: https://z.umn.edu/ogrg", "AI": {"tldr": "OGRG\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u5b9e\u73b0\u673a\u5668\u4eba\u6293\u53d6\uff0c\u652f\u6301\u5f00\u653e\u8bed\u8a00\u8868\u8fbe\u548c\u91cd\u590d\u5bf9\u8c61\u573a\u666f\uff0c\u63d0\u5347\u4e86\u6293\u53d6\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5f00\u653e\u8bed\u8a00\u8868\u8fbe\u548c\u91cd\u590d\u5bf9\u8c61\u573a\u666f\uff0c\u4e14\u4f9d\u8d56\u9ad8\u6210\u672c\u5bc6\u96c6\u6807\u6ce8\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOGRG\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u5411\u89c6\u89c9\u8bed\u8a00\u878d\u5408\u6a21\u5757\u548c\u6df1\u5ea6\u4fe1\u606f\uff0c\u652f\u6301RGS\u548cRGA\u4e24\u79cd\u76d1\u7763\u5b66\u4e60\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u663e\u793aOGRG\u5728\u6293\u53d6\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0cRGS\u8fbe17.59 FPS\uff0cRGA\u5728\u4eff\u771f\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "OGRG\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u6293\u53d6\u6027\u80fd\uff0c\u652f\u6301\u9ad8\u6548\u95ed\u73af\u6216\u591a\u5bf9\u8c61\u987a\u5e8f\u64cd\u4f5c\u3002"}}
{"id": "2509.08171", "pdf": "https://arxiv.org/pdf/2509.08171", "abs": "https://arxiv.org/abs/2509.08171", "authors": ["Amirhossein Taherpour", "Abbas Taherpour", "Tamer Khattab"], "title": "RAPID Quantum Detection and Demodulation of Covert Communications: Breaking the Noise Limit with Solid-State Spin Sensors", "categories": ["quant-ph", "cs.LG", "eess.SP"], "comment": null, "summary": "We introduce a comprehensive framework for the detection and demodulation of\ncovert electromagnetic signals using solid-state spin sensors. Our approach,\nnamed RAPID, is a two-stage hybrid strategy that leverages nitrogen-vacancy\n(NV) centers to operate below the classical noise floor employing a robust\nadaptive policy via imitation and distillation. We first formulate the joint\ndetection and estimation task as a unified stochastic optimal control problem,\noptimizing a composite Bayesian risk objective under realistic physical\nconstraints. The RAPID algorithm solves this by first computing a robust,\nnon-adaptive baseline protocol grounded in the quantum Fisher information\nmatrix (QFIM), and then using this baseline to warm-start an online, adaptive\npolicy learned via deep reinforcement learning (Soft Actor-Critic). This method\ndynamically optimizes control pulses, interrogation times, and measurement\nbases to maximize information gain while actively suppressing non-Markovian\nnoise and decoherence. Numerical simulations demonstrate that the protocol\nachieves a significant sensitivity gain over static methods, maintains high\nestimation precision in correlated noise environments, and, when applied to\nsensor arrays, enables coherent quantum beamforming that achieves\nHeisenberg-like scaling in precision. This work establishes a theoretically\nrigorous and practically viable pathway for deploying quantum sensors in\nsecurity-critical applications such as electronic warfare and covert\nsurveillance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fa\u6001\u81ea\u65cb\u4f20\u611f\u5668\u7684\u9690\u853d\u7535\u78c1\u4fe1\u53f7\u68c0\u6d4b\u4e0e\u89e3\u8c03\u6846\u67b6RAPID\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6df7\u5408\u7b56\u7565\u7ed3\u5408\u91cf\u5b50\u8d39\u5e0c\u5c14\u4fe1\u606f\u77e9\u9635\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u548c\u6297\u566a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u6218\u548c\u9690\u853d\u76d1\u89c6\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u91cf\u5b50\u4f20\u611f\u5668\u7684\u5b9e\u9645\u90e8\u7f72\u95ee\u9898\uff0c\u7a81\u7834\u7ecf\u5178\u566a\u58f0\u9650\u5236\u3002", "method": "\u63d0\u51faRAPID\u6846\u67b6\uff0c\u5148\u57fa\u4e8e\u91cf\u5b50\u8d39\u5e0c\u5c14\u4fe1\u606f\u77e9\u9635\u8ba1\u7b97\u975e\u81ea\u9002\u5e94\u57fa\u7ebf\u534f\u8bae\uff0c\u518d\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08Soft Actor-Critic\uff09\u5728\u7ebf\u5b66\u4e60\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u52a8\u6001\u4f18\u5316\u63a7\u5236\u53c2\u6570\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793aRAPID\u5728\u7075\u654f\u5ea6\u3001\u6297\u566a\u6027\u80fd\u548c\u9635\u5217\u534f\u540c\uff08\u91cf\u5b50\u6ce2\u675f\u6210\u5f62\uff09\u65b9\u9762\u4f18\u4e8e\u9759\u6001\u65b9\u6cd5\uff0c\u8fbe\u5230\u6d77\u68ee\u5821\u7ea7\u7cbe\u5ea6\u3002", "conclusion": "RAPID\u4e3a\u91cf\u5b50\u4f20\u611f\u5668\u5728\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u7528\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.08147", "pdf": "https://arxiv.org/pdf/2509.08147", "abs": "https://arxiv.org/abs/2509.08147", "authors": ["Zhen Tian", "Fujiang Yuan", "Chunhong Yuan", "Yanhong Peng"], "title": "Mean Field Game-Based Interactive Trajectory Planning Using Physics-Inspired Unified Potential Fields", "categories": ["cs.RO"], "comment": null, "summary": "Interactive trajectory planning in autonomous driving must balance safety,\nefficiency, and scalability under heterogeneous driving behaviors. Existing\nmethods often face high computational cost or rely on external safety critics.\nTo address this, we propose an Interaction-Enriched Unified Potential Field\n(IUPF) framework that fuses style-dependent benefit and risk fields through a\nphysics-inspired variational model, grounded in mean field game theory. The\napproach captures conservative, aggressive, and cooperative behaviors without\nadditional safety modules, and employs stochastic differential equations to\nguarantee Nash equilibrium with exponential convergence. Simulations on lane\nchanging and overtaking scenarios show that IUPF ensures safe distances,\ngenerates smooth and efficient trajectories, and outperforms traditional\noptimization and game-theoretic baselines in both adaptability and\ncomputational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u5747\u573a\u535a\u5f08\u7406\u8bba\u7684\u4ea4\u4e92\u589e\u5f3a\u7edf\u4e00\u52bf\u573a\u6846\u67b6(IUPF)\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u517c\u5177\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u4f9d\u8d56\u5916\u90e8\u5b89\u5168\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684\u53d8\u5206\u6a21\u578b\u878d\u5408\u98ce\u683c\u4f9d\u8d56\u7684\u6548\u76ca\u4e0e\u98ce\u9669\u573a\uff0c\u5229\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u4fdd\u8bc1\u7eb3\u4ec0\u5747\u8861\u7684\u6307\u6570\u6536\u655b\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1IUPF\u5728\u5b89\u5168\u8ddd\u79bb\u4fdd\u6301\u3001\u8f68\u8ff9\u5e73\u6ed1\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u548c\u535a\u5f08\u8bba\u65b9\u6cd5\u3002", "conclusion": "IUPF\u6846\u67b6\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u5b89\u5168\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u6355\u83b7\u591a\u79cd\u9a7e\u9a76\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2509.08656", "pdf": "https://arxiv.org/pdf/2509.08656", "abs": "https://arxiv.org/abs/2509.08656", "authors": ["Jiaqin He", "Max Malyi", "Jonathan Shek"], "title": "Analysis and Control of Acoustic Emissions from Marine Energy Converters", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": null, "summary": "This study investigates the mitigation of acoustic emissions from tidal\ncurrent converters (TCCs) through optimized control strategies to enhance power\ngeneration efficiency while minimizing environmental impacts on marine life. A\nMATLAB/Simulink-based model of a Tidal Current Conversion System (TCCS) was\ndeveloped to simulate the effects of variable control parameters, including\nswitching frequencies, maximum power point tracking (MPPT) coefficients, and\nthe elimination of the gearbox, on underwater noise levels. Acoustic emissions\nwere quantified in terms of sound pressure levels (SPLs), and their potential\nimpacts on marine mammals and fish were evaluated against species-specific\nauditory thresholds for temporary and permanent hearing threshold shifts. The\nresults indicate that adjusting control parameters can significantly reduce\nSPLs, with the removal of the gearbox yielding the greatest noise reduction.\nThe study identifies operational conditions under which marine species are at\nrisk of auditory damage and proposes control strategies to mitigate these risks\nwithout compromising energy output. These findings contribute to the\nunderstanding of how control system modifications can balance the efficiency of\nmarine energy systems with ecological considerations, offering guidance for the\ndesign and operation of environmentally compliant TCCs.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u63a7\u5236\u7b56\u7565\u51cf\u5c11\u6f6e\u6c50\u80fd\u8f6c\u6362\u5668\u7684\u6c34\u4e0b\u566a\u58f0\uff0c\u4ee5\u63d0\u5347\u53d1\u7535\u6548\u7387\u5e76\u964d\u4f4e\u5bf9\u6d77\u6d0b\u751f\u7269\u7684\u751f\u6001\u5f71\u54cd\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u517c\u987e\u80fd\u6548\u4e0e\u73af\u4fdd\u7684\u64cd\u4f5c\u7b56\u7565\u3002", "motivation": "\u6f6e\u6c50\u80fd\u8f6c\u6362\u5668\u5728\u53d1\u7535\u8fc7\u7a0b\u4e2d\u4f1a\u4ea7\u751f\u6c34\u4e0b\u566a\u58f0\uff0c\u53ef\u80fd\u5bf9\u6d77\u6d0b\u751f\u7269\uff08\u5982\u54fa\u4e73\u7c7b\u548c\u9c7c\u7c7b\uff09\u7684\u542c\u89c9\u7cfb\u7edf\u9020\u6210\u635f\u5bb3\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u63a7\u5236\u7b56\u7565\u51cf\u5c11\u566a\u58f0\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u53d1\u7535\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eMATLAB/Simulink\u7684\u6f6e\u6c50\u80fd\u8f6c\u6362\u7cfb\u7edf\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5f00\u5173\u9891\u7387\u3001MPPT\u7cfb\u6570\u548c\u53bb\u9664\u53d8\u901f\u7bb1\u7b49\u53c2\u6570\u5bf9\u6c34\u4e0b\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u91cf\u5316\u4e86\u58f0\u538b\u7ea7\u5e76\u8bc4\u4f30\u4e86\u5bf9\u6d77\u6d0b\u751f\u7269\u7684\u6f5c\u5728\u5371\u5bb3\u3002", "result": "\u8c03\u6574\u63a7\u5236\u53c2\u6570\u53ef\u663e\u8457\u964d\u4f4e\u566a\u58f0\uff0c\u53bb\u9664\u53d8\u901f\u7bb1\u6548\u679c\u6700\u4f73\u3002\u7814\u7a76\u8fd8\u786e\u5b9a\u4e86\u53ef\u80fd\u5bf9\u6d77\u6d0b\u751f\u7269\u9020\u6210\u542c\u89c9\u635f\u4f24\u7684\u8fd0\u884c\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u517c\u987e\u53d1\u7535\u6548\u7387\u7684\u566a\u58f0\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u63a7\u5236\u7cfb\u7edf\u7684\u4f18\u5316\u80fd\u591f\u5728\u63d0\u5347\u6f6e\u6c50\u80fd\u8f6c\u6362\u5668\u6548\u7387\u7684\u540c\u65f6\u51cf\u5c11\u751f\u6001\u5f71\u54cd\uff0c\u4e3a\u73af\u4fdd\u517c\u5bb9\u7684\u8bbe\u5907\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.08157", "pdf": "https://arxiv.org/pdf/2509.08157", "abs": "https://arxiv.org/abs/2509.08157", "authors": ["Viraj Parimi", "Brian C. Williams"], "title": "Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Safe navigation is essential for autonomous systems operating in hazardous\nenvironments, especially when multiple agents must coordinate using just visual\ninputs over extended time horizons. Traditional planning methods excel at\nsolving long-horizon tasks but rely on predefined distance metrics, while safe\nReinforcement Learning (RL) can learn complex behaviors using high-dimensional\ninputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work\ncombined these paradigms by leveraging goal-conditioned RL (GCRL) to build an\nintermediate graph from replay buffer states, pruning unsafe edges, and using\nConflict-Based Search (CBS) for multi-agent path planning. Although effective,\nthis graph-pruning approach can be overly conservative, limiting mission\nefficiency by precluding missions that must traverse high-risk regions. To\naddress this limitation, we propose RB-CBS, a novel extension to CBS that\ndynamically allocates and adjusts user-specified risk bound ($\\Delta$) across\nagents to flexibly trade off safety and speed. Our improved planner ensures\nthat each agent receives a local risk budget ($\\delta$) enabling more efficient\nnavigation while still respecting overall safety constraints. Experimental\nresults demonstrate that this iterative risk-allocation framework yields\nsuperior performance in complex environments, allowing multiple agents to find\ncollision-free paths within the user-specified $\\Delta$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u98ce\u9669\u9884\u7b97\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u65b9\u6cd5RB-CBS\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u56fe\u526a\u679d\u65b9\u6cd5\u5728\u5b89\u5168\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u5371\u9669\u73af\u5883\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u534f\u8c03\u5bfc\u822a\u9700\u8981\u5e73\u8861\u5b89\u5168\u548c\u6548\u7387\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u8981\u4e48\u96be\u4ee5\u5904\u7406\u590d\u6742\u573a\u666f\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u5206\u914d\u7528\u6237\u6307\u5b9a\u7684\u98ce\u9669\u9884\u7b97\uff08\u0394\uff09\uff0c\u5e76\u7ed3\u5408\u51b2\u7a81\u641c\u7d22\uff08CBS\uff09\uff0c\u6539\u8fdb\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cRB-C\u6846\u67b6\u5728\u591a\u667a\u80fd\u4f53\u590d\u6742\u73af\u5883\u4e2d\u80fd\u66f4\u9ad8\u6548\u5730\u627e\u5230\u65e0\u78b0\u649e\u8def\u5f84\u3002", "conclusion": "RB-CBS\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6548\u7387\u3002"}}
{"id": "2509.08672", "pdf": "https://arxiv.org/pdf/2509.08672", "abs": "https://arxiv.org/abs/2509.08672", "authors": ["Tong Wu", "Anna Scaglione", "Sandy Miguel", "Daniel Arnold"], "title": "Universal Graph Learning for Power System Reconfigurations: Transfer Across Topology Variations", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work addresses a fundamental challenge in applying deep learning to\npower systems: developing neural network models that transfer across\nsignificant system changes, including networks with entirely different\ntopologies and dimensionalities, without requiring training data from unseen\nreconfigurations. Despite extensive research, most ML-based approaches remain\nsystem-specific, limiting real-world deployment. This limitation stems from a\ndual barrier. First, topology changes shift feature distributions and alter\ninput dimensions due to power flow physics. Second, reconfigurations redefine\noutput semantics and dimensionality, requiring models to handle\nconfiguration-specific outputs while maintaining transferable feature\nextraction. To overcome this challenge, we introduce a Universal Graph\nConvolutional Network (UGCN) that achieves transferability to any\nreconfiguration or variation of existing power systems without any prior\nknowledge of new grid topologies or retraining during implementation. Our\napproach applies to both transmission and distribution networks and\ndemonstrates generalization capability to completely unseen system\nreconfigurations, such as network restructuring and major grid expansions.\nExperimental results across power system applications, including false data\ninjection detection and state forecasting, show that UGCN significantly\noutperforms state-of-the-art methods in cross-system zero-shot transferability\nof new reconfigurations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08UGCN\uff09\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u7535\u529b\u7cfb\u7edf\u4e2d\u65e0\u6cd5\u8de8\u7cfb\u7edf\u62d3\u6251\u53d8\u5316\u8fc1\u79fb\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u591a\u4e3a\u7cfb\u7edf\u4e13\u7528\uff0c\u96be\u4ee5\u9002\u5e94\u7535\u529b\u7cfb\u7edf\u62d3\u6251\u548c\u7ef4\u5ea6\u7684\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86UGCN\uff0c\u65e0\u9700\u65b0\u7f51\u683c\u62d3\u6251\u7684\u5148\u9a8c\u77e5\u8bc6\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u5373\u53ef\u9002\u5e94\u4efb\u4f55\u7cfb\u7edf\u91cd\u65b0\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUGCN\u5728\u8de8\u7cfb\u7edf\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UGCN\u4e3a\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u8fc1\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08159", "pdf": "https://arxiv.org/pdf/2509.08159", "abs": "https://arxiv.org/abs/2509.08159", "authors": ["Steven Yang", "Xiaoyu Tian", "Kshitij Goel", "Wennie Tabib"], "title": "Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial Rescaling for Autonomous Aerial Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper presents a methodology to predict metric depth from monocular RGB\nimages and an inertial measurement unit (IMU). To enable collision avoidance\nduring autonomous flight, prior works either leverage heavy sensors (e.g.,\nLiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of\nmonocular metric depth estimation methods. In contrast, we propose several\nlightweight zero-shot rescaling strategies to obtain metric depth from relative\ndepth estimates via the sparse 3D feature map created using a visual-inertial\nnavigation system. These strategies are compared for their accuracy in diverse\nsimulation environments. The best performing approach, which leverages\nmonotonic spline fitting, is deployed in the real-world on a\ncompute-constrained quadrotor. We obtain on-board metric depth estimates at 15\nHz and demonstrate successful collision avoidance after integrating the\nproposed method with a motion primitives-based planner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5355\u76eeRGB\u56fe\u50cf\u548cIMU\u9884\u6d4b\u5ea6\u91cf\u6df1\u5ea6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u907f\u969c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u91cd\u578b\u4f20\u611f\u5668\u6216\u590d\u6742\u7684\u6570\u636e\u8c03\u4f18\uff0c\u800c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u8f7b\u91cf\u5316\u7684\u96f6\u6837\u672c\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u79cd\u8f7b\u91cf\u5316\u7684\u96f6\u6837\u672c\u91cd\u7f29\u653e\u7b56\u7565\uff0c\u5229\u7528\u89c6\u89c9-\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u751f\u6210\u7684\u7a00\u758f3D\u7279\u5f81\u56fe\u4ece\u76f8\u5bf9\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u83b7\u5f97\u5ea6\u91cf\u6df1\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7b56\u7565\u7684\u51c6\u786e\u6027\uff0c\u6700\u4f18\u65b9\u6cd5\u5728\u5b9e\u9645\u8ba1\u7b97\u53d7\u9650\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u5b9e\u73b0\u4e8615 Hz\u7684\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u5e76\u6210\u529f\u907f\u969c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u5316\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u548c\u907f\u969c\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2509.08160", "pdf": "https://arxiv.org/pdf/2509.08160", "abs": "https://arxiv.org/abs/2509.08160", "authors": ["Viraj Parimi", "Brian C. Williams"], "title": "Diffusion-Guided Multi-Arm Motion Planning", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-arm motion planning is fundamental for enabling arms to complete\ncomplex long-horizon tasks in shared spaces efficiently but current methods\nstruggle with scalability due to exponential state-space growth and reliance on\nlarge training datasets for learned models. Inspired by Multi-Agent Path\nFinding (MAPF), which decomposes planning into single-agent problems coupled\nwith collision resolution, we propose a novel diffusion-guided multi-arm\nplanner (DG-MAP) that enhances scalability of learning-based models while\nreducing their reliance on massive multi-arm datasets. Recognizing that\ncollisions are primarily pairwise, we train two conditional diffusion models,\none to generate feasible single-arm trajectories, and a second, to model the\ndual-arm dynamics required for effective pairwise collision resolution. By\nintegrating these specialized generative models within a MAPF-inspired\nstructured decomposition, our planner efficiently scales to larger number of\narms. Evaluations against alternative learning-based methods across various\nteam sizes demonstrate our method's effectiveness and practical applicability.\nProject website can be found at https://diff-mapf-mers.csail.mit.edu", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u5f15\u5bfc\u7684\u591a\u81c2\u8fd0\u52a8\u89c4\u5212\u5668\uff08DG-MAP\uff09\uff0c\u901a\u8fc7\u5206\u89e3\u95ee\u9898\u548c\u751f\u6210\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u81c2\u8fd0\u52a8\u89c4\u5212\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u591a\u81c2\u8fd0\u52a8\u89c4\u5212\u5728\u5171\u4eab\u7a7a\u95f4\u4e2d\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u5177\u6709\u91cd\u8981\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u72b6\u6001\u7a7a\u95f4\u6307\u6570\u589e\u957f\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u4f9d\u8d56\u800c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u7684\u601d\u60f3\uff0c\u8bad\u7ec3\u4e24\u4e2a\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff1a\u4e00\u4e2a\u751f\u6210\u5355\u81c2\u8f68\u8ff9\uff0c\u53e6\u4e00\u4e2a\u89e3\u51b3\u53cc\u81c2\u78b0\u649e\u95ee\u9898\u3002", "result": "DG-MAP\u5728\u6269\u5c55\u6027\u548c\u51cf\u5c11\u6570\u636e\u96c6\u4f9d\u8d56\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "DG-MAP\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u548c\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u81c2\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2509.08177", "pdf": "https://arxiv.org/pdf/2509.08177", "abs": "https://arxiv.org/abs/2509.08177", "authors": ["Jonathan Lee", "Abhishek Rathod", "Kshitij Goel", "John Stecklein", "Wennie Tabib"], "title": "Quadrotor Navigation using Reinforcement Learning with Privileged Information", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper presents a reinforcement learning-based quadrotor navigation\nmethod that leverages efficient differentiable simulation, novel loss\nfunctions, and privileged information to navigate around large obstacles. Prior\nlearning-based methods perform well in scenes that exhibit narrow obstacles,\nbut struggle when the goal location is blocked by large walls or terrain. In\ncontrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged\ninformation and a yaw alignment loss to guide the robot around large obstacles.\nThe policy is evaluated in photo-realistic simulation environments containing\nlarge obstacles, sharp corners, and dead-ends. Our approach achieves an 86%\nsuccess rate and outperforms baseline strategies by 34%. We deploy the policy\nonboard a custom quadrotor in outdoor cluttered environments both during the\nday and night. The policy is validated across 20 flights, covering 589 meters\nwithout collisions at speeds up to 4 m/s.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56db\u65cb\u7ffc\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u53ef\u5fae\u6a21\u62df\u3001\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u548c\u7279\u6743\u4fe1\u606f\uff08\u5982\u5230\u8fbe\u65f6\u95f4\u5730\u56fe\uff09\u6765\u7ed5\u8fc7\u5927\u969c\u788d\u7269\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u969c\u788d\u7269\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5927\u969c\u788d\u7269\u5982\u5899\u58c1\u6216\u5730\u5f62\u906e\u6321\u76ee\u6807\u4f4d\u7f6e\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5230\u8fbe\u65f6\u95f4\uff08ToA\uff09\u5730\u56fe\u4f5c\u4e3a\u7279\u6743\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u504f\u822a\u5bf9\u9f50\u635f\u5931\u51fd\u6570\u6765\u5f15\u5bfc\u673a\u5668\u4eba\u7ed5\u8fc7\u5927\u969c\u788d\u7269\u3002", "result": "\u5728\u5305\u542b\u5927\u969c\u788d\u7269\u3001\u9510\u89d2\u548c\u6b7b\u89d2\u7684\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8686%\u7684\u6210\u529f\u7387\uff0c\u6bd4\u57fa\u7ebf\u7b56\u7565\u9ad8\u51fa34%\u3002\u5b9e\u9645\u98de\u884c\u6d4b\u8bd5\u4e2d\uff0c\u5728\u5ba4\u5916\u6742\u4e71\u73af\u5883\u4e2d\u5b8c\u6210\u4e8620\u6b21\u98de\u884c\uff0c\u8986\u76d6589\u7c73\u65e0\u78b0\u649e\uff0c\u6700\u9ad8\u901f\u5ea6\u8fbe4\u7c73/\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7279\u6743\u4fe1\u606f\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u65cb\u7ffc\u5728\u5927\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2509.08197", "pdf": "https://arxiv.org/pdf/2509.08197", "abs": "https://arxiv.org/abs/2509.08197", "authors": ["Jesse Morris", "Yiduo Wang", "Viorela Ila"], "title": "Online Dynamic SLAM with Incremental Smoothing and Mapping", "categories": ["cs.RO"], "comment": "8 pages, 8 figures, Submitted RA-L 2025", "summary": "Dynamic SLAM methods jointly estimate for the static and dynamic scene\ncomponents, however existing approaches, while accurate, are computationally\nexpensive and unsuitable for online applications. In this work, we present the\nfirst application of incremental optimisation techniques to Dynamic SLAM. We\nintroduce a novel factor-graph formulation and system architecture designed to\ntake advantage of existing incremental optimisation methods and support online\nestimation. On multiple datasets, we demonstrate that our method achieves equal\nto or better than state-of-the-art in camera pose and object motion accuracy.\nWe further analyse the structural properties of our approach to demonstrate its\nscalability and provide insight regarding the challenges of solving Dynamic\nSLAM incrementally. Finally, we show that our formulation results in problem\nstructure well-suited to incremental solvers, while our system architecture\nfurther enhances performance, achieving a 5x speed-up over existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u91cf\u4f18\u5316\u7684\u52a8\u6001SLAM\u65b9\u6cd5\uff0c\u9996\u6b21\u5728\u52a8\u6001SLAM\u4e2d\u5e94\u7528\u589e\u91cf\u4f18\u5316\u6280\u672f\uff0c\u65b0\u8bbe\u8ba1\u4e86\u56e0\u5b50\u56fe\u6846\u67b6\u548c\u7cfb\u7edf\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u52a8\u6001SLAM\u65b9\u6cd5\u51c6\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u65e0\u6cd5\u5728\u7ebf\u5e94\u7528\u3002", "method": "\u91c7\u7528\u589e\u91cf\u4f18\u5316\u6280\u672f\uff0c\u8bbe\u8ba1\u65b0\u56e0\u5b50\u56fe\u6846\u67b6\u548c\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u53475\u500d\u3002", "conclusion": "\u589e\u91cf\u4f18\u5316\u9002\u5408\u52a8\u6001SLAM\uff0c\u7cfb\u7edf\u67b6\u6784\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.08221", "pdf": "https://arxiv.org/pdf/2509.08221", "abs": "https://arxiv.org/abs/2509.08221", "authors": ["Elahe Delavari", "Feeza Khan Khanzada", "Jaerock Kwon"], "title": "A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous-driving research has recently embraced deep Reinforcement Learning\n(RL) as a promising framework for data-driven decision making, yet a clear\npicture of how these algorithms are currently employed, benchmarked and\nevaluated is still missing. This survey fills that gap by systematically\nanalysing around 100 peer-reviewed papers that train, test or validate RL\npolicies inside the open-source CARLA simulator. We first categorize the\nliterature by algorithmic family model-free, model-based, hierarchical, and\nhybrid and quantify their prevalence, highlighting that more than 80% of\nexisting studies still rely on model-free methods such as DQN, PPO and SAC.\nNext, we explain the diverse state, action and reward formulations adopted\nacross works, illustrating how choices of sensor modality (RGB, LiDAR, BEV,\nsemantic maps, and carla kinematics states), control abstraction (discrete vs.\ncontinuous) and reward shaping are used across various literature. We also\nconsolidate the evaluation landscape by listing the most common metrics\n(success rate, collision rate, lane deviation, driving score) and the towns,\nscenarios and traffic configurations used in CARLA benchmarks. Persistent\nchallenges including sparse rewards, sim-to-real transfer, safety guarantees\nand limited behaviour diversity are distilled into a set of open research\nquestions, and promising directions such as model-based RL, meta-learning and\nricher multi-agent simulations are outlined. By providing a unified taxonomy,\nquantitative statistics and a critical discussion of limitations, this review\naims to serve both as a reference for newcomers and as a roadmap for advancing\nRL-based autonomous driving toward real-world deployment.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u7ea6100\u7bc7\u5728CARLA\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u3001\u6d4b\u8bd5\u6216\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u8bba\u6587\uff0c\u603b\u7ed3\u4e86RL\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\u3001\u65b9\u6cd5\u5206\u7c7b\u53ca\u6311\u6218\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u88ab\u8ba4\u4e3a\u662f\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u6709\u524d\u666f\u6846\u67b6\uff0c\u4f46\u5bf9\u5176\u5f53\u524d\u5e94\u7528\u3001\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u5168\u9762\u68b3\u7406\u4ecd\u7136\u7f3a\u4e4f\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6587\u732e\uff08\u6a21\u578b\u65e0\u5173\u3001\u6a21\u578b\u76f8\u5173\u3001\u5206\u5c42\u548c\u6df7\u5408\u65b9\u6cd5\uff09\uff0c\u91cf\u5316\u7814\u7a76\u8d8b\u52bf\uff0c\u5e76\u5206\u6790\u72b6\u6001\u3001\u52a8\u4f5c\u3001\u5956\u52b1\u7684\u591a\u6837\u914d\u7f6e\u53ca\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u8d85\u8fc780%\u7684\u7814\u7a76\u4ecd\u4f9d\u8d56\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff08\u5982DQN\u3001PPO\u3001SAC\uff09\u3002\u7814\u7a76\u63ed\u793a\u4e86\u8bc4\u4f30\u6807\u51c6\u7684\u591a\u6837\u6027\uff0c\u5e76\u63d0\u51fa\u5f00\u653e\u6027\u95ee\u9898\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u65b0\u4eba\u63d0\u4f9b\u53c2\u8003\uff0c\u5e76\u4e3aRL\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u8def\u7ebf\u56fe\u3002"}}
{"id": "2509.08226", "pdf": "https://arxiv.org/pdf/2509.08226", "abs": "https://arxiv.org/abs/2509.08226", "authors": ["Yoshiki Kanai", "Akira Kanazawa", "Hideyuki Ichiwara", "Hiroshi Ito", "Naoaki Noguchi", "Tetsuya Ogata"], "title": "Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback Teleoperation Method for Low-cost Hardware", "categories": ["cs.RO"], "comment": null, "summary": "Effective data collection in contact-rich manipulation requires force\nfeedback during teleoperation, as accurate perception of contact is crucial for\nstable control. However, such technology remains uncommon, largely because\nbilateral teleoperation systems are complex and difficult to implement. To\novercome this, we propose a bilateral teleoperation method that relies only on\na simple feedback controller and does not require force sensors. The approach\nis designed for leader-follower setups using low-cost hardware, making it\nbroadly applicable. Through numerical simulations and real-world experiments,\nwe demonstrate that the method requires minimal parameter tuning, yet achieves\nboth high operability and contact stability, outperforming conventional\napproaches. Furthermore, we show its high robustness: even at low communication\ncycle rates between leader and follower, control performance degradation is\nminimal compared to high-speed operation. We also prove our method can be\nimplemented on two types of commercially available low-cost hardware with zero\nparameter adjustments. This highlights its high ease of implementation and\nversatility. We expect this method will expand the use of force feedback\nteleoperation systems on low-cost hardware. This will contribute to advancing\ncontact-rich task autonomy in imitation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u529b\u4f20\u611f\u5668\u7684\u53cc\u8fb9\u9065\u64cd\u4f5c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u786c\u4ef6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u64cd\u4f5c\u6027\u548c\u63a5\u89e6\u7a33\u5b9a\u6027\uff0c\u4e14\u6613\u4e8e\u5b9e\u65bd\u548c\u63a8\u5e7f\u3002", "motivation": "\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4e2d\uff0c\u529b\u53cd\u9988\u6570\u636e\u6536\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u53cc\u8fb9\u9065\u64cd\u4f5c\u6280\u672f\u590d\u6742\u4e14\u96be\u4ee5\u5b9e\u73b0\uff0c\u4e9f\u9700\u7b80\u5316\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u53cd\u9988\u63a7\u5236\u5668\u548c\u4f4e\u6210\u672c\u786c\u4ef6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u9700\u529b\u4f20\u611f\u5668\u7684\u53cc\u8fb9\u9065\u64cd\u4f5c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u8bbe\u7f6e\u3002", "result": "\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53c2\u6570\u8c03\u6574\u5c11\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\uff0c\u4e14\u5728\u4e0d\u540c\u901a\u4fe1\u901f\u7387\u4e0b\u4fdd\u6301\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u7b80\u5316\u4e86\u529b\u53cd\u9988\u9065\u64cd\u4f5c\u7cfb\u7edf\u7684\u5b9e\u65bd\uff0c\u6709\u671b\u63a8\u52a8\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u7684\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u81ea\u4e3b\u5316\u3002"}}
{"id": "2509.08235", "pdf": "https://arxiv.org/pdf/2509.08235", "abs": "https://arxiv.org/abs/2509.08235", "authors": ["Sheng Zhong", "Junkai Niu", "Yi Zhou"], "title": "Deep Visual Odometry for Stereo Event Cameras", "categories": ["cs.RO"], "comment": null, "summary": "Event-based cameras are bio-inspired sensors with pixels that independently\nand asynchronously respond to brightness changes at microsecond resolution,\noffering the potential to handle state estimation tasks involving motion blur\nand high dynamic range (HDR) illumination conditions. However, the versatility\nof event-based visual odometry (VO) relying on handcrafted data association\n(either direct or indirect methods) is still unreliable, especially in field\nrobot applications under low-light HDR conditions, where the dynamic range can\nbe enormous and the signal-to-noise ratio is spatially-and-temporally varying.\nLeveraging deep neural networks offers new possibilities for overcoming these\nchallenges. In this paper, we propose a learning-based stereo event visual\nodometry. Building upon Deep Event Visual Odometry (DEVO), our system (called\nStereo-DEVO) introduces a novel and efficient static-stereo association\nstrategy for sparse depth estimation with almost no additional computational\nburden. By integrating it into a tightly coupled bundle adjustment (BA)\noptimization scheme, and benefiting from the recurrent network's ability to\nperform accurate optical flow estimation through voxel-based event\nrepresentations to establish reliable patch associations, our system achieves\nhigh-precision pose estimation in metric scale. In contrast to the offline\nperformance of DEVO, our system can process event data of \\zs{Video Graphics\nArray} (VGA) resolution in real time. Extensive evaluations on multiple public\nreal-world datasets and self-collected data justify our system's versatility,\ndemonstrating superior performance compared to state-of-the-art event-based VO\nmethods. More importantly, our system achieves stable pose estimation even in\nlarge-scale nighttime HDR scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u4e8b\u4ef6\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff08Stereo-DEVO\uff09\uff0c\u901a\u8fc7\u9759\u6001\u7acb\u4f53\u5173\u8054\u7b56\u7565\u548c\u7d27\u5bc6\u8026\u5408\u7684\u675f\u8c03\u6574\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5728\u4f4e\u5149\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u5fae\u79d2\u7ea7\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u4f7f\u5176\u5728\u8fd0\u52a8\u6a21\u7cca\u548c\u9ad8\u52a8\u6001\u7167\u660e\u6761\u4ef6\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4f20\u7edf\u7684\u624b\u5de5\u6570\u636e\u5173\u8054\u65b9\u6cd5\u5728\u4f4e\u5149HDR\u6761\u4ef6\u4e0b\u4e0d\u53ef\u9760\u3002\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "method": "\u57fa\u4e8eDEVO\u6846\u67b6\uff0cStereo-DEVO\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9759\u6001\u7acb\u4f53\u5173\u8054\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u7684\u675f\u8c03\u6574\u4f18\u5316\u548c\u57fa\u4e8e\u4f53\u7d20\u7684\u4e8b\u4ef6\u8868\u793a\u8fdb\u884c\u5149\u6d41\u4f30\u8ba1\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u5b9e\u65f6\u5904\u7406VGA\u5206\u8fa8\u7387\u7684\u4e8b\u4ef6\u6570\u636e\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u91c7\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u591c\u95f4HDR\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "Stereo-DEVO\u5728\u6027\u80fd\u548c\u5b9e\u65f6\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u4e8b\u4ef6\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4f4e\u5149HDR\u73af\u5883\u3002"}}
{"id": "2509.08241", "pdf": "https://arxiv.org/pdf/2509.08241", "abs": "https://arxiv.org/abs/2509.08241", "authors": ["Zixin Zhang", "James Avtges", "Todd D. Murphey"], "title": "Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Data-driven control methods need to be sample-efficient and lightweight,\nespecially when data acquisition and computational resources are limited --\nsuch as during learning on hardware. Most modern data-driven methods require\nlarge datasets and struggle with real-time updates of models, limiting their\nperformance in dynamic environments. Koopman theory formally represents\nnonlinear systems as linear models over observables, and Koopman\nrepresentations can be determined from data in an optimization-friendly setting\nwith potentially rapid model updates. In this paper, we present a highly\nsample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning\n(RKL). We identify sufficient conditions for model convergence and provide\nformal algorithmic analysis supporting our claim that RKL is lightweight and\nfast, with complexity independent of dataset size. We validate our method on a\nsimulated planar two-link arm and a hybrid nonlinear hardware system with soft\nactuators, showing that real-time recursive Koopman model updates improve the\nsample efficiency and stability of data-driven controller synthesis --\nrequiring only <10% of the data compared to benchmarks. The high-performance\nC++ codebase is open-sourced. Website:\nhttps://www.zixinatom990.com/home/robotics/corl-2025-recursive-koopman-learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684Koopman\u5b66\u4e60\u6846\u67b6\uff08RKL\uff09\uff0c\u80fd\u591f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u66f4\u65b0\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u63a7\u5236\u5728\u6570\u636e\u83b7\u53d6\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff08\u5982\u786c\u4ef6\u5b66\u4e60\uff09\u65f6\u9700\u8981\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u6570\u636e\u96c6\u4e14\u96be\u4ee5\u5b9e\u65f6\u66f4\u65b0\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u57fa\u4e8eKoopman\u7406\u8bba\uff0c\u5c06\u975e\u7ebf\u6027\u7cfb\u7edf\u7ebf\u6027\u5316\u4e3a\u53ef\u89c2\u6d4b\u6a21\u578b\uff0c\u63d0\u51fa\u9012\u5f52Koopman\u5b66\u4e60\uff08RKL\uff09\uff0c\u7406\u8bba\u57fa\u7840\u652f\u6301\u5176\u8f7b\u91cf\u7ea7\u548c\u5feb\u901f\u66f4\u65b0\u7684\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cRKL\u5728\u6570\u636e\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u57fa\u51c6\u65b9\u6cd510%\u7684\u6570\u636e\u3002", "conclusion": "RKL\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u9a71\u52a8\u63a7\u5236\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002\u5f00\u6e90\u9ad8\u6027\u80fd\u4ee3\u7801\u5e93\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.08242", "pdf": "https://arxiv.org/pdf/2509.08242", "abs": "https://arxiv.org/abs/2509.08242", "authors": ["Nirabhra Mandal", "Aamodh Suresh", "Carlos Nieto-Granda", "Sonia Mart\u00ednez"], "title": "Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed Task Allocation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "10 pages, 5 figures", "summary": "We study a problem of multi-agent exploration with behaviorally heterogeneous\nrobots. Each robot maps its surroundings using SLAM and identifies a set of\nareas of interest (AoIs) or frontiers that are the most informative to explore\nnext. The robots assess the utility of going to a frontier using Behavioral\nEntropy (BE) and then determine which frontier to go to via a distributed task\nassignment scheme. We convert the task assignment problem into a\nnon-cooperative game and use a distributed algorithm (d-PBRAG) to converge to\nthe Nash equilibrium (which we show is the optimal task allocation solution).\nFor unknown utility cases, we provide robust bounds using approximate rewards.\nWe test our algorithm (which has less communication cost and fast convergence)\nin simulation, where we explore the effect of sensing radii, sensing accuracy,\nand heterogeneity among robotic teams with respect to the time taken to\ncomplete exploration and path traveled. We observe that having a team of agents\nwith heterogeneous behaviors is beneficial.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u884c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u7684\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u95ee\u9898\uff0c\u901a\u8fc7SLAM\u6784\u5efa\u5730\u56fe\u5e76\u8bc4\u4f30\u524d\u6cbf\u533a\u57df\u7684\u63a2\u7d22\u4ef7\u503c\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u7b97\u6cd5\u89e3\u51b3\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u9ad8\u6548\u6027\u548c\u5f02\u6784\u56e2\u961f\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u884c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u5728\u63a2\u7d22\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u5206\u5e03\u5f0f\u7b97\u6cd5\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u3002", "method": "\u4f7f\u7528SLAM\u6784\u5efa\u73af\u5883\u5730\u56fe\uff0c\u901a\u8fc7\u884c\u4e3a\u71b5\uff08BE\uff09\u8bc4\u4f30\u524d\u6cbf\u533a\u57df\u7684\u63a2\u7d22\u4ef7\u503c\uff0c\u5e76\u91c7\u7528\u5206\u5e03\u5f0f\u975e\u5408\u4f5c\u535a\u5f08\u7b97\u6cd5\uff08d-PBRAG\uff09\u6536\u655b\u81f3\u7eb3\u4ec0\u5747\u8861\uff0c\u5b9e\u73b0\u6700\u4f18\u4efb\u52a1\u5206\u914d\u3002\u5bf9\u4e8e\u672a\u77e5\u6548\u7528\u60c5\u51b5\uff0c\u63d0\u4f9b\u57fa\u4e8e\u8fd1\u4f3c\u5956\u52b1\u7684\u9c81\u68d2\u6027\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7b97\u6cd5\u901a\u4fe1\u6210\u672c\u4f4e\u4e14\u6536\u655b\u901f\u5ea6\u5feb\u3002\u5f02\u6784\u884c\u4e3a\u56e2\u961f\u5728\u63a2\u7d22\u5b8c\u6210\u65f6\u95f4\u548c\u8def\u5f84\u957f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u884c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u5206\u5e03\u5f0f\u7b97\u6cd5d-PBRAG\u5728\u4efb\u52a1\u5206\u914d\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.08257", "pdf": "https://arxiv.org/pdf/2509.08257", "abs": "https://arxiv.org/abs/2509.08257", "authors": ["Yongkai Tian", "Yirong Qi", "Xin Yu", "Wenjun Wu", "Jie Luo"], "title": "Symmetry-Guided Multi-Agent Inverse Reinforcement Learnin", "categories": ["cs.RO", "cs.AI"], "comment": "8pages, 6 figures. Accepted for publication in the Proceedings of the\n  2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025) as oral presentation", "summary": "In robotic systems, the performance of reinforcement learning depends on the\nrationality of predefined reward functions. However, manually designed reward\nfunctions often lead to policy failures due to inaccuracies. Inverse\nReinforcement Learning (IRL) addresses this problem by inferring implicit\nreward functions from expert demonstrations. Nevertheless, existing methods\nrely heavily on large amounts of expert demonstrations to accurately recover\nthe reward function. The high cost of collecting expert demonstrations in\nrobotic applications, particularly in multi-robot systems, severely hinders the\npractical deployment of IRL. Consequently, improving sample efficiency has\nemerged as a critical challenge in multi-agent inverse reinforcement learning\n(MIRL). Inspired by the symmetry inherent in multi-agent systems, this work\ntheoretically demonstrates that leveraging symmetry enables the recovery of\nmore accurate reward functions. Building upon this insight, we propose a\nuniversal framework that integrates symmetry into existing multi-agent\nadversarial IRL algorithms, thereby significantly enhancing sample efficiency.\nExperimental results from multiple challenging tasks have demonstrated the\neffectiveness of this framework. Further validation in physical multi-robot\nsystems has shown the practicality of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5bf9\u79f0\u6027\u63d0\u5347\u9006\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u7684\u901a\u7528\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u5927\u91cf\u4e13\u5bb6\u793a\u8303\u6570\u636e\uff0c\u4f46\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6536\u96c6\u6b64\u7c7b\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u63d0\u9ad8\u6837\u672c\u6548\u7387\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5bf9\u79f0\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5bf9\u6297\u9006\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u4e14\u5728\u7269\u7406\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u5bf9\u79f0\u6027\u5728\u591a\u667a\u80fd\u4f53\u9006\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u89e3\u51b3\u6837\u672c\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.08302", "pdf": "https://arxiv.org/pdf/2509.08302", "abs": "https://arxiv.org/abs/2509.08302", "authors": ["Rajendramayavan Sathyam", "Yueqi Li"], "title": "Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities", "categories": ["cs.RO", "cs.CV"], "comment": "32 pages, 14 figures, accepted at IEEE Open Journal of Vehicular\n  Technology (OJVT)", "summary": "Foundation models are revolutionizing autonomous driving perception,\ntransitioning the field from narrow, task-specific deep learning models to\nversatile, general-purpose architectures trained on vast, diverse datasets.\nThis survey examines how these models address critical challenges in autonomous\nperception, including limitations in generalization, scalability, and\nrobustness to distributional shifts. The survey introduces a novel taxonomy\nstructured around four essential capabilities for robust performance in dynamic\ndriving environments: generalized knowledge, spatial understanding,\nmulti-sensor robustness, and temporal reasoning. For each capability, the\nsurvey elucidates its significance and comprehensively reviews cutting-edge\napproaches. Diverging from traditional method-centric surveys, our unique\nframework prioritizes conceptual design principles, providing a\ncapability-driven guide for model development and clearer insights into\nfoundational aspects. We conclude by discussing key challenges, particularly\nthose associated with the integration of these capabilities into real-time,\nscalable systems, and broader deployment challenges related to computational\ndemands and ensuring model reliability against issues like hallucinations and\nout-of-distribution failures. The survey also outlines crucial future research\ndirections to enable the safe and effective deployment of foundation models in\nautonomous driving systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e86\u57fa\u7840\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u9886\u57df\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u901a\u7528\u67b6\u6784\u89e3\u51b3\u6cdb\u5316\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5206\u5e03\u504f\u79fb\u7b49\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u5927\u5173\u952e\u80fd\u529b\u7684\u65b0\u5206\u7c7b\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u9886\u57df\u6b63\u4ece\u4efb\u52a1\u4e13\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8f6c\u5411\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u5982\u4f55\u5b9e\u73b0\u5176\u5728\u5b9e\u9645\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u80fd\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u56db\u5927\u5173\u952e\u80fd\u529b\uff08\u6cdb\u5316\u77e5\u8bc6\u3001\u7a7a\u95f4\u7406\u89e3\u3001\u591a\u4f20\u611f\u5668\u7a33\u5065\u6027\u548c\u65f6\u95f4\u63a8\u7406\uff09\u4e3a\u6838\u5fc3\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u5e76\u4ece\u6982\u5ff5\u8bbe\u8ba1\u539f\u5219\u51fa\u53d1\uff0c\u7efc\u8ff0\u4e86\u524d\u6cbf\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u57fa\u7840\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u7a81\u51fa\u4e86\u5176\u6f5c\u529b\u4e0e\u5f53\u524d\u5c40\u9650\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5b9e\u65f6\u6027\u3001\u8ba1\u7b97\u9700\u6c42\u548c\u6a21\u578b\u53ef\u9760\u6027\u7b49\u5173\u952e\u95ee\u9898\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u8fd9\u4e9b\u65b9\u5411\u4ee5\u5b9e\u73b0\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2509.08333", "pdf": "https://arxiv.org/pdf/2509.08333", "abs": "https://arxiv.org/abs/2509.08333", "authors": ["Sai Puneeth Reddy Gottam", "Haoming Zhang", "Eivydas Keras"], "title": "Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry", "categories": ["cs.RO", "cs.CV"], "comment": "This short paper has been accepted as a workshop paper at European\n  Conference on Mobile Robots 2025", "summary": "Visual-based localization has made significant progress, yet its performance\noften drops in large-scale, outdoor, and long-term settings due to factors like\nlighting changes, dynamic scenes, and low-texture areas. These challenges\ndegrade feature extraction and tracking, which are critical for accurate motion\nestimation. While learning-based methods such as SuperPoint and SuperGlue show\nimproved feature coverage and robustness, they still face generalization issues\nwith out-of-distribution data. We address this by enhancing deep feature\nextraction and tracking through self-supervised learning with task specific\nfeedback. Our method promotes stable and informative features, improving\ngeneralization and reliability in challenging environments.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u8ddf\u8e2a\uff0c\u4ee5\u5e94\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u5149\u7167\u53d8\u5316\u548c\u4f4e\u7eb9\u7406\u533a\u57df\u7b49\u6311\u6218\u3002", "motivation": "\u89c6\u89c9\u5b9a\u4f4d\u5728\u5927\u89c4\u6a21\u6237\u5916\u548c\u957f\u671f\u573a\u666f\u4e2d\u56e0\u5149\u7167\u53d8\u5316\u3001\u52a8\u6001\u573a\u666f\u548c\u4f4e\u7eb9\u7406\u533a\u57df\u7b49\u95ee\u9898\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5728\u975e\u5206\u5e03\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u5e76\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u53cd\u9988\uff0c\u63d0\u5347\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u8ddf\u8e2a\u7684\u7a33\u5b9a\u6027\u4e0e\u4fe1\u606f\u91cf\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u7a33\u5b9a\u548c\u4e30\u5bcc\u7684\u4fe1\u606f\u7279\u5f81\uff0c\u4ece\u800c\u5728\u590d\u6742\u73af\u5883\u4e2d\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6539\u8fdb\u7684\u7279\u5f81\u63d0\u53d6\u548c\u8ddf\u8e2a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2509.08354", "pdf": "https://arxiv.org/pdf/2509.08354", "abs": "https://arxiv.org/abs/2509.08354", "authors": ["Ce Guo", "Xieyuanli Chen", "Zhiwen Zeng", "Zirui Guo", "Yihong Li", "Haoran Xiao", "Dewen Hu", "Huimin Lu"], "title": "Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration", "categories": ["cs.RO", "cs.AI"], "comment": "20 pages, 19 figures, accepted by IEEE Transactions on Robotics", "summary": "Tactile and kinesthetic perceptions are crucial for human dexterous\nmanipulation, enabling reliable grasping of objects via proprioceptive\nsensorimotor integration. For robotic hands, even though acquiring such tactile\nand kinesthetic feedback is feasible, establishing a direct mapping from this\nsensory feedback to motor actions remains challenging. In this paper, we\npropose a novel glove-mediated tactile-kinematic perception-prediction\nframework for grasp skill transfer from human intuitive and natural operation\nto robotic execution based on imitation learning, and its effectiveness is\nvalidated through generalized grasping tasks, including those involving\ndeformable objects. Firstly, we integrate a data glove to capture tactile and\nkinesthetic data at the joint level. The glove is adaptable for both human and\nrobotic hands, allowing data collection from natural human hand demonstrations\nacross different scenarios. It ensures consistency in the raw data format,\nenabling evaluation of grasping for both human and robotic hands. Secondly, we\nestablish a unified representation of multi-modal inputs based on graph\nstructures with polar coordinates. We explicitly integrate the morphological\ndifferences into the designed representation, enhancing the compatibility\nacross different demonstrators and robotic hands. Furthermore, we introduce the\nTactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage\nmultidimensional subgraph convolutions and attention-based LSTM layers to\nextract spatio-temporal features from graph inputs to predict node-based states\nfor each hand joint. These predictions are then mapped to final commands\nthrough a force-position hybrid mapping.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.08435", "pdf": "https://arxiv.org/pdf/2509.08435", "abs": "https://arxiv.org/abs/2509.08435", "authors": ["Lei Ye", "Haibo Gao", "Peng Xu", "Zhelin Zhang", "Junqi Shan", "Ao Zhang", "Wei Zhang", "Ruyi Zhou", "Zongquan Deng", "Liang Ding"], "title": "PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching", "categories": ["cs.RO"], "comment": "8 pages, 7 figures, conference paper", "summary": "Diffusion models offer powerful generative capabilities for robot trajectory\nplanning, yet their practical deployment on robots is hindered by a critical\nbottleneck: a reliance on imitation learning from expert demonstrations. This\nparadigm is often impractical for specialized robots where data is scarce and\ncreates an inefficient, theoretically suboptimal training pipeline. To overcome\nthis, we introduce PegasusFlow, a hierarchical rolling-denoising framework that\nenables direct and parallel sampling of trajectory score gradients from\nenvironmental interaction, completely bypassing the need for expert data. Our\ncore innovation is a novel sampling algorithm, Weighted Basis Function\nOptimization (WBFO), which leverages spline basis representations to achieve\nsuperior sample efficiency and faster convergence compared to traditional\nmethods like MPPI. The framework is embedded within a scalable, asynchronous\nparallel simulation architecture that supports massively parallel rollouts for\nefficient data collection. Extensive experiments on trajectory optimization and\nrobotic navigation tasks demonstrate that our approach, particularly\nAction-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,\nsignificantly outperforms baselines. In a challenging barrier-crossing task,\nour method achieved a 100% success rate and was 18% faster than the next-best\nmethod, validating its effectiveness for complex terrain locomotion planning.\nhttps://masteryip.github.io/pegasusflow.github.io/", "AI": {"tldr": "PegasusFlow\u662f\u4e00\u79cd\u65e0\u9700\u4e13\u5bb6\u6570\u636e\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6eda\u52a8\u53bb\u566a\u548cWBFO\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u4e2d\u4f9d\u8d56\u4e13\u5bb6\u6570\u636e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u4e13\u7528\u673a\u5668\u4eba\u4e0a\u3002", "method": "\u63d0\u51faPegasusFlow\u6846\u67b6\u548cWBFO\u7b97\u6cd5\uff0c\u5229\u7528\u6837\u6761\u57fa\u51fd\u6570\u4f18\u5316\u548c\u5e76\u884c\u6a21\u62df\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u3002", "result": "\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5b9e\u73b0100%\u6210\u529f\u7387\u548c18%\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PegasusFlow\u4e3a\u590d\u6742\u5730\u5f62\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u4e13\u5bb6\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08440", "pdf": "https://arxiv.org/pdf/2509.08440", "abs": "https://arxiv.org/abs/2509.08440", "authors": ["Kevin Saad", "Vincenzo Petrone", "Enrico Ferrentino", "Pasquale Chiacchio", "Francesco Braghin", "Loris Roveda"], "title": "Augmenting Neural Networks-based Model Approximators in Robotic Force-tracking Tasks", "categories": ["cs.RO"], "comment": "Accepted for publication at 22nd International Conference on\n  Informatics in Control, Automation and Robotic - ICINCO 2025", "summary": "As robotics gains popularity, interaction control becomes crucial for\nensuring force tracking in manipulator-based tasks. Typically, traditional\ninteraction controllers either require extensive tuning, or demand expert\nknowledge of the environment, which is often impractical in real-world\napplications. This work proposes a novel control strategy leveraging Neural\nNetworks (NNs) to enhance the force-tracking behavior of a Direct Force\nController (DFC). Unlike similar previous approaches, it accounts for the\nmanipulator's tangential velocity, a critical factor in force exertion,\nespecially during fast motions. The method employs an ensemble of feedforward\nNNs to predict contact forces, then exploits the prediction to solve an\noptimization problem and generate an optimal residual action, which is added to\nthe DFC output and applied to an impedance controller. The proposed\nVelocity-augmented Artificial intelligence Interaction Controller for Ambiguous\nModels (VAICAM) is validated in the Gazebo simulator on a Franka Emika Panda\nrobot. Against a vast set of trajectories, VAICAM achieves superior performance\ncompared to two baseline controllers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4ea4\u4e92\u63a7\u5236\u7b56\u7565VAICAM\uff0c\u901a\u8fc7\u9884\u6d4b\u63a5\u89e6\u529b\u5e76\u4f18\u5316\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u529b\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4ea4\u4e92\u63a7\u5236\u5668\u9700\u8981\u5927\u91cf\u8c03\u53c2\u6216\u73af\u5883\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u63a5\u89e6\u529b\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u751f\u6210\u6700\u4f18\u52a8\u4f5c\uff0c\u7ed3\u5408\u76f4\u63a5\u529b\u63a7\u5236\u5668\u548c\u963b\u6297\u63a7\u5236\u5668\u3002", "result": "\u5728Gazebo\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0cVAICAM\u5728\u591a\u79cd\u8f68\u8ff9\u4e0b\u8868\u73b0\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebf\u63a7\u5236\u5668\u3002", "conclusion": "VAICAM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u5927\u91cf\u8c03\u53c2\u7684\u4ea4\u4e92\u63a7\u5236\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2509.08460", "pdf": "https://arxiv.org/pdf/2509.08460", "abs": "https://arxiv.org/abs/2509.08460", "authors": ["Wenqing Wang", "Ye Zhang", "Haoyu Li", "Jingyu Wang"], "title": "Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic Environment", "categories": ["cs.RO"], "comment": null, "summary": "Recent advances in robotics have enabled the widespread deployment of\nautonomous robotic systems in complex operational environments, presenting both\nunprecedented opportunities and significant security problems. Traditional\nshepherding approaches based on fixed formations are often ineffective or risky\nin urban and obstacle-rich scenarios, especially when facing adversarial agents\nwith unknown and adaptive behaviors. This paper addresses this challenge as an\nextended herding problem, where defensive robotic systems must safely guide\nadversarial agents with unknown strategies away from protected areas and into\npredetermined safe regions, while maintaining collision-free navigation in\ndynamic environments. We propose a hierarchical hybrid framework based on\nreach-avoid game theory and local motion planning, incorporating a virtual\ncontainment boundary and event-triggered pursuit mechanisms to enable scalable\nand robust multi-agent coordination. Simulation results demonstrate that the\nproposed approach achieves safe and efficient guidance of adversarial agents to\ndesignated regions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u8fbe-\u907f\u969c\u535a\u5f08\u7406\u8bba\u548c\u5c40\u90e8\u8fd0\u52a8\u89c4\u5212\u7684\u5206\u5c42\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u7684\u5b89\u5168\u5f15\u5bfc\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u7684\u7f16\u961f\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u884c\u4e3a\u672a\u77e5\u4e14\u81ea\u9002\u5e94\u65f6\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u5316\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u865a\u62df\u8fb9\u754c\u548c\u4e8b\u4ef6\u89e6\u53d1\u7684\u8ffd\u9010\u673a\u5236\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b89\u5168\u9ad8\u6548\u5730\u5c06\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u5f15\u5bfc\u81f3\u6307\u5b9a\u533a\u57df\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u6297\u6027\u667a\u80fd\u4f53\u7684\u5f15\u5bfc\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.08495", "pdf": "https://arxiv.org/pdf/2509.08495", "abs": "https://arxiv.org/abs/2509.08495", "authors": ["Gabriel I. Fernandez", "Ruochen Hou", "Alex Xu", "Colin Togashi", "Dennis W. Hong"], "title": "CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust Geometric Approach in the Presence of Symmetries", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we present our localization method called CLAP, Clustering to\nLocalize Across $n$ Possibilities, which helped us win the RoboCup 2024\nadult-sized autonomous humanoid soccer competition. Competition rules limited\nour sensor suite to stereo vision and an inertial sensor, similar to humans. In\naddition, our robot had to deal with varying lighting conditions, dynamic\nfeature occlusions, noise from high-impact stepping, and mistaken features from\nbystanders and neighboring fields. Therefore, we needed an accurate, and most\nimportantly robust localization algorithm that would be the foundation for our\npath-planning and game-strategy algorithms. CLAP achieves these requirements by\nclustering estimated states of our robot from pairs of field features to\nlocalize its global position and orientation. Correct state estimates naturally\ncluster together, while incorrect estimates spread apart, making CLAP resilient\nto noise and incorrect inputs. CLAP is paired with a particle filter and an\nextended Kalman filter to improve consistency and smoothness. Tests of CLAP\nwith other landmark-based localization methods showed similar accuracy.\nHowever, tests with increased false positive feature detection showed that CLAP\noutperformed other methods in terms of robustness with very little divergence\nand velocity jumps. Our localization performed well in competition, allowing\nour robot to shoot faraway goals and narrowly defend our goal.", "AI": {"tldr": "CLAP\u662f\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u4eba\u4f20\u611f\u5668\u53d7\u9650\u5e76\u9700\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u548c\u566a\u58f0\u5e72\u6270\uff0c\u9700\u8981\u4e00\u79cd\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u5b9a\u4f4d\u7b97\u6cd5\u3002", "method": "CLAP\u901a\u8fc7\u805a\u7c7b\u4ece\u573a\u5bf9\u7279\u5f81\u4f30\u8ba1\u7684\u673a\u5668\u4eba\u72b6\u6001\u6765\u786e\u5b9a\u5168\u5c40\u4f4d\u7f6e\u548c\u65b9\u5411\uff0c\u5e76\u7ed3\u5408\u7c92\u5b50\u6ee4\u6ce2\u548c\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u63d0\u9ad8\u4e00\u81f4\u6027\u3002", "result": "CLAP\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u5176\u4ed6\u57fa\u4e8e\u5730\u6807\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u5728\u9ad8\u8bef\u68c0\u7387\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CLAP\u5728\u6bd4\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u4e86\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u548c\u6bd4\u8d5b\u7b56\u7565\u3002"}}
{"id": "2509.08510", "pdf": "https://arxiv.org/pdf/2509.08510", "abs": "https://arxiv.org/abs/2509.08510", "authors": ["Angela Higgins", "Stephen Potter", "Mauro Dragone", "Mark Hawley", "Farshid Amirabdollahian", "Alessandro Di Nuovo", "Praminda Caleb-Solly"], "title": "Facilitating the Emergence of Assistive Robots to Support Frailty: Psychosocial and Environmental Realities", "categories": ["cs.RO"], "comment": null, "summary": "While assistive robots have much potential to help older people with\nfrailty-related needs, there are few in use. There is a gap between what is\ndeveloped in laboratories and what would be viable in real-world contexts.\nThrough a series of co-design workshops (61 participants across 7 sessions)\nincluding those with lived experience of frailty, their carers, and healthcare\nprofessionals, we gained a deeper understanding of everyday issues concerning\nthe place of new technologies in their lives. A persona-based approach surfaced\nemotional, social, and psychological issues. Any assistive solution must be\ndeveloped in the context of this complex interplay of psychosocial and\nenvironmental factors. Our findings, presented as design requirements in direct\nrelation to frailty, can help promote design thinking that addresses people's\nneeds in a more pragmatic way to move assistive robotics closer to real-world\nuse.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u8f85\u52a9\u673a\u5668\u4eba\u867d\u6709\u6f5c\u529b\u5e2e\u52a9\u8001\u5e74\u4eba\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u5c11\uff0c\u539f\u56e0\u662f\u5b9e\u9a8c\u5ba4\u5f00\u53d1\u4e0e\u5b9e\u9645\u9700\u6c42\u5b58\u5728\u5dee\u8ddd\u3002\u901a\u8fc7\u7814\u8ba8\u4f1a\u6536\u96c6\u7528\u6237\u9700\u6c42\uff0c\u5f3a\u8c03\u9700\u8003\u8651\u5fc3\u7406\u548c\u793e\u4f1a\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u8f85\u52a9\u673a\u5668\u4eba\u4e3a\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8f83\u5c11\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u53c2\u4e0e\u7684\u8bbe\u8ba1\u65b9\u6cd5\u586b\u8865\u5b9e\u9a8c\u5ba4\u5f00\u53d1\u4e0e\u73b0\u5b9e\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc77\u573a\u517161\u540d\u53c2\u4e0e\u8005\uff08\u5305\u62ec\u8001\u5e74\u4eba\u3001\u62a4\u7406\u8005\u548c\u533b\u7597\u4e13\u4e1a\u4eba\u5458\uff09\u7684\u5171\u540c\u8bbe\u8ba1\u7814\u8ba8\u4f1a\uff0c\u91c7\u7528\u4eba\u7269\u89d2\u8272\u65b9\u6cd5\u5206\u6790\u60c5\u611f\u3001\u793e\u4f1a\u548c\u5fc3\u7406\u9700\u6c42\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8f85\u52a9\u673a\u5668\u4eba\u8bbe\u8ba1\u9700\u7efc\u5408\u5fc3\u7406\u793e\u4f1a\u548c\u73af\u5883\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u76f4\u63a5\u4e0e\u8870\u5f31\u76f8\u5173\u7684\u8bbe\u8ba1\u8981\u6c42\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8f85\u52a9\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u601d\u8def\uff0c\u63a8\u52a8\u5176\u66f4\u63a5\u8fd1\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.08521", "pdf": "https://arxiv.org/pdf/2509.08521", "abs": "https://arxiv.org/abs/2509.08521", "authors": ["Soheil Espahbodini Nia"], "title": "FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY", "I.2.9; I.2.8"], "comment": "35 pages, 8 figures, 2 tables, submitted to the International Journal\n  of Robotics Research (IJRR)", "summary": "Path planning in dynamic environments remains a core challenge in robotics,\nespecially as autonomous systems are deployed in unpredictable spaces such as\nwarehouses and public roads. While algorithms like Fast Marching Tree\n(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their\nsingle-pass design prevents path revisions which are essential for real-time\nadaptation. On the other hand, full replanning is often too computationally\nexpensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching\nTree algorithm that enables efficient and consistent replanning in dynamic\nenvironments. We revisit the neighbor selection rule of FMT$^{*}$ and\ndemonstrate that a minimal change overcomes its single-pass limitation,\nenabling the algorithm to update cost-to-come values upon discovering better\nconnections without sacrificing asymptotic optimality or computational\nefficiency. By maintaining a cost-ordered priority queue and applying a\nselective update condition that uses an expanding neighbor to identify and\ntrigger the re-evaluation of any node with a potentially suboptimal path,\nFMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the\nenvironment evolves. This targeted strategy preserves the inherent efficiency\nof FMT$^{*}$ while enabling robust adaptation to changes in obstacle\nconfiguration. FMT$^{x}$ is proven to recover an asymptotically optimal\nsolution after environmental changes. Experimental results demonstrate that\nFMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more\nswiftly to dynamic events with lower computational overhead and thus offering a\nmore effective solution for real-time robotic navigation in unpredictable\nworlds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FMT^x\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86FMT*\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u5730\u8fdb\u884c\u8def\u5f84\u91cd\u65b0\u89c4\u5212\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6700\u4f18\u6027\u4e0a\u4fdd\u6301\u5e73\u8861\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u662f\u673a\u5668\u4eba\u6280\u672f\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edf\u7b97\u6cd5\u5982FMT*\u5728\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u53d8\u5316\u7684\u73af\u5883\u9700\u6c42\u3002", "method": "FMT^x\u901a\u8fc7\u4fee\u6539FMT*\u7684\u90bb\u5c45\u9009\u62e9\u89c4\u5219\uff0c\u5f15\u5165\u4e86\u6210\u672c\u6709\u5e8f\u4f18\u5148\u961f\u5217\u548c\u9009\u62e9\u6027\u66f4\u65b0\u6761\u4ef6\uff0c\u4ee5\u5728\u4e0d\u727a\u7272\u6700\u4f18\u6027\u6216\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u8def\u5f84\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cFMT^x\u5728\u5904\u7406\u52a8\u6001\u4e8b\u4ef6\u65f6\u6bd4RRT^x\u66f4\u5feb\u901f\u4e14\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u5b9e\u65f6\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FMT^x\u586b\u8865\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u8def\u5f84\u89c4\u5212\u7684\u7a7a\u767d\uff0c\u7ed3\u5408\u4e86\u6700\u4f18\u6027\u548c\u5b9e\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.08522", "pdf": "https://arxiv.org/pdf/2509.08522", "abs": "https://arxiv.org/abs/2509.08522", "authors": ["Hanyu Liu", "Yunsheng Ma", "Jiaxin Huang", "Keqiang Ren", "Jiayi Wen", "Yilin Zheng", "Baishu Wan", "Pan Li", "Jiejun Hou", "Haoru Luan", "Zhihua Wang", "Zhigong Song"], "title": "RoboMatch: A Mobile-Manipulation Teleoperation Platform with Auto-Matching Network Architecture for Long-Horizon Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents RoboMatch, a novel unified teleoperation platform for\nmobile manipulation with an auto-matching network architecture, designed to\ntackle long-horizon tasks in dynamic environments. Our system enhances\nteleoperation performance, data collection efficiency, task accuracy, and\noperational stability. The core of RoboMatch is a cockpit-style control\ninterface that enables synchronous operation of the mobile base and dual arms,\nsignificantly improving control precision and data collection. Moreover, we\nintroduce the Proprioceptive-Visual Enhanced Diffusion Policy (PVE-DP), which\nleverages Discrete Wavelet Transform (DWT) for multi-scale visual feature\nextraction and integrates high-precision IMUs at the end-effector to enrich\nproprioceptive feedback, substantially boosting fine manipulation performance.\nFurthermore, we propose an Auto-Matching Network (AMN) architecture that\ndecomposes long-horizon tasks into logical sequences and dynamically assigns\nlightweight pre-trained models for distributed inference. Experimental results\ndemonstrate that our approach improves data collection efficiency by over 20%,\nincreases task success rates by 20-30% with PVE-DP, and enhances long-horizon\ninference performance by approximately 40% with AMN, offering a robust solution\nfor complex manipulation tasks.", "AI": {"tldr": "RoboMatch\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7edf\u4e00\u9065\u64cd\u4f5c\u5e73\u53f0\uff0c\u901a\u8fc7\u81ea\u52a8\u5339\u914d\u7f51\u7edc\u67b6\u6784\u548c\u589e\u5f3a\u7684\u6269\u6563\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6570\u636e\u6536\u96c6\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6311\u6218\uff0c\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u64cd\u4f5c\u79fb\u52a8\u5e95\u5ea7\u548c\u53cc\u81c2\u7684\u9065\u64cd\u4f5c\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u9a7e\u9a76\u8231\u5f0f\u63a7\u5236\u63a5\u53e3\u5b9e\u73b0\u540c\u6b65\u64cd\u4f5c\uff0c\u7ed3\u5408\u57fa\u4e8eDWT\u7684\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u548c\u9ad8\u7cbe\u5ea6IMU\u7684PVE-DP\u7b56\u7565\uff0c\u4ee5\u53ca\u52a8\u6001\u5206\u914d\u9884\u8bad\u7ec3\u6a21\u578b\u7684AMN\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6570\u636e\u6536\u96c6\u6548\u7387\u63d0\u534720%\u4ee5\u4e0a\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad820-30%\uff0c\u957f\u65f6\u7a0b\u63a8\u7406\u6027\u80fd\u63d0\u5347\u7ea640%\u3002", "conclusion": "RoboMatch\u4e3a\u590d\u6742\u64cd\u7eb5\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.08638", "pdf": "https://arxiv.org/pdf/2509.08638", "abs": "https://arxiv.org/abs/2509.08638", "authors": ["Rebecca Martin", "Jay Patrikar", "Sebastian Scherer"], "title": "AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models", "categories": ["cs.RO"], "comment": null, "summary": "Specialized machine learning models, regardless of architecture and training,\nare susceptible to failures in deployment. With their increasing use in high\nrisk situations, the ability to audit these models by determining their\noperational design domain (ODD) is crucial in ensuring safety and compliance.\nHowever, given the high-dimensional input spaces, this process often requires\nsignificant human resources and domain expertise. To alleviate this, we\nintroduce \\coolname, an LLM-Agent centric framework for automated generation of\nsemantically relevant test cases to search for failure modes in specialized\nblack-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit\na uncertainty-aware failure distribution model on a learned text-embedding\nmanifold by projecting the high-dimension input space to low-dimension\ntext-embedding latent space. The LLM-Agent is tasked with iteratively building\nthe failure landscape by leveraging tools for generating test-cases to probe\nthe model-under-test (MUT) and recording the response. The agent also guides\nthe search using tools to probe uncertainty estimate on the low dimensional\nmanifold. We demonstrate this process in a simple case using models trained\nwith missing digits on the MNIST dataset and in the real world setting of\nvision-based intruder detection for aerial vehicles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM-Agent\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u8bed\u4e49\u76f8\u5173\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4ee5\u641c\u7d22\u9ed1\u76d2\u6a21\u578b\u7684\u6545\u969c\u6a21\u5f0f\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u786e\u5b9a\u5176\u64cd\u4f5c\u8bbe\u8ba1\u57df\uff08ODD\uff09\u5bf9\u4e8e\u786e\u4fdd\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f46\u7531\u4e8e\u8f93\u5165\u7a7a\u95f4\u7ef4\u5ea6\u9ad8\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u529b\u548c\u9886\u57df\u77e5\u8bc6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\\coolname\u7684\u6846\u67b6\uff0c\u901a\u8fc7LLM-Agent\u4f5c\u4e3a\u5de5\u5177\u534f\u8c03\u5668\uff0c\u5c06\u9ad8\u7ef4\u8f93\u5165\u7a7a\u95f4\u6295\u5f71\u5230\u4f4e\u7ef4\u6587\u672c\u5d4c\u5165\u6f5c\u7a7a\u95f4\uff0c\u5e76\u8fed\u4ee3\u6784\u5efa\u6545\u969c\u5206\u5e03\u6a21\u578b\u3002", "result": "\u901a\u8fc7MNIST\u6570\u636e\u96c6\u4e2d\u7684\u7f3a\u5931\u6570\u5b57\u6a21\u578b\u548c\u65e0\u4eba\u673a\u89c6\u89c9\u5165\u4fb5\u68c0\u6d4b\u7684\u5b9e\u9645\u573a\u666f\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u5316\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e2e\u52a9\u53d1\u73b0\u9ed1\u76d2\u6a21\u578b\u7684\u6f5c\u5728\u6545\u969c\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.08699", "pdf": "https://arxiv.org/pdf/2509.08699", "abs": "https://arxiv.org/abs/2509.08699", "authors": ["Stefan Podgorski", "Sourav Garg", "Mehdi Hosseinzadeh", "Lachlan Mares", "Feras Dayoub", "Ian Reid"], "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "9 pages, 5 figures, ICRA 2025", "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5168\u5c403D\u5730\u56fe\u6216\u9884\u8bad\u7ec3\u63a7\u5236\u5668\u7684RGB\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u62d3\u6251\u8def\u5f84\u89c4\u5212\u548c\u5c40\u90e8\u8f68\u8ff9\u63a7\u5236\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u957f\u8ddd\u79bb\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u5bfc\u822a\u4f9d\u8d56\u5168\u5c403D\u5730\u56fe\u6216\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5bf9\u8c61\u7ea7\u62d3\u6251\u5bfc\u822a\u7ba1\u9053\uff0c\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u548c\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u9884\u6d4b\u5c40\u90e8\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u5207\u6362\u673a\u5236\u81f3\u57fa\u7ebf\u63a7\u5236\u5668\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u5e94\u6027\u5f3a\u4e14\u5f00\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u5f00\u653e\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08743", "pdf": "https://arxiv.org/pdf/2509.08743", "abs": "https://arxiv.org/abs/2509.08743", "authors": ["Anoop Bhat", "Geordan Gutow", "Bhaskar Vundurthy", "Zhongqiang Ren", "Sivakumar Rathinam", "Howie Choset"], "title": "Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling Salesman Problems", "categories": ["cs.RO"], "comment": null, "summary": "The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent\ntrajectory that intercepts several moving targets, within a particular time\nwindow for each target. In the presence of generic nonlinear target\ntrajectories or kinematic constraints on the agent, no prior algorithm\nguarantees convergence to an optimal MT-TSP solution. Therefore, we introduce\nthe Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is\nto alternate between randomly sampling a set of agent configuration-time\npoints, corresponding to interceptions of targets, and finding a sequence of\ninterception points by solving a generalized TSP (GTSP). This alternation\nenables asymptotic convergence to the optimum. We introduce two parallel\nalgorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves\nGTSPs using PGLNS, our parallelized extension of the state-of-the-art solver\nGLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs\ncorresponding to several sets of points simultaneously. We present numerical\nresults for three variants of the MT-TSP: one where intercepting a target only\nrequires coming within a particular distance, another where the agent is a\nvariable-speed Dubins car, and a third where the agent is a redundant robot\narm. We show that IRG-PGLNS and PCG both converge faster than a baseline based\non prior work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86IRG\u6846\u67b6\u6765\u89e3\u51b3\u79fb\u52a8\u76ee\u6807\u65c5\u884c\u5546\u95ee\u9898\uff08MT-TSP\uff09\uff0c\u901a\u8fc7\u4ea4\u66ff\u968f\u673a\u91c7\u6837\u548c\u6c42\u89e3\u5e7f\u4e49TSP\u5b9e\u73b0\u6e10\u8fdb\u6700\u4f18\u3002\u4e24\u79cd\u5e76\u884c\u7b97\u6cd5IRG-PGLNS\u548cPCG\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u79fb\u52a8\u76ee\u6807\u65c5\u884c\u5546\u95ee\u9898\u5728\u975e\u7ebf\u6027\u76ee\u6807\u8f68\u8ff9\u6216\u4ee3\u7406\u8fd0\u52a8\u7ea6\u675f\u4e0b\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "IRG\u6846\u67b6\u7ed3\u5408\u968f\u673a\u91c7\u6837\u548c\u5e7f\u4e49TSP\u6c42\u89e3\uff0c\u5e76\u5f15\u5165\u4e86IRG-PGLNS\u548cPCG\u4e24\u79cd\u5e76\u884c\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aIRG-PGLNS\u548cPCG\u5728MT-TSP\u7684\u4e09\u4e2a\u53d8\u4f53\u4e2d\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "IRG\u6846\u67b6\u89e3\u51b3\u4e86MT-TSP\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u5e76\u884c\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2509.08757", "pdf": "https://arxiv.org/pdf/2509.08757", "abs": "https://arxiv.org/abs/2509.08757", "authors": ["Michael J. Munje", "Chen Tang", "Shuijing Liu", "Zichao Hu", "Yifeng Zhu", "Jiaxun Cui", "Garrett Warnell", "Joydeep Biswas", "Peter Stone"], "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://larg.github.io/socialnav-sub", "summary": "Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub .", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SocialNav-SUB\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u573a\u666f\u4e2d\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u7814\u7a76VLMs\u662f\u5426\u80fd\u7406\u89e3\u590d\u6742\u793e\u4ea4\u5bfc\u822a\u573a\u666f\uff08\u5982\u63a8\u65ad\u4ee3\u7406\u4e4b\u95f4\u7684\u65f6\u7a7a\u5173\u7cfb\u548c\u4eba\u7c7b\u610f\u56fe\uff09\uff0c\u4ee5\u652f\u6301\u5b89\u5168\u4e14\u7b26\u5408\u793e\u4ea4\u89c4\u8303\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002", "method": "\u521b\u5efaSocialNav-SUB\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u8bc4\u4f30VLMs\u5728\u7a7a\u95f4\u3001\u65f6\u7a7a\u548c\u793e\u4ea4\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136\u8868\u73b0\u6700\u4f73\u7684VLM\u4e0e\u4eba\u7c7b\u7b54\u6848\u7684\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u4f46\u4ecd\u4e0d\u53ca\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548c\u4eba\u7c7b\u5171\u8bc6\u57fa\u51c6\u3002", "conclusion": "SocialNav-SUB\u4e3a\u672a\u6765\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524dVLMs\u5728\u793e\u4ea4\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5173\u952e\u4e0d\u8db3\u3002"}}
{"id": "2509.08775", "pdf": "https://arxiv.org/pdf/2509.08775", "abs": "https://arxiv.org/abs/2509.08775", "authors": ["Wonsuhk Jung", "Utkarsh A. Mishra", "Nadun Ranawaka Arachchige", "Yongxin Chen", "Danfei Xu", "Shreyas Kousik"], "title": "Joint Model-based Model-free Diffusion for Planning with Constraints", "categories": ["cs.RO"], "comment": "The first two authors contributed equally. Last three authors advised\n  equally. Accepted to CoRL 2025", "summary": "Model-free diffusion planners have shown great promise for robot motion\nplanning, but practical robotic systems often require combining them with\nmodel-based optimization modules to enforce constraints, such as safety.\nNaively integrating these modules presents compatibility challenges when\ndiffusion's multi-modal outputs behave adversarially to optimization-based\nmodules. To address this, we introduce Joint Model-based Model-free Diffusion\n(JM2D), a novel generative modeling framework. JM2D formulates module\nintegration as a joint sampling problem to maximize compatibility via an\ninteraction potential, without additional training. Using importance sampling,\nJM2D guides modules outputs based only on evaluations of the interaction\npotential, thus handling non-differentiable objectives commonly arising from\nnon-convex optimization modules. We evaluate JM2D via application to aligning\ndiffusion planners with safety modules on offline RL and robot manipulation.\nJM2D significantly improves task performance compared to conventional safety\nfilters without sacrificing safety. Further, we show that conditional\ngeneration is a special case of JM2D and elucidate key design choices by\ncomparing with SOTA gradient-based and projection-based diffusion planners.\nMore details at: https://jm2d-corl25.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJM2D\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6a21\u578b\u81ea\u7531\u548c\u6a21\u578b\u57fa\u7840\u7684\u6269\u6563\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u91c7\u6837\u4f18\u5316\u6a21\u5757\u517c\u5bb9\u6027\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u4e14\u4e0d\u727a\u7272\u5b89\u5168\u6027\u3002", "motivation": "\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u81ea\u7531\u6269\u6563\u89c4\u5212\u4e0e\u6a21\u578b\u57fa\u7840\u4f18\u5316\u6a21\u5757\u7684\u7ed3\u5408\u5e38\u56e0\u517c\u5bb9\u6027\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "JM2D\u6846\u67b6\u901a\u8fc7\u8054\u5408\u91c7\u6837\u548c\u91cd\u8981\u6027\u91c7\u6837\u6700\u5927\u5316\u6a21\u5757\u517c\u5bb9\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u4e2d\uff0cJM2D\u663e\u8457\u63d0\u5347\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u3002", "conclusion": "JM2D\u4e3a\u6a21\u5757\u6574\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u68af\u5ea6\u57fa\u7840\u548c\u6295\u5f71\u57fa\u7840\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.08813", "pdf": "https://arxiv.org/pdf/2509.08813", "abs": "https://arxiv.org/abs/2509.08813", "authors": ["Davide Allegro", "Matteo Terreran", "Stefano Ghidoni"], "title": "Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "Robots often rely on RGB images for tasks like manipulation and navigation.\nHowever, reliable interaction typically requires a 3D scene representation that\nis metric-scaled and aligned with the robot reference frame. This depends on\naccurate camera-to-robot calibration and dense 3D reconstruction, tasks usually\ntreated separately, despite both relying on geometric correspondences from RGB\ndata. Traditional calibration needs patterns, while RGB-based reconstruction\nyields geometry with an unknown scale in an arbitrary frame. Multi-camera\nsetups add further complexity, as data must be expressed in a shared reference\nframe. We present Calib3R, a patternless method that jointly performs\ncamera-to-robot calibration and metric-scaled 3D reconstruction via unified\noptimization. Calib3R handles single- and multi-camera setups on robot arms or\nmobile robots. It builds on the 3D foundation model MASt3R to extract pointmaps\nfrom RGB images, which are combined with robot poses to reconstruct a scaled 3D\nscene aligned with the robot. Experiments on diverse datasets show that Calib3R\nachieves accurate calibration with less than 10 images, outperforming\ntarget-less and marker-based methods.", "AI": {"tldr": "Calib3R\u662f\u4e00\u79cd\u65e0\u9700\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u4f18\u5316\u540c\u65f6\u5b8c\u6210\u76f8\u673a\u5230\u673a\u5668\u4eba\u7684\u6821\u51c6\u548c\u5ea6\u91cf\u5c3a\u5ea6\u76843D\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u5355\u548c\u591a\u76f8\u673a\u8bbe\u7f6e\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\u901a\u5e38\u9700\u89813D\u573a\u666f\u8868\u793a\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u5206\u5f00\u5904\u7406\u6821\u51c6\u548c\u91cd\u5efa\uff0c\u4f9d\u8d56\u6a21\u5f0f\u4e14\u590d\u6742\u3002", "method": "\u5229\u75283D\u57fa\u7840\u6a21\u578bMASt3R\u4eceRGB\u56fe\u50cf\u63d0\u53d6\u70b9\u56fe\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u59ff\u6001\u91cd\u5efa\u5bf9\u9f50\u76843D\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCalib3R\u4ec5\u9700\u5c11\u4e8e10\u5f20\u56fe\u50cf\u5373\u53ef\u5b9e\u73b0\u7cbe\u51c6\u6821\u51c6\uff0c\u4f18\u4e8e\u65e0\u76ee\u6807\u548c\u6807\u8bb0\u65b9\u6cd5\u3002", "conclusion": "Calib3R\u4e3a\u89e3\u51b3\u673a\u5668\u4eba3D\u6821\u51c6\u548c\u91cd\u5efa\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4e00\u4f53\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08820", "pdf": "https://arxiv.org/pdf/2509.08820", "abs": "https://arxiv.org/abs/2509.08820", "authors": ["Zongzheng Zhang", "Chenghao Yue", "Haobo Xu", "Minwen Liao", "Xianglin Qi", "Huan-ang Gao", "Ziwei Wang", "Hao Zhao"], "title": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation", "categories": ["cs.RO"], "comment": "Accepted to CoRL 2025, Project Page:\n  https://zzongzheng0918.github.io/RoboChemist.github.io/", "summary": "Robotic chemists promise to both liberate human experts from repetitive tasks\nand accelerate scientific discovery, yet remain in their infancy. Chemical\nexperiments involve long-horizon procedures over hazardous and deformable\nsubstances, where success requires not only task completion but also strict\ncompliance with experimental norms. To address these challenges, we propose\n\\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language\nModels (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based\nsystems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with\ntransparent labware, and existing VLA systems (e.g., RDT, pi0) that lack\nsemantic-level feedback for complex tasks, our method leverages a VLM to serve\nas (1) a planner to decompose tasks into primitive actions, (2) a visual prompt\ngenerator to guide VLA models, and (3) a monitor to assess task success and\nregulatory compliance. Notably, we introduce a VLA interface that accepts\nimage-based visual targets from the VLM, enabling precise, goal-conditioned\ncontrol. Our system successfully executes both primitive actions and complete\nmulti-step chemistry protocols. Results show 23.57% higher average success rate\nand a 0.298 average increase in compliance rate over state-of-the-art VLA\nbaselines, while also demonstrating strong generalization to objects and tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoboChemist\u7684\u53cc\u73af\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5316\u5b66\u5b9e\u9a8c\u4e2d\u590d\u6742\u4efb\u52a1\u7684\u89c4\u5212\u3001\u6267\u884c\u548c\u76d1\u63a7\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVLM\u6216VLA\u7684\u7cfb\u7edf\u5728\u900f\u660e\u5b9e\u9a8c\u5668\u76bf\u6216\u7f3a\u4e4f\u8bed\u4e49\u53cd\u9988\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5316\u5b66\u5b9e\u9a8c\u4e2d\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "\u901a\u8fc7VLM\u4f5c\u4e3a\u4efb\u52a1\u5206\u89e3\u5668\u3001\u89c6\u89c9\u63d0\u793a\u751f\u6210\u5668\u548c\u76d1\u63a7\u5668\uff0c\u7ed3\u5408VLA\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u7684\u76ee\u6807\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u591a\u6b65\u5316\u5b66\u534f\u8bae\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad823.57%\uff0c\u5408\u89c4\u7387\u63d0\u53470.298\uff0c\u4e14\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RoboChemist\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5316\u5b66\u5bb6\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u548c\u5408\u89c4\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.07996", "pdf": "https://arxiv.org/pdf/2509.07996", "abs": "https://arxiv.org/abs/2509.07996", "authors": ["Lingdong Kong", "Wesley Yang", "Jianbiao Mei", "Youquan Liu", "Ao Liang", "Dekai Zhu", "Dongyue Lu", "Wei Yin", "Xiaotao Hu", "Mingkai Jia", "Junyuan Deng", "Kaiwen Zhang", "Yang Wu", "Tianyi Yan", "Shenyuan Gao", "Song Wang", "Linfeng Li", "Liang Pan", "Yong Liu", "Jianke Zhu", "Wei Tsang Ooi", "Steven C. H. Hoi", "Ziwei Liu"], "title": "3D and 4D World Modeling: A Survey", "categories": ["cs.CV", "cs.RO"], "comment": "Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at\n  https://github.com/worldbench/survey", "summary": "World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u586b\u8865\u4e863D\u548c4D\u4e16\u754c\u5efa\u6a21\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u5b9a\u4e49\u548c\u5206\u7c7b\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e2D\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf93D\u548c4D\u8868\u793a\u7684\u7edf\u4e00\u8ba8\u8bba\uff0c\u4e14\u4e16\u754c\u6a21\u578b\u7684\u5b9a\u4e49\u548c\u5206\u7c7b\u4e0d\u6e05\u6670\uff0c\u5bfc\u81f4\u7814\u7a76\u5206\u6563\u4e14\u4e0d\u4e00\u81f4\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u660e\u786e\u7684\u5b9a\u4e49\u548c\u5206\u7c7b\uff08VideoGen\u3001OccGen\u3001LiDARGen\uff09\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u9002\u7528\u4e8e3D/4D\u573a\u666f\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u9996\u6b21\u5168\u9762\u56de\u987e\u4e863D\u548c4D\u4e16\u754c\u5efa\u6a21\u4e0e\u751f\u6210\u7814\u7a76\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u7ed3\u6784\u548c\u53c2\u8003\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a3D/4D\u4e16\u754c\u5efa\u6a21\u9886\u57df\u5960\u5b9a\u4e86\u7edf\u4e00\u7684\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.07997", "pdf": "https://arxiv.org/pdf/2509.07997", "abs": "https://arxiv.org/abs/2509.07997", "authors": ["Abigail Breitfeld", "Alberto Candela", "Juan Delfa", "Akseli Kangaslahti", "Itai Zilberstein", "Steve Chien", "David Wettergreen"], "title": "Learning-Based Planning for Improving Science Return of Earth Observation Satellites", "categories": ["cs.AI", "cs.RO"], "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "Earth observing satellites are powerful tools for collecting scientific\ninformation about our planet, however they have limitations: they cannot easily\ndeviate from their orbital trajectories, their sensors have a limited field of\nview, and pointing and operating these sensors can take a large amount of the\nspacecraft's resources. It is important for these satellites to optimize the\ndata they collect and include only the most important or informative\nmeasurements. Dynamic targeting is an emerging concept in which satellite\nresources and data from a lookahead instrument are used to intelligently\nreconfigure and point a primary instrument. Simulation studies have shown that\ndynamic targeting increases the amount of scientific information gathered\nversus conventional sampling strategies. In this work, we present two different\nlearning-based approaches to dynamic targeting, using reinforcement and\nimitation learning, respectively. These learning methods build on a dynamic\nprogramming solution to plan a sequence of sampling locations. We evaluate our\napproaches against existing heuristic methods for dynamic targeting, showing\nthe benefits of using learning for this application. Imitation learning\nperforms on average 10.0\\% better than the best heuristic method, while\nreinforcement learning performs on average 13.7\\% better. We also show that\nboth learning methods can be trained effectively with relatively small amounts\nof data.", "AI": {"tldr": "\u52a8\u6001\u76ee\u6807\u5b9a\u4f4d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u4f18\u5316\u536b\u661f\u6570\u636e\u6536\u96c6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u536b\u661f\u53d7\u9650\u4e8e\u8f68\u9053\u548c\u4f20\u611f\u5668\uff0c\u9700\u4f18\u5316\u6570\u636e\u6536\u96c6\uff0c\u52a8\u6001\u76ee\u6807\u5b9a\u4f4d\u80fd\u63d0\u5347\u79d1\u5b66\u4fe1\u606f\u83b7\u53d6\u6548\u7387\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u89c4\u5212\u751f\u6210\u91c7\u6837\u5e8f\u5217\uff0c\u5e76\u4e0e\u542f\u53d1\u5f0f\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u6a21\u4eff\u5b66\u4e60\u5e73\u5747\u4f18\u4e8e\u6700\u4f73\u542f\u53d1\u5f0f\u65b9\u6cd510.0%\uff0c\u5f3a\u5316\u5b66\u4e60\u4f1813.7%\uff0c\u4e14\u6570\u636e\u9700\u6c42\u91cf\u5c0f\u3002", "conclusion": "\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u52a8\u6001\u76ee\u6807\u5b9a\u4f4d\u6548\u7387\uff0c\u9002\u7528\u4e8e\u536b\u661f\u6570\u636e\u4f18\u5316\u3002"}}
{"id": "2509.08085", "pdf": "https://arxiv.org/pdf/2509.08085", "abs": "https://arxiv.org/abs/2509.08085", "authors": ["Aakash Khandelwal", "Ranjan Mukherjee"], "title": "Planar Juggling of a Devil-Stick using Discrete VHCs", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "7 pages, 4 figures", "summary": "Planar juggling of a devil-stick using impulsive inputs is addressed using\nthe concept of discrete virtual holonomic constraints (DVHC). The location of\nthe center-of-mass of the devil-stick is specified in terms of its orientation\nat the discrete instants when impulsive control inputs are applied. The\ndiscrete zero dynamics (DZD) resulting from the choice of DVHC provides\nconditions for stable juggling. A control design that enforces the DVHC and an\norbit stabilizing controller are presented. The approach is validated in\nsimulation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u79bb\u6563\u865a\u62df\u5168\u606f\u7ea6\u675f\uff08DVHC\uff09\u548c\u51b2\u52a8\u8f93\u5165\u5b9e\u73b0\u5e73\u9762\u9b54\u9b3c\u68cd\u6742\u6280\uff0c\u901a\u8fc7\u79bb\u6563\u96f6\u52a8\u529b\u5b66\uff08DZD\uff09\u63d0\u4f9b\u7a33\u5b9a\u6742\u6280\u7684\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u63a7\u5236\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u79bb\u6563\u865a\u62df\u5168\u606f\u7ea6\u675f\u548c\u51b2\u52a8\u8f93\u5165\u65b9\u6cd5\uff0c\u89e3\u51b3\u9b54\u9b3c\u68cd\u6742\u6280\u7684\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5e73\u9762\u6742\u6280\u8868\u6f14\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u6307\u5b9a\u9b54\u9b3c\u68cd\u7684\u8d28\u5fc3\u4f4d\u7f6e\u4e0e\u5176\u5728\u63a7\u5236\u51b2\u52a8\u65f6\u523b\u7684\u53d6\u5411\u5173\u7cfb\uff0c\u5efa\u7acb\u79bb\u6563\u96f6\u52a8\u529b\u5b66\uff0c\u8bbe\u8ba1\u63a7\u5236\u65b9\u6848\u4ee5\u786e\u4fddDVHC\u548c\u8f68\u9053\u7a33\u5b9a\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u63a7\u5236\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u7a33\u5b9a\u7684\u9b54\u9b3c\u68cd\u6742\u6280\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7DVHC\u548cDZD\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u6210\u529f\u89e3\u51b3\u5e73\u9762\u9b54\u9b3c\u68cd\u6742\u6280\u7684\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u7c7b\u4f3c\u7cfb\u7edf\u7684\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
